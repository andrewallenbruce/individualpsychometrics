[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Individual Psychometrics",
    "section": "",
    "text": "Preface\nI have ambitious goals for this book, but it is not nearly complete. I have been working on it off and on since 2012. It is accompanied by the R package psycheval (Schneider, 2023), which is also in a preliminary state of development.\n\nSchneider, W. J. (2023). Psycheval: A psychological evaluation toolkit. https://github.com/wjschne/psycheval\n\n\n\nMost of the figures and tables for this book were created in R or in \\LaTeX. To make the content as accessible and transparent as possible, I have included buttons that will reveal the code used to make each figure or table. For example, the initial setup code used for this book can be seen by clicking the button below:\n\n\n R Code\n\n\n# Load packages\nlibrary(extrafont)\nloadfonts(\"win\", quiet = TRUE)\nlibrary(tufte)\nlibrary(knitr)\nlibrary(sn)\nlibrary(fMultivar)\nlibrary(IDPmisc)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(ggforce)\nlibrary(sjmisc)\nlibrary(WJSmisc)\nlibrary(tippy)\nlibrary(tikzDevice)\nlibrary(patchwork)\nlibrary(qualvar)\nlibrary(modeest)\nlibrary(tinter)\nlibrary(ggfx)\nlibrary(ggtext)\nlibrary(lemon)\nlibrary(signs)\nlibrary(scales)\nlibrary(psycheval)\nlibrary(bezier)\nlibrary(DescTools)\n\n# Set options\noptions(knitr.kable.digits = 2, knitr.kable.na = '')\nknitr::opts_template$set(marginfigure = list(fig.column = \"margin\", fig.cap.location = \"top\", out.width = \"100%\", fig.align = \"left\"),\n                         bodyfigure = list(fig.column = \"body\", fig.cap.location = \"margin\"))\n\n# Default fonts and colors\nbfont = \"Equity Text A Tab\"\nbsize = 16\nmyfills <- c(\"royalblue4\", \"firebrick4\", \"#51315E\")\n\n# Function for converting base size to geom_text size\nggtext_size <- function(base_size, ratio = 0.8) {\n  ratio * base_size / ggplot2:::.pt\n}\n\nbtxt_size = ggtext_size(bsize)\n\n# Default geoms and themes\nggplot2::update_geom_defaults(\"text\",\n                              list(family = bfont, size = btxt_size))\nggplot2::update_geom_defaults(\"label\",\n                              list(\n                                family = bfont,\n                                size = btxt_size,\n                                label.padding = unit(0, \"lines\"),\n                                label.size = 0\n                              ))\n\nggplot2::update_geom_defaults(\"richtext\",\n                              list(family = bfont, size = btxt_size))\n\nggplot2::update_geom_defaults(\"density\", list(fill = myfills[1]))\ngeom_text_fill <- function(...) {\n  geom_label(...,\n             label.padding = unit(0, \"lines\"),\n             label.size = 0)\n}\ntheme_set(theme_minimal(base_size = bsize, base_family = bfont))\n\n\n# font family\nspan_style <- function(x, style = \"font-family:serif\") {\n  paste0('<span style=\\\"',\n         style,\n         '\\\">',\n         x,\n         \"</span>\")\n}\n\n# Probability labels\nprob_label <- function(p,\n                       accuracy = 0.01,\n                       digits = NULL,\n                       max_digits = NULL,\n                       remove_leading_zero = TRUE,\n                       round_zero_one = TRUE) {\n  if (is.null(digits)) {\n    l <- scales::number(p, accuracy = accuracy)\n  } else {\n    sig_digits <- abs(ceiling(log10(p + p / 1000000000)) - digits)\n    sig_digits[p > 0.99] <- abs(ceiling(log10(1 - p[p > 0.99])) - digits + 1)\n    sig_digits[(ceiling(log10(p)) == log10(p)) & (-log10(p) >= digits)] <- sig_digits[ceiling(log10(p)) == log10(p)] - 1\n    sig_digits[is.infinite(sig_digits)] <- 0\n    l <- purrr::map2_chr(p,\n                         sig_digits,\n                         formatC,\n                         format = \"f\",\n                         flag = \"#\")\n\n  }\n  if (remove_leading_zero) l <- sub(\"^-0\",\"-\", sub(\"^0\",\"\", l))\n\n  if (round_zero_one) {\n    l[p == 0] <- \"0\"\n    l[p == 1] <- \"1\"\n    l[p == -1] <- \"-1\"\n  }\n\n  if (!is.null(max_digits)) {\n    if (round_zero_one) {\n      l[round(p, digits = max_digits) == 0] <- \"0\"\n      l[round(p, digits = max_digits) == 1] <- \"1\"\n      l[round(p, digits = max_digits) == -1] <- \"-1\"\n    } else {\n      l[round(p, digits = max_digits) == 0] <- paste0(\".\", paste0(rep(\"0\", max_digits), collapse = \"\"))\n      l[round(p, digits = max_digits) == 1] <- paste0(\"1.\", paste0(rep(\"0\", max_digits), collapse = \"\"))\n      l[round(p, digits = max_digits) == -1] <- paste0(\"-1.\", paste0(rep(\"0\", max_digits), collapse = \"\"))\n    }\n  }\n\n  dim(l) <- dim(p)\n  l\n}\n\n# Set table column width\n# https://github.com/rstudio/bookdown/issues/122#issuecomment-221101375\nhtml_table_width <- function(kable_output, width, tag = \"</caption>\"){\n  width_html <- paste0(\n    paste0('<col width=\"',\n           width,\n           '\">'),\n    collapse = \"\\n\")\n  sub(tag,\n      paste0(tag,\n             \"\\n\",\n             width_html),\n      kable_output)\n}\n\n# Make a matrix with braces\nbmatrix <- function(M, brace = \"bmatrix\", includenames=TRUE) {\n  if (includenames) {\n    M <- cbind(rownames(M),M)\n    M <- rbind(colnames(M), M)\n  }\n  M <-  paste(apply(M,\n                    MARGIN = 1,\n                    FUN = paste0,\n                    collapse = \" & \"),\n              collapse = \"\\\\\\\\\\n\")\n\n\n  if (!is.null(brace)) {\n    M <- paste0(\"\\\\begin{\",brace,\"}\\n\", M, \"\\n\\\\end{\", brace , \"}\")\n    }\n  M\n}\n\n# defword <- function(word,\n#                     note,\n#                     wordclass=\"defword\",\n#                     noteclass = \"aside defword\",\n#                     icon = \"&#8853;\") {\n#   # Adapted from tufte:::marginnote_html\n# \n#   sprintf(\n#     paste0(\n#       \"<span class=\\\"%s\\\">%s</span>\",\n#       \"<span class=\\\"%s\\\">\",\n#       \"<label for=\\\"tufte-mn-\\\" class=\\\"margin-toggle\\\">%s</label>\",\n#       \"<input type=\\\"checkbox\\\" id=\\\"tufte-mn-\\\" class=\\\"margin-toggle\\\">%s\",\n#       \"</span>\"\n#     ),\n#     wordclass,\n#     word,\n#     noteclass,\n#     icon,\n#     note\n#   )\n# }\n\n# Hooks -------\n\n# Enclose collapsible r chunk in  button\nknitr::opts_hooks$set(button_r = function(options) {\n  if (isTRUE(options$button_r)) {\n    options$button_before_r <- TRUE\n    options$button_after <- TRUE\n    options$echo = TRUE; options$eval = FALSE\n  }\n\n  options\n})\n\n# Enclose collapsible latex chunk in  button\nknitr::opts_hooks$set(button_latex = function(options) {\n  if (isTRUE(options$button_latex)) {\n    options$button_before_latex <- TRUE\n    options$button_after <- TRUE\n    options$echo = TRUE; options$eval = FALSE\n  }\n\n  options\n})\n\n# before button for collapsible r chunk\nknit_hooks$set(\n  button_before_r = function(before, options, envir) {\n    if (before) {\n      codetype <- \"R Code\"\n      if (!is.null(options$figlabel)) {\n        codetype <- paste0(codetype, \" for @\", options$figlabel)\n      } \n      return(\n        paste0(\n          '<div class=\"wrap-collapsible\" style=\"margin-top: 1em\">',\n          \"\\n\",\n          '<input id=\"collapsible-',\n          options$label,\n          '\" class=\"toggle\" type=\"checkbox\">',\n          \"\\n\",\n          '<label for=\"collapsible-',\n          options$label,\n          '\" class=\"lbl-toggle\">', codetype,'</label>',\n          '<div class=\"collapsible-content\">',\n          \"\\n\",\n          '<div class=\"content-inner\">'\n        )\n      )\n    }\n  }\n)\n\n# before button for collapsible latex chunk\nknit_hooks$set(\n  button_before_latex = function(before, options, envir) {\n    if (before) {\n      codetype <- \"$\\\\rm\\\\LaTeX~Code$\"\n      if (!is.null(options$figlabel)) {\n        codetype <- paste0(codetype, \" for @\", options$figlabel)\n      } \n      return(\n        paste0(\n          '<div class=\"wrap-collapsible\" style=\"margin-top: 1em\">',\n          \"\\n\",\n          '<input id=\"collapsible-',\n          options$label,\n          '\" class=\"toggle\" type=\"checkbox\">',\n          \"\\n\",\n          '<label for=\"collapsible-',\n          options$label,\n          '\" class=\"lbl-toggle\">',\n          codetype,\n          '</label>',\n          '<div class=\"collapsible-content\">',\n          \"\\n\",\n          '<div class=\"content-inner\">'\n        )\n      )\n    }\n  }\n)\n\n# After button for collapsible chunks\nknit_hooks$set(button_after = function(before, options, envir) {\n  if (!before) return('</div></div></div>')\n})\n\n\n\n\nIn addition, all the files and code used to create this book can be found in its Github repository.\nTo avoid repeated citation, I must note that in preparing this book, I have drawn heavily—and no doubt unconsciously—from many authoritative sources on psychometrics, statistical analysis, and linear algebra (Cohen et al., 2003; Crocker & Algina, 2006; Eaton, 2007; Furr, 2017; Nunnally, 1967; Raykov & Marcoulides, 2011; Strang, 2016). I am also grateful to the many unsung authors at Wikipedia and Mathematica who maintain wonderfully comprehensive, up-to-date, and well-referenced documentation of all things mathematical and statistical.\n\n\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied multiple regression/correlation analysis for the behavioral sciences. L. Erlbaum Associates.\n\nCrocker, L., & Algina, J. (2006). Introduction to classical and modern test theory. Cengage Learning.\n\nEaton, M. L. (2007). Multivariate statistics: A vector space approach. Inst. of Mathematical Statistics.\n\nFurr, R. (2017). Psychometrics: An introduction (3rd ed.). Sage.\n\nNunnally, J. C. (1967). Psychometric theory. McGraw-Hill.\n\nRaykov, T., & Marcoulides, G. A. (2011). Introduction to psychometric theory. Routledge.\n\nStrang, G. (2016). Introduction to linear algebra (5th edition). Cambridge press."
  },
  {
    "objectID": "intro.html#purpose-of-this-book",
    "href": "intro.html#purpose-of-this-book",
    "title": "1  Introduction",
    "section": "1.1 Purpose of this book",
    "text": "1.1 Purpose of this book\nMost introductory psychometrics textbooks are designed to help researchers create well constructed tests and therefore cover many details that are not useful to clinicians and fail to cover many practical issues that clinicians should know about. This book is intended to help you extract useful information from the data you already have in ways that you may not have known were possible.\nThat my emphasis is on the practical in no way implies that this book is dumbed down. My aim is to make psychometrics useful to clinicians. If some useful ideas are complex, I hope to make them accessible—but without resorting to superficial glossing. Some background knowledge of psychometrics is necessary to understand how these tools work and, more importantly, when their underlying assumptions have been violated."
  },
  {
    "objectID": "intro.html#inspiration",
    "href": "intro.html#inspiration",
    "title": "1  Introduction",
    "section": "1.2 Inspiration",
    "text": "1.2 Inspiration\n\n\n\n\n\n\nFigure 1.1: Philip Ley (1933–2015)Image Credit\n\n\n\nWhenever I go to a library, I find the book I need and then run my fingers over the books nearby that might also be interesting. Sometimes I see what I expect to find, but sometimes I go places I never expected. I like the idea of “wasting time” productively. I like learning things that seem to be the product of a disciplined mind even if they have no apparent value to me at the time. Casting a wide net can pull in some big surprises.\nThis book would never have been written had I not in graduate school stumbled across Ley’s (1972) Quantitative aspects of psychological assessment while wandering the stacks at Evans library at Texas A&M University. The book was a little quirky, but I like quirky. I especially admire the book’s blend of clarity, practicality, and depth. Why did I write my own book instead of recommending that clinicians download and use Ley’s now freely available book? Well, I do recommend reading Ley’s book. In contrast to my approach, Ley often takes time to gently lay out mathematical proofs of many ideas. Thus Ley’s book is a wonderful and beginner-friendly introduction to the not-famous-for-being-accessible corpus of academic writings on psychometrics.\n\nLey, P. (1972). Quantitative aspects of psychological assessment: An introduction. Duckworth.\n\nR Core Team. (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/\n\nSchneider, W. J. (2023). Psycheval: A psychological evaluation toolkit. https://github.com/wjschne/psycheval\nI wanted to present much of the same material as Ley does but with more of an eye to application. I also wanted to present many ideas not included in Ley’s book. In addition, I chose to write this book because I believe that Ley had the right idea but that in an era in which no one had a home computer, few clinicians would have the knowledge, motivation, and stamina to use his equations on a regular basis. Now that computers are used by all clinicians, equations like those presented by Ley can be be made easy to use. All of the “tools” in this toolkit are freely available in various packages programming environment (R Core Team, 2022). Any tools not elsewhere available have been collected in psycheval (Schneider, 2023), a package specifically developed for users of this book."
  },
  {
    "objectID": "variables.html#nominal",
    "href": "variables.html#nominal",
    "title": "2  Variables",
    "section": "2.1 Nominal Scales",
    "text": "2.1 Nominal Scales\nIn a nominal scale, we note only that some things are different from others and that they belong to two or more mutually exclusive categories. If we say that a person has Down syndrome (trisomy 21), we are implicitly using a nominal scale in which there are people with Down syndrome and people without Down syndrome (as in Figure 2.2). In a true nominal scale, there are no cases that fall between categories. To be sure, we might have some difficulty figuring out and reliably agreeing upon the category to which something belongs—but there is no conceptual space between categories.A nominal scale groups observations into unordered categories.In mutually exclusive categories nothing belongs to more than one category (at the same time and in the same sense).\n\n\n\n\n\n\nFigure 2.2: A (nearly) true dichotomy\n\n\n\n\n\n \\rm\\LaTeX~Code for Figure 2.2\n\n\n% A (nearly) true dichotomy\n\n\\documentclass[tikz = true, border=0mm]{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{arrows,shapes}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}[SmallCapsFont={Equity Caps A}]\n\\definecolor{firebrick4}{HTML}{8B1A1A}\n\\definecolor{royalblue4}{RGB}{39,64,139}\n\\definecolor{myviolet}{HTML}{51315E}\n\n\\begin{document}\n    \\begin{tikzpicture}\n    [node distance=1.1cm and 1.1cm,\n    post/.style={->,\n                 black!70,\n                 shorten >=2pt,\n                 shorten <=2pt,\n                 >=latex',\n                 very thick,\n                 font=\\large},\n    ob/.style={rectangle,\n               inner sep=1mm,\n               minimum width=2.5cm,\n               minimum height=2.5cm,\n               rounded corners=0.75mm,\n               font=\\large,\n               text = white}]\n    \\node[shape=diamond,\n          fill=myviolet,\n          rounded corners=0.5mm,\n          shape aspect=1.25,\n          text = white,\n          font = \\large] (Trisomy) at (0,0) {Trisomy 21?};\n    \\node [ob,\n           fill=firebrick4,\n           left=of Trisomy,\n           align=center] (Down) {Down\\\\Syndrome};\n    \\node [ob,\n          fill=royalblue4,\n          right=of Trisomy,\n          align=center] (NoDown) {No Down\\\\Syndrome};\n    \\draw[post] (Trisomy) -- (Down) node [pos=.46,above] {Yes};\n    \\draw[post] (Trisomy) -- (NoDown) node [pos=.46,above] {No};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\nIn the messy world of observable reality, few true nominal variables exist as defined here. Most so-called nominal variables in psychology are merely nominal-ish. With respect to Down syndrome, we could say that people either have three copies of the 21st chromosome or they do not. However, if we did say that—and meant it—we’d be wrong. In point of fact, there are many cases of partial trisomy. There are other cases of Down syndrome in which part of chromosome 21 is copied to another chromosome. However, because cases like this are sufficiently rare and because such distinctions are usually not of vital importance, Down Syndrome is treated as if it were a true nominal variable. Even though Down Syndrome might technically come in degrees (both phenotypically and in terms of the underlying chromosomal abnormalities), the distinction between having the condition and not having it is not arbitrary.\n\n\n\n\n\n\nFigure 2.3: A (nearly) true dichotomy\n\n\n\n\n\n R Code for Figure 2.3\n\n\n# A dichotomized continuum\nggplot() + \n  # geom_vline(xintercept = 70, linetype = \"dashed\") + \n  stat_function(xlim = c(40, 70), \n                fun = dnorm, \n                args = list(mean = 100, sd = 15), \n                geom = \"area\", \n                fill = myfills[2], \n                n = 1000) + \n  stat_function(xlim = c(70, 160), \n                fun = dnorm, \n                args = list(mean = 100, sd = 15), \n                geom = \"area\", \n                fill = myfills[1], \n                n = 1000) + \n  geom_segment(data = tibble(x = c(71, 69), \n                             xend = c(160, 40), \n                             y = dnorm(100, 100, 15) * 1.024),\n               aes(x = x, \n                   y = y, \n                   xend = xend, \n                   yend = y), \n               color = myfills[1:2], \n               linejoin = \"mitre\",\n               arrow = arrow(type = \"closed\", \n                             angle = 15, \n                             length = unit(2.25, \"mm\"))) + \n  geom_text(data = tibble(x = c(100, 55), \n                          y = dnorm(100, 100, 15) * 1.02,\n                          l = c(\"No Intellectual\\nDisability\", \n                                \"Intellectual\\nDisability\")),\n            aes(x = x, \n                y = y,\n                label = l),\n            size = ggtext_size(bsize),\n            vjust = -0.3, \n            lineheight = 1,\n            color = myfills[1:2]) +\n  annotate(\"segment\", \n           x = 70, \n           xend = 70, \n           linewidth = 0.25,\n           y = dnorm(70, 100, 15), \n           yend = dnorm(100, 100, 15) * 1.14,\n           linetype = \"dashed\",\n           color = \"gray30\") +\n  scale_x_continuous(\"IQ and Adaptive Functioning\", \n                     breaks = seq(40, 160, 15),\n                     expand = expansion(0)) + \n  scale_y_continuous(NULL, breaks = NULL, \n                     expand = expansion(c(.02,0))) + \n  # theme_minimal(base_size = bsize, base_family = bfont) + \n  ggthemes::theme_tufte(base_size = bsize) +\n  theme(\n    axis.ticks.x = element_line(linewidth = 0.25),\n        # axis.line.x = element_line(size = 0.25),\n        # panel.grid = element_blank()\n        ) +\n  coord_cartesian(xlim = c(40, 161), clip = \"off\") +\n  ggthemes::geom_rangeframe(data = tibble(x = c(40, 160)), aes(x = x), size = 0.25)\n\n\n\n\nIn contrast, consider the diagnosis of intellectual disability. We might have only two categories in our coding scheme (Intellectual Disability: Yes or No), but it is widely recognized that the condition comes in degrees (e.g., none, borderline, mild, moderate, severe, and profound). Thus, intellectual disability is not even conceptually nominal. It is a continuum that has been divided at a convenient but mostly arbitrary point (Figure 2.3). Distinguishing between a dichotomous  variable that is nominal by nature and one that has an underlying continuum matters because there are statistics that apply only to the latter type of variable, such as the tetrachoric correlation coefficient (Pearson & Heron, 1913). Even so, in many procedures, the two types of dichotomies can be treated identically (e.g., comparing the means of two groups with an independent-samples t-test).A dichotomy is a division of something into two categories.\nPearson, K., & Heron, D. (1913). On Theories of Association. Biometrika, 9(1/2), 159–315. https://doi.org/10.2307/2331805\n\nWhat if the categories in a nominal scale are not mutually exclusive? For example, suppose that we have a variable in which people can be classified as having either Down syndrome or Klinefelter syndrome (a condition in which a person has two X chromosomes and one Y chromosome). Obviously, this is a false dichotomy. Most people have neither condition. Thus, we need to expand the nominal variable to have three categories: Down syndrome, Klinefelter syndrome, and neither. What if a person has both Down syndrome and Klinefelter syndrome? Okay, we just add a fourth category: both. This combinatorial approach is not so much a problem for some purposes (e.g., the ABO blood group system), but for many variables, it quickly becomes unwieldy. If we wanted to describe all chromosomal abnormalities with a single nominal variable, the number of combinations increases exponentially with each new category added. This might be okay if having two or more chromosomal abnormalities is very rare. If, however, the categories are not mutually exclusive and combinations are common enough to matter, it is generally easiest to make the variable into two or more nominal variables (Down syndrome: Yes or No; Klinefelter syndrome: Yes or No; Edwards syndrome: Yes or No; and so forth). Some false dichotomies are so commonly used that people know what you mean, even though they are incomplete (e.g., Democrat vs. Republican) or insensitive to people who do not fit neatly into any of the typical categories (e.g., male vs. female).In a false dichotomy, two alternatives are presented as if they are the only alternatives when, in fact, there are others available.\nWhen we list the categories of a nominal variable, the order in which we do so is mostly arbitrary. In the variable college major, no major intrinsically comes before any other. It is convenient to list the categories alphabetically, but the order will change as the names of college majors evolve and will differ from language to language. However, strict alphabetical order is not always logical or convenient. For example, in a variable such as ethnic identity, the number of possible categories is very large, and members of very small groups are given the option of writing in their answer next to the word “other.” To avoid confusion, the other category is placed at the end of the list rather than its alphabetical position."
  },
  {
    "objectID": "variables.html#ordinal-scales",
    "href": "variables.html#ordinal-scales",
    "title": "2  Variables",
    "section": "2.2 Ordinal Scales",
    "text": "2.2 Ordinal Scales\nIn an ordinal scale, things are still classified by category, but the categories have a particular order. Suppose that we are conducting behavioral observations of a child in school and we record when the behavior occurred. The precise time at which the behavior occurred (e.g., 10:38 AM) may be uninformative. If the class keeps a fairly regular schedule, it might be more helpful to divide the day into categories such as early morning, recess, late morning, lunch, and afternoon. This way it is easy to see if behavior problems are more likely to occur in some parts of the daily schedule than in others. It does not matter that these divisions are of unequal lengths or that they do not occur at precisely the same time each day. In a true ordinal variable, the distance between categories is either undefined, unspecified, or irrelevant.An ordinal scale groups observations into ordered categories.\n\n\n\n\n\n\nFigure 2.4: In a Likert scale, the distances between categories are undefined.\n\n\n\nMost measurement in psychological assessment involves ordinal scales, though in many cases ordinal scales might appear to be other types of scales. Questionnaires that use Likert scales are clearly ordinal (e.g., Figure 2.4). Even though true/false items on questionnaires might seem like nominal scales, they are usually ordinal because the answer indicates whether a person has either more or less of an attribute. That is, more and less are inherently ordinal concepts. Likewise, ability test items are ordinal, even though correct vs. incorrect might seem like nominal categories. Ability tests are designed such that a correct response indicates more ability than an incorrect response. The ordinal nature of ability test items is especially clear in cases that allow for partial credit.\n\n\n \\rm\\LaTeX~Code for Figure 2.4\n\n\n% Likert Scale\n\\documentclass[tikz,border={60pt 10pt 60pt 0pt}]{standalone}\n\\usetikzlibrary{positioning, calc, arrows}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}[SmallCapsFont={Equity Caps A}]\n\\definecolor[named]{mycolor}{RGB}{39,64,139}\n\\begin{document}\n    \\begin{tikzpicture} [\n        font=\\normalsize,\n        xtext/.style={\n            align=center,\n            font=\\Large,\n            mycolor}\n        ]\n    \\foreach \\i [count = \\n] in {Strongly Disagree,\n                                 Disagree,\n                                 Neutral,\n                                 Agree,\n                                 Strongly Agree} {\n        \\node [xtext]  (n\\n) at (0, \\n * 2) {\\i};\n    }\n\n\n    \\foreach \\i [count=\\n] in {2,3,4,5}  {\n        \\coordinate (mid) at ($(n\\n)!.5!(n\\i)$);\n        \\draw[mycolor, very thick] (n\\n.north) -- (n\\i.south);\n        \\node[xshift = 6pt] at (mid) {?};\n}\n\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\nSome scales are only partially ordinal. For example, educational attainment is ordinal up to a certain point in most societies, but branches out as people acquire specialized training. For a career in psychology, the educational sequence is high school diploma, associate’s degree, bachelor’s degree, master’s degree, and doctoral degree.1 However, this is not the sequence for real estate agents, hair stylists, and pilots. If we wanted to compare educational degrees across professions, how would we rank them? For example, how would we compare a law degree with a doctorate in geology? Does one degree indicate higher educational attainment than the other? The answers depend on the criteria that we care about—and different people care about different things. Thus, it is difficult to say that we have an ordinal scale when we compare educational attainment across professions.1 Obviously, some of these degrees can be skipped and the endpoint is different for different careers in psychology. Furthermore, not all degrees fit neatly in this sequence (e.g., the school psychology specialist degree).\nLike educational attainment, many psychological traits are more differentiated at some points in the continuum than at others. For example, as seen in Figure 2.5, it sometimes convenient to lump the various flavors of trait anxiety together at the low and middle range of distress and then to distinguish among them at the high end. It is difficult to say who is more anxious, a person who is extremely paranoid 2 or a person with a severe case of panic disorder. We can say that each is more anxious than the average person (an ordinal comparison) but each has a qualitatively different kind of anxiety. This problem is easily solved by simply talking about two different scales (paranoia and panic). However, there are different kinds of paranoia (e.g., different mixtures of hostility, fear, and psychosis) and different kinds of panic (e.g., panic vs. fear of panic). One can always divide psychological variables into ever narrower categories, making comparisons among and across related constructs problematic. At some point, we gloss over certain qualitative differences and treat them as if they were comparable, even though, strictly speaking, they are not.2 Although paranoia is not traditionally considered a type of anxiety, it is clear that anxiety (about the possibly malevolent intentions of others) is a core feature of the trait.\n\n\n\n\n\n\nFigure 2.5: Anxiety is more differentiated at higher levels of distress\n\n\n\n\n\n R Code for Figure 2.5\n\n\n# Anxiety is more differentiated at higher levels of distress\nbind_rows(\n  tibble(label = \"Paranoia\",\n         color = myfills[2],\n         x = c(0, 7, 8, 10),\n         y = c(4.8, 4.8, 3.5, 3.5)),\n  tibble(label = \"Obsession\",\n         color = colorspace::lighten(myfills[2], amount = 0.02),\n         x = c(0, 7, 8, 10),\n         y = c(4.9, 4.9, 4.5, 4.5)),\n  tibble(label = \"Worry\",\n         color = colorspace::lighten(myfills[1], amount = 0.02),\n         x = c(0, 7, 8, 10),\n         y = c(5.1, 5.1, 5.5, 5.5)),\n  tibble(label = \"Panic\",\n         color = myfills[1],\n         x = c(0, 7, 8, 10),\n         y = c(5.2, 5.2, 6.5, 6.5))) %>% \n  mutate(color = scales::col2hcl(color, c = 100 * ((x / 10) ^ 3))) %>%\n  ggplot(aes(x,y)) + \n  ggforce::geom_bezier2(aes(group = label, color = color)) + \n  geom_text(aes(label = label, color = color), \n            hjust = 0,\n            data = . %>% filter(x == 10), \n            nudge_x = 0.1,\n            size = ggtext_size(30)) + \n  annotate(\"segment\", \n           linewidth = 1,\n           x = 0, \n           y = 5, \n           xend = 10.5, \n           yend = 5, \n           arrow = arrow(12, type = \"closed\",), \n           linejoin = \"mitre\",\n           color = \"gray20\") + \n  annotate(geom = \"label\", \n           x = 6, \n           y = 5, \n           label = \"Distress Level\", \n           size = ggtext_size(30, 1), \n           label.size = 0, \n           color = \"gray20\") +\n  theme_void() + \n  theme(legend.position = \"none\") + \n  scale_color_identity() + \n  scale_x_continuous(expand = expansion(c(0, 0.21))) +\n  scale_y_continuous(expand = expansion(c(0.07, 0.03)))"
  },
  {
    "objectID": "variables.html#interval-scales",
    "href": "variables.html#interval-scales",
    "title": "2  Variables",
    "section": "2.3 Interval Scales",
    "text": "2.3 Interval Scales\nWith interval scales, not only are the numbers on the scale ordered, the distance between the numbers (i.e., intervals) is meaningful. A good example of an interval scale is the calendar year. The time elapsed from 1960 to 1970 is the same as the time elapsed from 1970 to 1980. In contrast, consider a standard Likert scale from a questionnaire. What is the distance between disagree and agree? Is it the same as the distance between agree and strongly agree? If it were, how would we know? In interval scales, all such mysteries disappear.In an interval scale the distance between numbers has a consistent meaning at every point on the scale.\nIt is not always easy to distinguish between an interval scale and an ordinal scale. Therapists sometimes ask clients to rate their distress “on a scale from 0 to 10.” Probably, in the mind of the therapist, the distance between each point of the scale is equal. In the mind of the client, however, it may not work that way. In Figure 2.6, a hypothetical client thinks of the distance between 9 and 10 as much greater than the distance between 0 and 1.\n\n\n\n\n\n\nFigure 2.6: Subjective units of distress may not be of equal length\n\n\n\n\n\n \\rm\\LaTeX~Code for Figure 2.6\n\n\n% Subjective Units of Distress\n\n\\documentclass[tikz = true, border = 2pt]{standalone}\n\n\\usepackage{amsmath}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{graphicx}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning, calc}\n\\usetikzlibrary{intersections}\n\\usetikzlibrary{decorations.pathreplacing}\n\\usetikzlibrary{decorations.text}\n\\usetikzlibrary{arrows,shapes,backgrounds, shadows,fadings}\n\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}[SmallCapsFont={Equity Caps A}]\n\n\\begin{document}\n\n\\begin{tikzpicture}[scale=1]\n\\definecolor{firebrick2}{RGB}{205,38,38};\n\\definecolor{royalblue2}{RGB}{67,110,238};\n\\pgfmathtruncatemacro{\\T}{10}\n\\tikzstyle{every node}=[draw=none,\n                        shape=circle,\n                        ball color=royalblue2,\n                        minimum size=5.5mm,\n                        white];\n\\foreach \\n in {0,...,\\T} \\node (\\n) at (0,\\n) {\\footnotesize{\\n}};\n\\foreach \\n [remember=\\n as \\lastn (initially 0)] in {0,...,\\T}\n\\draw (\\lastn) -- (\\n);\n\\tikzstyle{every node}=[draw=none,\n                        shape=circle,\n                        ball color=firebrick2,\n                        minimum size=5.5mm, white];\n\\def\\myarray{{0,0.63,1.45,2.36,3.33,4.35,5.42,6.52,7.65,8.81,11}};\n\\foreach \\x [count=\\xi] in {0,...,10}\n\\node (\\x) at (1,\\myarray[\\x]) {\\footnotesize{\\x}};\n\\foreach \\x [remember=\\x as \\lastx (initially 0)] in {0,...,\\T}\n\\draw (\\lastx) -- (\\x);\n\\tikzstyle{every node}=[align=left, font=\\small];\n\\node at (-1.8,5) {Scale you intend\\\\ your client to use\\\\ (equal intervals)};\n\\node at (2.8,5) {Scale your client\\\\ is actually using\\\\ (for now, at least)};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\nIt is doubtful that any subjectively scaled measurement is a true interval scale. Even so, it is clear that some ordinal scales are more interval-like than others. Using item response theory (Embretson & Reise, 2000), it is possible to sum many ordinal-level items and scale the total score such that it approximates an interval scale. It is important to note, however, that item response theory does not accomplish magic. The application of item response theory in this way is justified only if the ordinal items are measuring an underlying construct that is by nature at the interval (or ratio) level. No amount of statistical wizardry can alter the nature of the underlying construct. Sure, you can apply fancy math to the numbers, but a construct that is ordinal by nature will remain ordinal no matter what you do or convince yourself that you have done.\n\nEmbretson, S. E., & Reise, S. P. (2000). Item response theory for psychologists. Lawrence Erlbaum.\n\nMichell, J. (2004). Item response models, Pathological science and the shape of error: Reply to Borsboom and Mellenbergh. Theory & Psychology, 14(1), 121–129.\n\nMichell, J. (1997). Quantitative science and the definition of measurement in psychology. British Journal of Psychology, 88(3), 355–383. https://doi.org/10.1111/j.2044-8295.1997.tb02641.x\n\nMichell, J. (2008). Is psychometrics pathological science? Measurement, 6(1-2), 7–24.\nMost of the tools used in psychological assessment make use of ordinal scales and transform them such that they are treated as if they were interval scales. Is this defensible? Yes, a defense is possible (e.g., Michell, 2004) but not all scholars will be convinced by it (Michell, 1997, 2008). As an act of faith, I will assume that most of the scales used in psychological assessment (measures of abilities, personality traits, attitudes, interests, motivation, and so forth) are close enough to interval scales that they can be treated as such. In many instances, my faith may be misplaced, but where exactly can only be determined by high quality evidence. While I await such evidence, I try to balance my faith with moderate caution."
  },
  {
    "objectID": "variables.html#ratio-scales",
    "href": "variables.html#ratio-scales",
    "title": "2  Variables",
    "section": "2.4 Ratio Scales",
    "text": "2.4 Ratio Scales\nIn a ratio scale, zero represents something special: the absence of the quantity being measured. In an interval scale, there may be a zero, but the zero is just another number in the scale. For example, 0°C happens to be the freezing point of water at sea level but it does not represent the absence of heat.3 Ratio scales do not usually have negative numbers, but there are exceptions. For example, in a checking account balance, negative numbers indicate that the account is overdrawn. Still, a checking account balance is a true ratio scale because a zero indicates that there is no money in the account.A ratio scale has a true zero, in addition to all the properties of an interval scale.3 The absence of heat occurs at −273.15°C or 0°K.\nWhat does a true zero have to do with ratios? In interval scales, numbers can be added and subtracted but they cannot be sensibly divided. Why not? Because when you divide one number by another, you are creating a ratio. A ratio tells you how big one number is compared to another number. Well, how big is any number? The magnitude of a real number is its distance from zero (i.e., its absolute value). If zero is not a meaningful number on a particular scale, then ratios computed from numbers on that scale will not be meaningful. Therefore, because interval scales do not have a true zero, meaningful ratios are not possible. For example, although 20°C is twice as far from 0°C as 10°C, it does not mean that 20°C is twice as hot as 10°C. In contrast, these types of comparisons are possible on the Kelvin scale because 0°K is a true zero representing the complete absence of heat. That is, 20°K really is twice as hot as 10°K.\nIn psychological assessment, there are a few true ratio scales that are commonly used. Whenever anything is counted (e.g., counting how often a behavior occurs in a direct observation), it is a ratio scale. However, treating counts of behavior as ratio scales can be tricky. If I observe how many times a child speaks out of turn in class, and I use this as an index of impulsivity, it is no longer a ratio scale. Why? The actual variable, number of outbursts is a true ratio variable because 0 outbursts means the absence of outbursts. However, if I use the number of outbursts as a proxy variable for impulsivity, then 0 outbursts probably does not indicate the absence of impulsivity. At best it indicates lower levels of impulsivity. We can observed two children with 0 outbursts yet imagine that one child is still more impulsive than the other. This same problem exists for the measurement of reaction times. Reaction time is a true ratio scale because a reaction time of 0 means that no time has elapsed between the onset of the stimulus and the response. However, reaction time data used in clinical applications are often proxies for traits that are interval-level concepts, such as inattention on a continuous performance test. Why are psychological traits such as cognitive abilities, personality traits, and so forth interval-level concepts? Because we do not yet have any means of defining what, for example, zero intelligence or zero extroversion would look like. Attempts have been made (Jensen, 2006), but they have not yet proved persuasive.\n\nJensen, A. R. (2006). Clocking the mind: Mental chronometry and individual differences. Elsevier Science Limited."
  },
  {
    "objectID": "variables.html#sec:DiscreteVsContinuous",
    "href": "variables.html#sec:DiscreteVsContinuous",
    "title": "2  Variables",
    "section": "2.5 Discrete vs. Continuous Variables",
    "text": "2.5 Discrete vs. Continuous Variables\nInterval and ratio variables can be either discrete or continuous. Discrete variables can assume some values but not others. Once the list of acceptable values has been specified, there are no cases that fall between those values. For example, the number of bicycles a person owns is a discrete variable because the variable can assume only the non-negative integers. Fractions of bicycles are not considered. Discrete variables often take on integer values, but any set of exact, isolated values can be used in a discrete variable. For example, one could specify a discrete variable that is divided at every half point: 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, ….A discrete variable can only take on exact, isolated values from a specified list.\nWhen a variable can assume any value within a specified interval, the variable is said to be continuous. With a continuous variable, we can use fractions and decimals to achieve any level of precision that we desire. In Figure 2.7, blue circles present the set of integers from 0 to 10. No fractions between the integers occur in the variable. By contrast, the solid red line from 0 to 10 represents the values that can occur in a continuous variable. Any and all fractional values are possible, but in practice we must round to a feasibly useful level of precision.A continuous variable can take on any value within a specified range.\n\n\n\n\n\nFigure 2.7: Discrete variables have gaps whereas continuous variables have none.\n\n\n\n\n\n\n \\rm\\LaTeX~Code for Figure 2.7\n\n\n% Discrete Continuous\n\\documentclass[tikz = true, border = 2pt]{standalone}\n\n\\usepackage{amsmath}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{graphicx}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning, calc}\n\\usetikzlibrary{intersections}\n\\usetikzlibrary{decorations.pathreplacing}\n\\usetikzlibrary{decorations.text}\n\\usetikzlibrary{arrows,shapes,backgrounds, shadows,fadings}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}[SmallCapsFont={Equity Caps A}, Numbers=Lining]\n\\begin{document}\n\n\\begin{tikzpicture}[xscale=0.8]\n\\definecolor{firebrick2}{RGB}{205,38,38};\n\\definecolor{royalblue2}{RGB}{67,110,238};\n\\node [anchor=west] at (10.5,1) {Discrete};\n\\node [anchor=west] at (10.5,1.75) {Continuous};\n\\foreach \\n in {0,...,10} {\n    \\node at (\\n,0.15) {\\n};\n    \\draw (\\n,0.35)--(\\n,0.45);\n}\n\\tikzstyle{every node}=[draw=none,\n                        shape=circle,\n                        ball color=royalblue2,\n                        minimum size=1mm];\n\\foreach \\n in {0,...,10} {\n    \\node at (\\n,1) {};\n}\n\\draw (0,0.45) --(10,0.45);\n\\tikzstyle{every node}=[draw=none,\n                        shape=circle,\n                        ball color=firebrick2,\n                        minimum size=1mm];\n\\draw [color=firebrick2,ultra thick] (0,1.75)--(10,1.75);                        \n\\node (r1) at (0,1.75) {};\n\\node  (r10)  at (10,1.75) {};\n\n\\end{tikzpicture}\n\\end{document}"
  },
  {
    "objectID": "distributions.html#random-variables",
    "href": "distributions.html#random-variables",
    "title": "3  Distributions",
    "section": "3.1 Random Variables",
    "text": "3.1 Random Variables\nBecause we first learn about variables in an algebra class, we tend to think of variables as having values that can be solved for—if we have enough information about them. If I say that x is a variable and that x+6=8, we can use algebra to find that x must equal 2.\nRandom variables are not like algebraic variables. Random variables simply take on values because of some random process. If we say that the outcome of a throw of a six-sided die is a random variable, there is nothing to “solve for.” There is no equation that determines the value of the die. Instead, it is determined by chance and the physical constraints of the die. That is, the outcome must be one of the numbers printed on the die, and the six numbers are equally likely to occur. This illustrates an important point. The word random here does not mean “anything can happen.” On a six-sided die, you will never roll a 7, 3.5, \\sqrt{\\pi}, −36,000, or any other number that does not appear on the six sides of the die. Random variables have outcomes that are subject to random processes, but those random processes do have constraints on them such that some outcomes are more likely than others—and some outcomes never occur at all.Random variables have values that are determined by a random process.\n\n\n\n\n\n\nFigure 3.1: Rolling a six-sided die is a process that creates a randomly ordered series of integers from 1 to 6.\n\n\n\nWhen we say that the throw of a six-sided die is a random variable, we are not talking about any particular throw of a particular die but, in a sense, every throw (that has ever happened or ever could happen) of every die (that has ever existed or could exist). Imagine an immense, roaring, neverending, cascading flow of dice falling from the sky. As each die lands and disappears, a giant scoreboard nearby records the relative frequencies of ones, twos, threes, fours, fives, and sixes. That’s a random variable.\n\n\n R Code for Figure 3.1\n\n\n# Function to make dice\nmakedice <- function(i, id) {\n  x = switch(\n           i,\n           `1` = 0,\n           `2` = c(-1, 1),\n           `3` = c(-1, 1, 0),\n           `4` = c(-1, 1, -1, 1),\n           `5` = c(-1, 1, -1, 1, 0),\n           `6` = c(-1, 1, -1, 1, -1, 1)\n         )\n  y = switch(\n           i,\n           `1` = 0,\n           `2` = c(1,-1),\n           `3` = c(1,-1, 0),\n           `4` = c(1,-1,-1, 1),\n           `5` = c(1,-1,-1, 1, 0),\n           `6` = c(1,-1,-1, 1, 0, 0))\n  \n  tibble(id = id * 1,\n         i = i,\n         x = x,\n         y = y) %>% \n    add_case(id = id + 0.5,\n             i = 0,\n             x = NA,\n             y = NA)\n}\n\n# Die radius\nr <- 0.35\ndpos <- 1.5\n\n# Die round arcs\ndround <- tibble(x0 = c(-dpos, dpos, -dpos, dpos), \n                 y0 = c(dpos,dpos,-dpos,-dpos), \n                 r = r, \n                 start = c(-pi / 2, pi / 2, -pi, pi / 2) , \n                 end = c(0,0, -pi / 2,pi) )\n\n# Line segments\ndsegments <- tibble(x = c(-dpos, dpos + r, dpos, -dpos - r), \n                    y = c(dpos + r,dpos,-dpos - r,-dpos),\n                    xend = c(dpos, dpos + r, -dpos,-dpos - r), \n                    yend = c(dpos + r,-dpos,-dpos - r, dpos))\n# Number of throws\nk <- 50\n\n# Dot positions\nd <- map2_df(sample(1:6,k, replace = T), 1:k, makedice)  \n\n# Plot\np <- ggplot(d) + \n  geom_point(pch = 16, size = 50, aes(x,y))  + \n  geom_arc(aes(x0 = x0,\n               y0 = y0,\n               r = r,\n               start = start,\n               end = end,\n               linetype = factor(r)),\n           data = dround,\n           linewidth = 2) + \n  geom_segment(data = dsegments,\n               aes(x = x, y = y, xend = xend, yend = yend),\n               linewidth = 2) +\n  coord_equal() + \n  theme_void() + \n  theme(legend.position = \"none\") + \n  transition_manual(id) \n\n# Render animation\nanimate(p, \n        fps = 1,\n        device = \"svg\",\n        renderer = magick_renderer(), \n        width = 8,\n        height = 8)"
  },
  {
    "objectID": "distributions.html#sets",
    "href": "distributions.html#sets",
    "title": "3  Distributions",
    "section": "3.2 Sets",
    "text": "3.2 Sets\nA set refers to a collection of objects. Each distinct object in a set is an element.A set is a collection of distinct objects.An element is a distinct member of a set.\n\n3.2.1 Discrete Sets\nTo show that a list of discrete elements is a discrete set, we can use curly braces. For example, the set of positive single-digit even numbers is \\{2, 4, 6, 8\\}. With large sets with repeating patterns, it is convenient to use an ellipsis (“…”), the punctuation mark signifying an omission or pause. For example, rather than listing every two-digit positive even number, we can show the pattern like so:A discrete set has numbers that are isolated, meaning that each number has a range in which it is the only number in the overall set.\n\\{10, 12, 14,\\ldots, 98\\}\nIf we want the pattern to repeat forever, we can set an ellipsis on the left, right, or both sides. The set of odd integers extends to infinity in both directions:\n\\{\\ldots, -5, -3, -1, 1, 3, 5, \\ldots\\}\n\n\n3.2.2 Interval Sets\nWith continuous variables, we can define sets in terms of intervals. Whereas the discrete set \\{0,1\\} refers just to the numbers 0 and 1, the interval set (0,1) refers to all the numbers between 0 and 1.Intervals are a continous range of numbers.\n\n\n\n\n\n\nFigure 3.2: Interval Notation\n\n\n\nAs shown in Figure 3.2, some intervals include their endpoints and others do not. Intervals noted with square brackets include their endpoints and intervals written with parentheses exclude them. Some intervals extend to positive or negative infinity: (-\\infty,5] and (-8,+\\infty). Use a parenthesis with infinity instead of a square bracket because infinity is not a specific number that can be included in an interval.\n\n\n R Code for Figure 3.2\n\n\n# Interval notation\ntibble(lb = 1L,\n       ub = 5L,\n       y = 1:4,\n       meaning = c(\"includes 1 and 5\",\n                   \"excludes 1 and 5\",\n                   \"includes 1 but not 5\",\n                   \"includes 5 but not 1\"),\n       l_bracket = c(\"[\", \"(\", \"[\", \"(\"),\n       u_bracket = c(\"]\", \")\", \")\", \"]\")) %>% \n  mutate(Interval = paste0(l_bracket, lb, \",\", ub, u_bracket) %>% \n           fct_inorder() %>% \n           fct_rev,\n         l_fill = ifelse(l_bracket == \"[\", myfills[1], \"white\"),\n         u_fill = ifelse(u_bracket == \"]\", myfills[1], \"white\")) %>% \n  ggplot(aes(lb, Interval)) + \n  geom_segment(aes(xend = ub, yend = Interval), \n               linewidth = 2, \n               color = myfills[1]) +\n  geom_point(aes(fill = l_fill), \n             size = 5, \n             pch = 21, \n             stroke = 2, color = myfills[1]) + \n  geom_point(aes(fill = u_fill, x = ub), \n             size = 5, \n             pch = 21, \n             stroke = 2, \n             color = myfills[1]) +\n  geom_label(aes(label = paste(Interval, meaning), x = 3), \n             vjust = -.75, \n             label.padding = unit(0,\"lines\"), \n             label.size = 0, \n             family = bfont, \n             size = ggtext_size(27)) +\n  scale_fill_identity() +\n  scale_y_discrete(NULL, expand = expansion(c(0.08, 0.25))) +\n  scale_x_continuous(NULL, minor_breaks = NULL) +\n  theme_minimal(base_size = 27, \n                base_family = bfont) + \n  theme(axis.text.y = element_blank(), \n        panel.grid.major = element_blank(),\n        axis.line.x = element_line(linewidth = .5),\n        axis.ticks.x = element_line(linewidth = .25), \n        plot.margin = margin())"
  },
  {
    "objectID": "distributions.html#sec:SampleSpace",
    "href": "distributions.html#sec:SampleSpace",
    "title": "3  Distributions",
    "section": "3.3 Sample Spaces",
    "text": "3.3 Sample Spaces\nThe set of all possible outcomes of a random variable is the sample space. Continuing with our example, the sample space of a single throw of a six-sided die is the set \\{1,2,3,4,5,6\\}. Sample space is a curious term. Why sample and why space? With random variables, populations are infinitely large, at least theoretically. Random variables just keep spitting out numbers forever! So any time we actually observe numbers generated by a random variable, we are always observing a sample; actual infinities cannot be observed in their entirety. A space is a set that has mathematical structure. Most random variables generate either integers or real numbers, both of which are structured in many ways (e.g., order).A sample space is the set of all possible values that a random variable can assume.A population consists of all entities under consideration.A sample is a subset of a population.\nUnlike distributions having to do with dice, many distributions have a sample space with an infinite number of elements. Interestingly, there are two kinds of infinity we can consider. A distribution’s sample space might be the set of whole numbers: \\{0,1,2,...\\}, which extends to positive infinity. The sample space of all integers extends to infinity in both directions: \\{...-2,-1,0,1,2,...\\}.\nThe sample space of continuous variables is infinitely large for another reason. Between any two points in a continuous distribution, there is an infinite number of other points. For example, in the beta distribution, the sample space consists of all real numbers between 0 and 1: (0,1). Many continuous distributions have sample spaces that involve both kinds of infinity. For example, the sample space of the normal distribution consists of all real numbers from negative infinity to positive infinity: (-\\infty, +\\infty)."
  },
  {
    "objectID": "distributions.html#sec:ProbabilityDistribution",
    "href": "distributions.html#sec:ProbabilityDistribution",
    "title": "3  Distributions",
    "section": "3.4 Probability Distributions",
    "text": "3.4 Probability Distributions\nEach element of a random variable’s sample space occurs with a particular probability. When we list the probabilities of each possible outcome, we have specified the variable’s probability distribution. In other words, if we know the probability distribution of a variable, we know how probable each outcome is. In the case of a throw of a single die, each outcome is equally likely (Figure 3.3).In a probability distribution, there is an assignment of a probability to each possible element in a variable’s sample space.\n\n\n\n\n\n\nFigure 3.3: The probability distribution of a throw of a single die\n\n\n\n\n\n \\rm\\LaTeX~Code for Figure 3.3\n\n\n% Dice PMF\n\n\\documentclass[tikz = true, border = 2pt]{standalone}\n\\usepackage{tikz}\n\\usepackage{xfrac}\n\\usetikzlibrary{shapes,calc}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}\n\\tikzset{\n    dot hidden/.style={},\n    line hidden/.style={},\n    dot colour/.style={dot hidden/.append style={color=#1}},\n    dot colour/.default=black,\n    line colour/.style={line hidden/.append style={color=#1}},\n    line colour/.default=black\n}\n\n\\usepackage{xparse}\n\\NewDocumentCommand{\\drawdie}{O{}m}{\n    \\begin{tikzpicture}[x=1em,y=1em,radius=0.1,#1]\n    \\draw[rounded corners=2,line hidden] (0,0) rectangle (1,1);\n    \\ifodd#2\n    \\fill[dot hidden] (0.5,0.5) circle;\n    \\fi\n    \\ifnum#2>1\n    \\fill[dot hidden] (0.25,0.25) circle;\n    \\fill[dot hidden] (0.75,0.75) circle;\n    \\ifnum#2>3\n    \\fill[dot hidden] (0.25,0.75) circle;\n    \\fill[dot hidden] (0.75,0.25) circle;\n    \\ifnum#2>5\n    \\fill[dot hidden] (0.75,0.5) circle;\n    \\fill[dot hidden] (0.25,0.5) circle;\n    \\ifnum#2>7\n    \\fill[dot hidden] (0.5,0.75) circle;\n    \\fill[dot hidden] (0.5,0.25) circle;\n    \\fi\n    \\fi\n    \\fi\n    \\fi\n    \\end{tikzpicture}\n}\n\n\\begin{document}\n    \\begin{tikzpicture}\n    \\foreach \\n in {1,...,6} {\n        \\node at ($(0,7)-(0,\\n)$) {\\drawdie [scale = 2]{\\n}};\n        \\node [fill=gray!50,\n               minimum height = 2cm,\n               minimum width = 0.1cm,\n               single arrow,\n               single arrow head extend =.15cm,\n               single arrow head indent =.08cm,\n               inner sep=1mm] at ($(1.55,7)-(0,\\n)$) {};\n        \\node  (p1) at (3,\\n) {\\large{$\\sfrac{\\text{1}}{\\text{6}}$}};\n    }\n    \\node [text centered,\n           anchor=south,\n           text height = 1.5ex,\n           text depth = .25ex] (p3) at (0,6.6) {\\large{{Sample Space}}};\n    \\node [text centered,\n           anchor = south,\n           text height = 1.5ex,\n           text depth = .25ex] (p4) at (3,6.6) {\\large{{Probability}}};\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\nThere is an infinite variety of probability distributions, but a small subset of them have been given names. Now, one can manage one’s affairs quite well without ever knowing what a Bernoulli distribution is, or what a \\chi{^2} distribution is, or even what a normal distribution is. However, sometimes life is a little easier if we have names for useful things that occur often. Most of the distributions with names are not really single distributions, but families of distributions. The various members of a family are unique but they are united by the fact that their probability distributions are generated by a particular mathematical function (more on that later). In such cases, the probability distribution is often represented by a graph in which the sample space is on the X-axis and the associated probabilities are on the Y-axis. In Figure 3.4, 16 probability distributions that might be interesting and useful to clinicians are illustrated. Keep in mind that what are pictured are only particular members of the families listed; some family members look quite different from what is shown in Figure 3.4.\n\n\n\n\n\nFigure 3.4: A gallery of useful distributions\n\n\n\n\n\n\n R Code for Figure 3.4\n\n\n# A gallery of useful distributions\n# Run output file pdfIllustration.tex in LaTeX\n# pdflatex --enable-write18 --extra-mem-bot=10000000 --synctex=1 pdfIllustration.tex\ntikzpackages <- paste(\n  \"\\\\usepackage{tikz}\",\n  \"\\\\usepackage{amsmath}\",\n  \"\\\\usepackage[active,tightpage,psfixbb]{preview}\",\n  \"\\\\PreviewEnvironment{pgfpicture}\",\n  collapse = \"\\n\"\n)\n\ntikzDevice::tikz('pdfIllustration.tex',\n                 standAlone = TRUE, \n                 packages = tikzpackages, \n                 width = 11, \n                 height = 11)\npar(\n  mar = c(1.75, 1.3, 1.75, 0),\n  mfrow = c(4, 4),\n  las = 1,\n  xpd = TRUE,\n  family = 'serif',\n  pty = \"s\",\n  mgp = c(2, 0.5, 0),\n  tcl = -0.3,\n  cex = 1.35\n)\n\n# Bernoulli\nplot(\n  c(0.2, 0.8) ~ seq(0, 1),\n  type = \"b\",\n  ylim = c(0, 1),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Bernoulli\",\n  lty = 3,\n  pch = 19,\n  xaxp = c(0, 1, 1),\n  xlim = c(-0.1, 1)\n)\ntext(x = .7, y = .8, \"$p=0.8$\")\n\n# Binomial\nplot(\n  dbinom(seq(0, 5), 5, 0.2) ~ seq(0, 5),\n  type = \"b\",\n  xlim = c(-0.1, 5),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Binomial\",\n  lty = 3,\n  pch = 19\n)\ntext(\n  x = c(3.5, 3.5),\n  y = c(.35, .25),\n  c(\"$p=0.2$\", \"$n=5$\"),\n  adj = 0\n)\n\n# Poisson\nplot(\n  dpois(0:10, 3) ~ seq(0, 10),\n  type = \"b\",\n  xlim = c(-0.1, 10),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Poisson\",\n  lty = 3,\n  pch = 19\n)\ntext(x = 7, y = .15, r\"($\\lambda=3$)\")\n\n# Geometric\nplot(\n  dgeom(0:4, prob = 0.8) ~ seq(0, 4),\n  type = \"b\",\n  ylim = c(0, .8),\n  xlim = c(-0.1, 4),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Geometric\",\n  lty = 3,\n  pch = 19\n)\ntext(x = 2, y = .6, \"$p=0.8$\")\n\n# Discrete Uniform\nplot(\n  rep(1 / 4, 4) ~ seq(1, 4),\n  type = \"b\",\n  ylim = c(0, 1),\n  xlim = c(0, 5),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Discrete Uniform\",\n  lty = 3,\n  pch = 19\n)\ntext(\n  x = c(1, 4),\n  y = c(.5, .5),\n  c(\"$a=1$\", \"$b=4$\"),\n  adj = 0.5\n)\n\n# Continuous\nplot(\n  c(0, 1 / 3, 1 / 3, 0) ~ c(1, 1, 4, 4),\n  type = \"n\",\n  ylim = c(0, 1),\n  xlim = c(0, 5),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 2,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Continuous Uniform\"\n)\npolygon(\n  c(1, 1, 4, 4),\n  c(0, 1 / 3, 1 / 3, 0),\n  col = myfills[1],\n  xpd = FALSE,\n  border = NA\n)\ntext(\n  x = c(1, 4),\n  y = c(.5, .5),\n  c(\"$a=1$\", \"$b=4$\"),\n  adj = 0.5\n)\n\n# Normal\nx <- seq(-4, 4, 0.02)\nplot(\n  dnorm(x) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Normal\",\n  lwd = 2,\n  bty = \"n\",\n  axes = F\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dnorm(x), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = FALSE,\n  border = NA\n)\naxis(2)\n\ntext(\n  x = c(-1.5, -1.5),\n  y = c(.35, .25),\n  c(\"$\\\\mu=0$\", \"$\\\\sigma^2=1$\"),\n  adj = 1\n)\n\n\ncenter_neg <- function(x) {\n  signs <- sign(x)\n  paste0(ifelse(signs < 0,\"$\",\"\"), x, ifelse(signs < 0,\"\\\\phantom{-}$\",\"\"))\n}\n\nall_tick_labels <- function(side = 1, at, labels = at) {\n  axis(side, labels = rep(\"\",length(at)), at = at)\nfor (i in 1:length(at)) {\n  axis(side, \n       at = at[i], \n       labels = labels[i],\n       tick = F)\n  }\n}\naxis_ticks <- seq(-4,4,2)\naxis_labs <- center_neg(axis_ticks)\nall_tick_labels(1, at = axis_ticks, labels = axis_labs)\n\n\n# Student t\nx <- seq(-6, 6, 0.02)\nplot(\n  dt(x, 2) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Student's $\\\\boldsymbol{t}$\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.4),\n  axes = F\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dt(x, 2), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = FALSE,\n  border = NA\n)\ntext(x = 3, y = .3, \"$\\\\nu=2$\")\naxis(2)\naxis_ticks <- seq(-6,6,2)\naxis_labs <- center_neg(axis_ticks)\nall_tick_labels(1, at = axis_ticks, labels = axis_labs)\n\n# Chi-Square\nx <- seq(0, 40, 0.05)\nplot(\n  dchisq(x, 13) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"$\\\\boldsymbol{\\\\chi^2}$\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.1)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dchisq(x, 13), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = FALSE,\n  border = NA\n)\ntext(x = 20, y = .08, \"$k=2$\", adj = 0)\n\n# F\nx <- seq(0, 6, 0.01)\nplot(\n  df(x, 3, 120) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"$\\\\boldsymbol{F}$\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.8)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, df(x, 3, 120), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(2, 2),\n  y = c(.6, .4),\n  c(\"$d_1=3$\", \"$d_2=120$\"),\n  adj = 0\n)\n\n# Weibull\nx <- seq(6.5, 11.5, 0.01)\nplot(\n  dweibull(x, 20, 10) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Weibull\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.8)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dweibull(x, 20, 10), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(7, 7),\n  y = c(.6, .4),\n  c(\"$k=20$\", \"$\\\\lambda=10$\"),\n  adj = 0\n)\n\n# Beta\nx <- seq(0, 1, 0.01)\nplot(\n  dbeta(x, 2, 2.5) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Beta\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 2),\n  xaxp = c(0, 1, 1)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dbeta(x, 2, 2.5), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(0.75, 0.75),\n  y = c(1.75, 1.25),\n  c(\"$\\\\alpha=2$\", \"$\\\\beta=2.5$\"),\n  adj = 0\n)\n\n# Log Normal\nx <- c(seq(0, .999, 0.001), seq(1, 15, .05))\nplot(\n  dlnorm(x, 2, .5) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Log Normal\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.4)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dlnorm(x, 1, 0.5), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(7, 7),\n  y = c(.3, .2),\n  c(\"$\\\\mu=2$\", \"$\\\\sigma=0.5$\"),\n  adj = 0\n)\n\n\n# Skew Normal\nx <- seq(-4, 4, 0.01)\nplot(\n  sn::dsn(x, 2, 2.5) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Skew Normal\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.5),\n  axes = F\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, sn::dsn(\n    x,\n    xi = 1,\n    omega = 1.5,\n    alpha = -4\n  ), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(-3.9, -3.9, -3.9),\n  y = c(.45, .35, .25),\n  c(\"$\\\\xi=1$\", \"$\\\\omega=1.5$\", \"$\\\\alpha=-4$\"),\n  adj = 0\n)\n\naxis(2)\naxis_ticks <- seq(-4,4,2)\naxis_labs <- center_neg(axis_ticks)\nall_tick_labels(1, at = axis_ticks, labels =axis_labs)\n\n# Normal Mixture\nx <- seq(-4, 4, 0.01)\ny <- (dnorm(x, -2, 0.5) * .25 + dnorm(x))\nplot(\n  y ~ x,\n  type = \"n\",\n  col = \"violet\",\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Normal Mixture\",\n  lwd = 4,\n  bty = \"n\",\n  ylim = c(0, 0.5),\n  axes = F\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, y, 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dnorm(x, -2, 0.5) * .25, 0),\n  col = myfills[2],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(-0.85, -0.85),\n  y = c(.45, .35),\n  c(\"$\\\\mu_1=-2$\", \"$\\\\sigma_1^2=0.5$\"),\n  adj = 1\n)\ntext(\n  x = c(3.5, 3.5),\n  y = c(.45, .35),\n  c(\"$\\\\mu_2=0$\", \"$\\\\sigma_2^2=1$\"),\n  adj = 1\n)\naxis(2)\naxis_ticks <- seq(-4,4,2)\naxis_labs <- center_neg(axis_ticks)\nall_tick_labels(1, at = axis_ticks, labels =axis_labs)\n# Bivariate Normal\n\nx = seq(-4, 4, 0.1)\nX = fMultivar::grid2d(x)\nz = fMultivar::dnorm2d(X$x, X$y, rho = 0.6)\nZ = list(x = x,\n         y = x,\n         z = matrix(z, ncol = length(x)))\npersp(\n  Z,\n  theta = 20,\n  phi = 25,\n  col = \"royalblue1\",\n  xlab = \"\\nX\",\n  ylab = \"\\nY\",\n  zlab = \"\",\n  zlim = c(0, .20),\n  border = NA,\n  expand = .7,\n  box = FALSE,\n  ticktype = \"simple\",\n  ltheta = 0,\n  shade = 0.5,\n  main = \"Bivariate Normal\",\n  lwd = 0.5\n)\ntext(c(-.11, .32), c(-.42, -.25), c(\"X\", \"Y\"))\ntext(0, 0.25, \"$\\\\rho_{XY}=0.6$\")\ndev.off()"
  },
  {
    "objectID": "distributions.html#discrete-distrubitions",
    "href": "distributions.html#discrete-distrubitions",
    "title": "3  Distributions",
    "section": "3.5 Discrete Distrubitions",
    "text": "3.5 Discrete Distrubitions\nThe sample spaces in discrete distributions are discrete sets. Thus, in the x-axis of the probability distributions, you will see isolated numbers with gaps between each number (e.g., integers).\n\n3.5.1 Discrete Uniform Distributions\nThe throw of a single die is a member of a family of distributions called the discrete uniform distribution. It is “discrete” because the elements in the sample space are countable, with evenly spaced gaps between them. For example, there might be a sequence of 8, 9, 10, and 11 in the sample space but there are no numbers in between. It is “uniform” because all outcomes are equally likely. With dice, the numbers range from a lower bound of 1 to an upper bound of 6. In the family of discrete uniform distributions, the lower and upper bounds are typically integers, mostly likely starting with 1. However, any real number a can be the lower bound and the spacing k between numbers can be any positive real number. For the sake of simplicity and convenience, I will assume that the discrete uniform distribution refers to consecutive integers ranging from a lower bound of a and an upper bound of b.A discrete uniform distribution is a family of random variable distributions in which the sample space is an evenly spaced sequence of numbers, each of which is equally likely to occur.\nThis kind of discrete uniform distribution has a number of characteristics listed in Table 3.1. I will explain each of them in the sections that follow. As we go, I will also explain the mathematical notation. For example, a \\in \\{\\ldots,-1,0,1,\\ldots\\} means that a is an integer because \\in means is a member of and \\{\\ldots,-1,0,1,\\ldots\\} is the set of all integers.1 x \\in \\{a,a+1,\\ldots,b\\} means that the each member of the sample space x is a member of the set of integers that include a, b, and all the integers between a and b. The notation for the probability mass function and the cumuluative distribution function function will be explained later in this chapter.1 Notation note: Sometimes the set of all integers is referred to with the symbol \\mathbb{Z}.\n\n\n\n\n\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\n\nLower Bound\na \\in \\{\\ldots,-1,0,1,\\ldots\\}\n\n\nUpper Bound\nb \\in \\{a + 1, a + 2, \\ldots\\}\n\n\nSample Space\nx \\in\\{a, a + 1,\\ldots,b\\}\n\n\nNumber of points\nn=b-a+1\n\n\nMean\n\\mu=\\frac{a+b}{2}\n\n\nVariance\n\\sigma^2=\\frac{n^2-1}{12}\n\n\nSkewness\n\\gamma_1=0\n\n\nKurtosis\n\\gamma_2=-\\frac{6(n^2+1)}{5(n^2-1)}\n\n\nProbability Mass Function\nf_X(x;a,b)=\\frac{1}{n}\n\n\nCumulative Distribution Function\nF_X(x;a,b)=\\frac{x-a+1}{n}\n\n\n\nTable 3.1: Features of Discrete Uniform Distributions\n\n\n\n\n3.5.2 Parameters of Random Variables\nThe lower bound a and the upper bound b are the discrete uniform distribution’s parameters. The word parameter has many meanings, but here it refers to a characteristic of a distribution family that helps us identify precisely which member of the family we are talking about. Most distribution families have one, two, or three parameters.A parameter is a defining feature of a random variable’s probability distribution.\nIf you have taken an algebra class, you have seen parameters before, though the word parameter many not have been used. Think about the formula of a line:\n\ny=mx+b\n\nBoth x and y are variables, but what are m and b? Well, you probably remember that m is the slope of the line and that b is the y-intercept. If we know the slope and the intercept of a line, we know exactly which line we are talking about. No additional information is needed to graph the line. Therefore, m and b are the line’s parameters, because they uniquely identify the line.2 All lines have a lot in common but there is an infinite variety of lines because the parameters, the slope and the intercept, can take on the value of any real number. Each unique combination of parameter values (slope and intercept) will produce a unique line. So it is with probability distribution families. All family members are alike in many ways but they also differ because of different parameter values.2 What about other mathematical functions? Do they have parameters? Yes! Most do! For example, in the equation for a parabola (y=ax^2+bx+c), a, b, and c determine its precise shape.\nThe discrete uniform distribution (i.e., the typical variety consisting of consecutive integers) is defined by the lower and upper bound. Once we know the lower bound and the upper bound, we know exactly which distribution we are talking about.3 Not all distributions are defined by their lower and upper bounds. Indeed, many distribution families are unbounded on one or both sides. Therefore, other features are used to characterize the distributions, such as the population mean (\\mu).3 If we allow the lower bound to be any real number and the spacing to be any positive real number, the discrete uniform distribution can be specified by three parameters: the lower bound a, the spacing between numbers k (k>0), and the number of points n (n>1). The upper bound b of such a distribution would be b=a+k(n-1)\n\n\n3.5.3 Probability Mass Functions\nMany distribution families are united by the fact that their probability distributions are generated by a particular mathematical function. For discrete distributions, those functions are called probability mass functions. In general, a mathematical function is an expression that takes one or more constants (i.e., parameters) and one or more input variables, which are then transformed according to some sort of rule to yield a single number.A probability mass function is a mathematical expression that gives the probability that a discrete random variable will equal a particular element of the variable’s sample space.\nA probability mass function transforms a random variable’s sample space elements into probabilities. In Figure 3.3, the probability mass function can be thought of as the arrows between the sample space and the probabilities. That is, the probability mass function is the thing that was done to the sample space elements to calculate the probabilities. In Figure 3.3, each outcome of a throw of the the die was mapped onto a probability of ⅙. Why ⅙, and not some other number? The probability mass function of the discrete uniform distribution tells us the answer.\n\n\n\n\n\nFigure 3.5: Probability mass functions tell us how probable each sample space element is. That is, they are functions that convert samples spaces (\\boldsymbol{x}) into probabilities (\\boldsymbol{p}) according to specific parameters (\\boldsymbol{\\theta}).\n\n\n\n\n\n\n \\rm\\LaTeX~Code for Figure 3.5\n\n\n% Probability mass functions tell us how probable each sample space element is\n\n\\documentclass[tikz = true, border = 2pt]{standalone}\n\n\\usepackage{amsmath}\n\\usepackage{tikz}\n\\usetikzlibrary{decorations.text, arrows}\n\\usetikzlibrary{shapes}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}\n\n\\begin{document}\n\\begin{tikzpicture}[>=stealth,scale=0.9]\n\\definecolor{royalblue2}{RGB}{39,64,139};\n\\node [rectangle,\n       draw,\n       rounded corners= 2pt,\n       text depth=0.25ex,\n       minimum height = 7mm](ss) at (0,0) {$\\boldsymbol{x}=x_1,x_2,x_3,\\ldots,x_n$};\n\\node [rectangle,\n       draw,\n       rounded corners= 2pt,\n       text depth=0.25ex,\n       minimum height = 7mm](ps) at (7,0) {$\\boldsymbol{p}=p_1,p_2,p_3,\\ldots,p_n$};\n\\node [single arrow,\n       fill=royalblue2,\n       single arrow head extend=1.1ex,\n       transform shape,\n       minimum height=0.9cm,\n       text depth=0.25ex, text=white] (fx) at (3.5,0) {$\\quad\\;\\; f_X\\quad\\;\\;$};\n\\node [text depth=2.25ex,\n       text height= 5ex,\n       anchor=south,\n       yshift=-3.5ex](sst) at (0,0.75) {\\textbf{Sample Space}};\n\\node [text depth=2.25ex,\n       text height= 5ex,\n       anchor=south,\n       yshift=-3.5ex](pst) at (7,0.75) {\\textbf{Probabilities}};\n\\node [shape=rectangle,\n       text depth=2.25ex,\n       color=royalblue2,\n       align=center,\n       text height= 5ex,\n       anchor=south,\n       yshift=-3.5ex](pmf) at (3.5,0.75) {\\textbf{Probability}\\\\\n    \\textbf{Mass Function}};\n\\node [text depth=2.25ex,\n       text height= 5ex,\n       anchor=south,\n       yshift=-3.5ex] (pt) at (3.5,-1.5) {\\textbf{Parameters}};\n\\node [rectangle,\n       draw,\n       rounded corners = 2pt,\n       text depth=0.25ex,\n       minimum height = 7mm] (pts) at (3.5,-2.25) {$\\boldsymbol{\\theta}=\\theta_1,\\theta_2,\\theta_3,\\ldots,\\theta_k$};\n%\\node [align=center,\n%       shape=rectangle,\n%       rounded corners = 3pt,\n%       draw](formula) at (3.5,4) {\\textbf{\\Large{Scary~Math!}}\\\\\n%   $f_X\\!\\left(\\boldsymbol{x};\\boldsymbol{\\theta}\\right)=\\boldsymbol{p}$};\n%\\draw [rounded corners=5pt] (-2.5,-3) rectangle (9.5,2.65);\n\\node at (3.5,2) {$f_X\\!\\left(\\boldsymbol{x};\\boldsymbol{\\theta}\\right)=\\boldsymbol{p}$};\n\\draw[->,>=latex', very thick] (3.5,-1.15) to (3.5,-0.5);\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\nThe probability mass function of the discrete uniform distribution is fairly simple but the notation can be intimidating at first (Figure 3.5). By convention, a single random variable is denoted by a capital letter X. Any particular value of X in its sample space is represented by a lowercase x. In other words, X represents the variable in its totality whereas x is merely one value that X can take on. Confusing? Yes, statisticians work very hard to confuse us—and most of the time they succeed!\nThe probability mass function of random variable X is denoted by f_X(x). This looks strange at first. It means, “When random variable X generates a number, what is the probability that the outcome will be a particular value x?” That is, f_X(x)=P(X=x), where P means “What is the probability that…?” Thus, P(X=x) reads, “What is the probability that random variable X will generate a number equal to a particular value x?” So, f_X(7) reads, “When random variable X generates a number, what is the probability that the outcome will be 7?”\nMost probability mass functions also have parameters, which are listed after a semi-colon. In the case of the discrete uniform distribution consisting of consecutive integers, the lower and upper bounds a and b are included in the function’s notation like so: f_X(x;a,b). This reads, “For random variable X with parameters a and b, what is the probability that the outcome will be x?” Some parameters can be derived from other parameters, as was the case with the number of points n in the sample space of a discrete uniform distribution: n=b-a+1. The probability for each outcome in the sample space is the same and there are n possible outcomes. Therefore, the probability associated with each outcome is \\frac{1}{n}.\nPutting all of this together, if a and b are integers and a<b, for all n integers x between a and b, inclusive:\n\n\\begin{aligned}\nf_X\\left(x;a,b\\right)&=\\frac{1}{b-a+1}\\\\[2ex]\n&=\\frac{1}{n}\n\\end{aligned}\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\nX\nA random variable with a discrete uniform distribution\n\n\nf_X\nThe probability mass function of X\n\n\nx\nAny particular member of the sample space of X\n\n\na\nThe lower bound of the sample space\n\n\nb\nThe upper bound of the sample space\n\n\nn\nb-a+1 (The number of points in the sample space)\n\n\n\n\n\nYou might notice that x is not needed to calculate the probability. Why? Because this is a uniform distribution. No matter which sample space element x we are talking about, the probability associated with it is always the same. In distributions that are not uniform, the position of x matters and thus influences the probability of its occurrence.\n\n\n3.5.4 Cumulative Distribution Functions\nThe cumulative distribution function tells us where a sample space element ranks in a distribution. Whereas the probability mass function tells us the probability that a random variable will generate a particular number, the cumulative distribution function tells us the probability that a random variable will generate a particular number or less.A cumulative distribution function is a mathematical expression that gives the probability that a random variable will equal a particular element of the variable’s sample space or less.\n\nF_X(x) = P(X \\le x)=p\n\n\n\n\n\n\n\nFigure 3.6: The cumulative distribution function of the roll of a die is F_X(x)=\\frac{x}{6}\n\n\n\nThe cumulative distribution function of the roll of a die (Figure 3.6) tells us that the probability of rolling at least a 4 is 4⁄6 (i.e., ⅔).\n\n\n \\rm\\LaTeX~Code for Figure 3.6\n\n\n% CDF Dice\n\n\\documentclass[tikz = true, border = 2pt]{standalone}\n\\usepackage{tikz}\n\\usepackage{xfrac}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}\n\n\\definecolor{firebrick}{RGB}{205,38,38}\n\\definecolor{royalblue}{RGB}{67,110,238}\n\n\\tikzset{\n    dot hidden/.style={},\n    line hidden/.style={},\n    dot colour/.style={dot hidden/.append style={color=#1}},\n    dot colour/.default=black,\n    line colour/.style={line hidden/.append style={color=#1}},\n    line colour/.default=black\n}\n\n\\NewDocumentCommand{\\drawdie}{O{}m}{\n    \\begin{tikzpicture}[x=1em,y=1em,radius=0.1,#1]\n    \\draw[rounded corners=2,line hidden] (0,0) rectangle (1,1);\n    \\ifodd#2\n    \\fill[dot hidden] (0.5,0.5) circle;\n    \\fi\n    \\ifnum#2>1\n    \\fill[dot hidden] (0.25,0.25) circle;\n    \\fill[dot hidden] (0.75,0.75) circle;\n    \\ifnum#2>3\n    \\fill[dot hidden] (0.25,0.75) circle;\n    \\fill[dot hidden] (0.75,0.25) circle;\n    \\ifnum#2>5\n    \\fill[dot hidden] (0.75,0.5) circle;\n    \\fill[dot hidden] (0.25,0.5) circle;\n    \\ifnum#2>7\n    \\fill[dot hidden] (0.5,0.75) circle;\n    \\fill[dot hidden] (0.5,0.25) circle;\n    \\fi\n    \\fi\n    \\fi\n    \\fi\n    \\end{tikzpicture}\n}\n\n\\begin{document}\n\n\\begin{tikzpicture}\n\\foreach \\i in {1,...,6} {\n    \\node at (0.2,\\i){$\\sfrac{\\text{\\i}}{\\text{6}}$};\n    \\node at (\\i,0.2){\\large{\\i}};\n    \\foreach \\j in {1,...,6} {\n        \\ifnum \\j>\\i\n        \\node at (\\i,\\j) {\\drawdie [scale=2]{\\j}};\n        \\else\n        \\node at (\\i,\\j) {\\drawdie [scale=2,line colour=royalblue,dot colour=royalblue]{\\j}};\n        \\fi\n    }\n}\n\n\\draw [firebrick,\n       very thick,\n       rounded corners]\n       (0.55,0.5)-- ++\n       (0,1)-- ++\n       (1,0)-- ++\n       (0,1)-- ++\n       (1,0)-- ++\n       (0,1)-- ++\n       (1,0)-- ++\n       (0,1)-- ++\n       (1,0)-- ++\n       (0,1)-- ++\n       (1,0)-- ++\n       (0,1)-- ++\n       (1,0)-- ++\n       (0,-6)--cycle;\n\\node[rotate=90] at (-0.3,3.5) {{Probability}};\n\\node at (3.5,-0.2) {{Die roll is this value or less}};\n\\end{tikzpicture}\n\n\\end{document}\n\n\n\n\nThe cumulative distribution function is often distinguished from the probability mass function with a capital F instead of a lowercase f. In the case of a discrete uniform distribution consisting of n consecutive integers from a to b, the cumulative distribution function is:\n\n\\begin{align*}\nF_X(x;a,b)=\\frac{x-a+1}{b-a+1}\\\\[2ex]\n=\\frac{x-a+1}{n}\n\\end{align*}\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\nX\nA random variable with a discrete uniform distribution\n\n\nF_X\nThe cumulative distribution function of X\n\n\nx\nAny particular member of the sample space of X\n\n\na\nThe lower bound of the sample space\n\n\nb\nThe upper bound of the sample space\n\n\nn\nb-a+1 (The number of points in the sample space)\n\n\n\n\n\nIn the case of the the six-sided die, the cumulative distribution function is\n\n\\begin{aligned}\nF_X(x;a=1,b=6)&=\\frac{x-a+1}{b-a+1}\\\\[2ex]\n&=\\frac{x-1+1}{6-1+1}\\\\[2ex]\n&=\\frac{x}{6}\n\\end{aligned}\n\nThe cumulative distribution function is so-named because it adds all the probabilities in the probability mass function up to and including a particular member of the sample space. Figure 3.7 shows how the each probability in the cumulative distribution function of the roll of a six-sided die is the sum of the current and all previous probabilities in the probability mass function.\n\n\n\n\n\nFigure 3.7: The cumulative distribution function is the sum of the current and all previous elements of the probability mass function.\n\n\n\n\n\n\n R Code for Figure 3.7\n\n\n# The cumulative distribution function is the sum of the current \n# and all previous elements of the probability mass function\np <- crossing(id = 1:6,\n         x = 1:6) %>% \n  mutate(pmf = 1 / 6) %>% \n  mutate(cdf = ifelse(id < x, 1 / 6, x / 6)) %>% \n  ggplot(aes(x = x, y = cdf)) + \n  geom_segment(aes(yend = cdf - pmf, xend = x), \n               color = myfills[2]) + \n  geom_segment(aes(y = 0, yend = pmf, xend = x), \n               color = myfills[1]) + \n  geom_line(aes(y = pmf), lty = \"dotted\", color = myfills[1]) +\n  geom_line(data = . %>% dplyr::filter(x <= id), \n            lty = \"dotted\", \n            color = myfills[2] ) +\n  geom_point(aes(y = pmf),\n             color = myfills[1],\n             size = 5) +\n  geom_point(data = . %>% dplyr::filter(x <= id), \n             color = myfills[2],\n             size = 3.5) +\n  scale_x_continuous(\"Sample Space\", \n                     breaks = 1:6,\n                     expand = c(0.03,0),\n                     minor_breaks = NULL) +\n  scale_y_continuous(\"Probability\",\n                     breaks = 0:6 / 6, \n                     labels = c(0, paste0(1:5, \"/\", 6),1),\n                     expand = c(0.03,0),\n                     minor_breaks = NULL) +\n  theme_minimal(base_size = 18, base_family = bfont) +\n  coord_fixed(6) + \n  annotate(\"point\",\n           size = 5, \n           x = 1.2,  \n           y = 5.33 / 6, \n           color = myfills[1]) + \n  annotate(\"point\",\n           size = 3.5, \n           x = 1.2,  \n           y = 5.66 / 6, \n           color = myfills[2])  + \n  annotate(\"label\",\n           size = 6, \n           x = 1.33,  \n           y = 5.33 / 6, \n           color = myfills[1],\n           label = \"Probability Mass Function\",\n           hjust = 0,\n           family = bfont, \n           label.padding = unit(0, \"lines\"), \n           label.size = 0\n           ) + \n  annotate(\"label\",\n           size = 6, \n           x = 1.33,  \n           y = 5.66 / 6, \n           color = myfills[2],\n           label = \"Cumulative Distribution Function\",\n           hjust = 0,\n           family = bfont, \n           label.padding = unit(0, \"lines\"), \n           label.size = 0) +\n  transition_states(id,2,1) +\n  ease_aes(\"sine-in-out\")\n\nanimate(p, \n        device = \"svg\",\n        renderer = magick_renderer(), \n        width = 8,\n        height = 8)\n\n\n\n\n\n\n3.5.5 Quantile functions\nThe inverse of the cumulative distribution function is the quantile function. The cumulative distribution starts with a value x in the sample space and tells us p, the proportion of values in that distribution that are less than or equal to x. A quantile function starts with a proportion p and tells us the value x that splits the distribution such that the proportion p of the distribution is less than or equal to x.A quantile function tells us which value in the sample space of a random variable is greater than a particular proportion of the values the random variable generates.\n\n\n\n\n\nFigure 3.8: The quantile function is the inverse of the cumulative distribution function: Just flip the X and Y axes!\n\n\n\n\nAs seen in Figure 3.8, if you see a graph of a continuous distribution function, just flip the X and Y axes, and you have a graph of a quantile function.\n\n\n R Code for Figure 3.8\n\n\n# The quantile function is the inverse of the cumulative distribution function\n\nd <- tibble(x = seq(-4, 4, 0.01)) %>%\n  mutate(p = pnorm(x))\n\np1 <- ggplot(d, aes(x, p)) +\n  geom_path(\n    arrow = arrow(type = \"closed\",\n                  ends = \"both\",\n                  angle = 12,\n                  length = unit(0.15, \"inches\")),\n    color = myfills[1],\n    lwd = 1\n  ) +\n  scale_x_continuous(\"Sample Space (*x*)\", labels = \\(x) signs::signs(x, accuracy = 1)) +\n  scale_y_continuous(\"Proportion (*p*)\", labels = prob_label) +\n  theme_minimal(base_size = 26, base_family = bfont) +\n  coord_fixed(8) +\n  annotate(\n    \"richtext\",\n    x = -2,\n    y = .5 + .25 / 4,\n    label = paste0(\"*X* ~ \",\n                   span_style(\"N\", style = \"font-family:'Lucida Calligraphy'\"),\n                   \"(0, 1<sup>2</sup>)\"),\n    family = bfont,\n    label.size = 0,\n    label.padding = unit(0, \"mm\"),\n    size = ggtext_size(26)\n  ) +\n  ggtitle(\"Cumulative Distribution Function *F<sub>X</sub>*(*x*) = *p*\") +\n  theme(\n    plot.title = ggtext::element_markdown(size = 26 * 0.8), \n    plot.title.position = \"plot\",\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    axis.text.x = element_text(hjust = c(0.7, 0.7, 0.5, 0.5, 0.5))\n  )\n\np2 <- ggplot(d, aes(p, x)) +\n  geom_path(\n    arrow = arrow(type = \"closed\",\n                  ends = \"both\",\n                  angle = 12,\n                  length = unit(0.15, \"inches\")),\n    color = myfills[1],\n    lwd = 1\n  ) +\n  scale_y_continuous(\"Sample Space (*x*)\", labels = \\(x) signs::signs(x, accuracy = 1)) +\n  scale_x_continuous(\"Proportion (*p*)\", labels = prob_label) +\n  theme_minimal(base_size = 26, base_family = bfont) +\n  coord_fixed(1 / 8) +\n  annotate(\n    \"richtext\",\n    y = 0.5,\n    x = .25,\n    label = paste0(\"*X* ~ \",\n                   span_style(\"N\", style = \"font-family:'Lucida Calligraphy'\"),\n                   \"(0, 1<sup>2</sup>)\"),\n    family = bfont,\n    label.size = 0,\n    label.padding = unit(0, \"mm\"),\n    size = ggtext_size(26)\n  ) +\n  ggtitle(\"Quantile Function *Q*<sub>*X*</sub>(*p*) = *x*\") +\n  theme(plot.title = ggtext::element_markdown(size = 26 * 0.8), \n        plot.title.position = \"plot\",\n        axis.title.x = ggtext::element_markdown(),\n        axis.title.y = ggtext::element_markdown())\n\np1 + p2\n\n\n\n\n\n\n3.5.6 Generating a Random Sample in R\nIn R, the sample function generates numbers from the discrete uniform distribution.\n\n# n = the sample size\nn <- 6000\n# a = the lower bound\na <- 1\n# b = the upper bound\nb <- 6\n# The sample space is the sequence of integers from a to b\nsample_space <- seq(a, b)\n# X = the sample with a discrete uniform distribution\n# The sample function selects n values\n# from the sample space with replacement at random\nX <- sample(sample_space, \n            size = n, \n            replace = TRUE)\n\n\n\n\n\n\n\nFigure 3.9: Frequency distribution of a discrete uniform random variable from 1 to 6 (n = 6,000)\n\n\n\n\n\n R Code for Figure 3.9\n\n\n# Frequency distribution of a discrete uniform random variable from 1 to 6 \n\ntibble(X = factor(X)) %>% \n  group_by(X) %>% \n  summarise(Frequency = n()) %>% \n  ggplot(aes(X,Frequency, fill = X)) + \n  geom_col(width = 0.7, fill = myfills[1]) + \n  geom_label(aes(label = Frequency), \n             vjust = -0.3, \n             label.size = 0,\n             label.padding = unit(0,\"mm\"), \n             family = bfont,\n             size = 8,\n             color = \"gray30\",\n             fill = \"white\") + \n  theme_minimal(base_size = 28, \n                base_family = bfont) + \n  scale_y_continuous(\"Count\", \n                     expand = expansion(c(0,0.075)), \n                     breaks = seq(0,1000,200)) + \n  scale_x_discrete(NULL) +\n  theme(panel.grid.major.x = element_blank(), \n        legend.position = \"none\") \n\n\n\n\nThe frequencies of the random sample can be seen in Figure 3.9. Because of sampling error, the frequencies are approximately the same, but not exactly the same. If the sample is larger, the sampling error is smaller, meaning that the sample’s characteristics will tend to more closely resemble the population characteristics. In this case, a larger sample size will produce frequency counts that will appear more even in their magnitude. However, as long as the sample is smaller than the population, sampling error will always be present. With random distributions, the population is assumed to be infinitely large, and thus sampling error at best becomes negligibly small.Samples imperfectly represent the population from which they are drawn. Sampling error refers to differences between sample statistics and population parameters.\n\n\n3.5.7 Bernoulli Distributions\n\n\n\n\n\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\n\nSample Space:\nx \\in \\{0,1\\}\n\n\nProbability that x=1\np \\in {[0,1]}\n\n\nProbability that x=0\nq = 1 - p\n\n\nMean\n\\mu = p\n\n\nVariance\n\\sigma^2 = pq\n\n\nSkewness\n\\gamma_1 = \\frac{1 - 2p}{\\sqrt{pq}}\n\n\nKurtosis\n\\gamma_2 = \\frac{1}{pq} - 6\n\n\nProbability Mass Function\nf_X(x;p) = p^xq^{1 - x}\n\n\nCumulative Distribution Function\nF_X(x;p) = x+p(1 - x)\n\n\n\nTable 3.2: Features of Bernoulli DistributionsNotation note: Whereas {a,b} is the set of just two numbers, a and b, [a,b] is the set of all real numbers between a and b.\n\n\nThe toss of a single coin has the simplest probability distribution that I can think of—there are only two outcomes and each outcome is equally probable (Figure 3.10). This is a special case of the Bernoulli distribution. Jakob Bernoulli (Figure 3.11) was a famous mathematician from a famous family of mathematicians. The Bernoulli distribution is just one of the ideas that made Jakob and the other Bernoullis famous.In the Bernoulli distribution, there are only two outcomes: a “success” (1) and a “failure” (0). If a success has a probability p then a failure has a probability of q = 1 - p.\n\n\n \\rm\\LaTeX~Code for Figure 3.10\n\n\n% Coin toss Bernoulli\n\\documentclass[tikz = true, border = 2pt]{standalone}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}\n\\usepackage{tikz}\n\\usetikzlibrary{shapes}\n\\usepackage{xfrac}\n\n\\begin{document}\n        \\begin{tikzpicture}[scale=0.9]\n        \\node (H) at (0,2) {\n            \\includegraphics [width=1.5cm]{../QuarterHeads.png}\n        };\n        \\node (T) at (0,0) {\n            \\includegraphics [width=1.5cm]{../QuarterTails.png}\n        };\n        \\node [fill=gray!50,\n               minimum height=1.5cm,\n               minimum width=0.1cm,\n               single arrow,\n               single arrow head extend=.15cm,\n               single arrow head indent=.08cm,\n               inner sep=1mm] (arrowtails1) at (1.9,2) {};\n        \\node [fill=gray!50,\n               minimum height=1.5cm,\n               minimum width=0.1cm,\n               single arrow,\n               single arrow head extend=.15cm,\n               single arrow head indent=.08cm,\n               inner sep=1mm] (arrowheads2) at (1.9,0) {};\n        \\node  (p1) at (3.4,2) {\\huge{$\\sfrac{\\text{1}}{\\text{2}}$}};\n        \\node  (p2) at (3.4,0) {\\huge{$\\sfrac{\\text{1}}{\\text{2}}$}};\n        \\node [text centered,\n               anchor=south,\n               text height=1.5ex,\n               text depth=.25ex] (p3) at (0,3) {\\large{Sample Space}};\n        \\node [text centered,\n               anchor=south,\n               text height=1.5ex,\n               text depth=.25ex] (p4) at (3.4,3) {\\large{Probability}};\n        \\end{tikzpicture}\n\n\\end{document}\n\n\n\n\n\n\n\n\n\n\nFigure 3.10: The probability distribution of a coin toss\n\n\n\n\n\n\n\n\n\nFigure 3.11: Jakob Bernoulli (1654–1705)Image Credits\n\n\n\nThe Bernoulli distribution can describe any random variable that has two outcomes, one of which has a probability p and the other has a probability q=1-p. In the case of a coin flip, p=0.5. For other variables with a Bernoulli distribution, p can range from 0 to 1.\nIn psychological assessment, many of the variables we encounter have a Bernoulli distribution. In ability test items in which there is no partial credit, examinees either succeed or fail. The probability of success on an item (in the whole population) is p. In other words, p is the proportion of the entire population that correctly answers the question. Some ability test items are very easy and the probability of success is high. In such cases, p is close to 1. When p is close to 0, few people succeed and items are deemed hard. Thus, in the context of ability testing, p is called the difficulty parameter. This is confusing because when p is high, the item is easy, not difficult. Many people have suggested that it would make more sense to call it the “easiness parameter” but the idea has never caught on.The difficulty parameter is the proportion of people who succeed on an item (or say ‘Yes’ or ‘True’ or otherwise score a 1 on a random variable with a Bernoulli distribution.).\nTrue/False and Yes/No items on questionnaires also have Bernoulli distributions. If an item is frequently endorsed as true (“I like ice cream.”), p is high. If an item is infrequently endorsed (“I like black licorice and mayonnaise in my ice cream.”), p is very low. Oddly, the language of ability tests prevails even here. Frequently endorsed questionnaire items are referred to as “easy” and infrequently endorsed items are referred to as “difficult,” even though there is nothing particularly easy or difficult about answering them either way.\n\n3.5.7.1 Generating a Random Sample from the Bernoulli Distribution\n\n\n\n\n\n\nFigure 3.12: Counts of a Random Variable with a Bernoulli Distribution (p = 0.8, n = 1000)\n\n\n\nIn R, there is no specialized function for the Bernoulli distribution because it turns out that the Bernoulli distribution is a special case of the binomial distribution, which will be described in the next section. With the function rbinom, we can generate data with a Bernoulli distribution by setting the size parameter equal to 1.\n\n# n = sample size\nn <- 1000\n# p = probability\np <- 0.8\n# X = sample\nX <- rbinom(n, size = 1, prob = p)\n# Make a basic plot\nbarplot(table(X))\n\nIn Figure 3.12, we can see that the random variable generated a sequence that consists of about 80% ones and 20% zeroes. However, because of sampling error, the results are rarely exactly what the population parameter specifies.\n\n\n R Code for Figure 3.12\n\n\n# Counts of a Random Variable with a Bernoulli Distribution\nset.seed(4)\n# n = sample size\nn <- 1000\n# p = probability\np <- 0.8\n# X = sample\nX <- rbinom(n, size = 1, prob = p)\n# Make a basic plot\nggplot(tibble(X = factor(X)), aes(X)) +\n  geom_bar(fill = myfills[1]) +\n  scale_x_discrete(NULL) + \n  scale_y_continuous(NULL, expand = expansion(c(0.01,0.1))) +\n  geom_text(aes(label = after_stat(count)), \n            stat = \"count\", \n            vjust = -0.4, \n            size = ggtext_size(18),\n            family = bfont,\n            color = \"gray10\") + \n    theme_minimal(18, bfont) + \n  theme(panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n3.5.8 Binomial Distributions\n\n\n\n\n\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\n\nNumber of Trials\nn \\in \\{1,2,3,\\ldots\\}\n\n\nSample Space\nx \\in \\{0,...,n\\}\n\n\nProbability of success in each trial\np \\in [0,1]\n\n\nProbability of failure in each trial\nq = 1 - p\n\n\nMean\n\\mu = np\n\n\nVariance\n\\sigma = npq\n\n\nSkewness\n\\gamma_1 = \\frac{1-2p}{\\sqrt{npq}}\n\n\nKurtosis\n\\gamma_2 = \\frac{1}{npq} - \\frac{6}{n}\n\n\nProbability Mass Function\nf_X(x;n,p)=\\frac{n!}{x!\\left(n-x\\right)!}p^x q^{n-x}\n\n\nCumulative Distribution Function\nF_X(x;n,p)=\\sum_{i=0}^{x}{\\frac{n!}{i!(n-i)!} p^i q^{n-i}}\n\n\n\nTable 3.3: Features of Binomial Distributions\n\n\n\n\n\n\n\n\nFigure 3.13: Probability distribution of the number of heads observed when two coins are tossed\n\n\n\nLet’s extend the idea of coin tosses and see where it leads. Imagine that two coins are tossed at the same time and we count how many heads there are. The outcome we might observe will be zero, one, or two heads. Thus, the sample space for the outcome of the tossing of two coins is the set \\{0,1,2\\} heads. There is only one way that we will observe no heads (both coins tails) and only one way that we will observe two heads (both coins heads). In contrast, as seen in Figure 3.13, there are two ways that we can observe one head (heads-tails & tails-heads).\n\n\n \\rm\\LaTeX~Code for Figure 3.13\n\n\n% Probability distribution of the number of heads observed when two coins are tossed\n\\documentclass[tikz = true,border = 2pt]{standalone}\n\\usepackage{tikz}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}\n\\definecolor[named]{fillColor}{RGB}{39,64,139}\n\\begin{document}\n\\begin{tikzpicture}[x=1pt,\n                      y=1pt,\n                      xscale=.4,\n                      yscale=.66,\n                      axisline/.style={\n                        draw=black!70,\n                        line width= 0.4pt,\n                        line join=round,\n                        line cap=round},\n                      axislabel/.style={\n                        text=black!70,\n                        inner sep=0pt,\n                        font=\\footnotesize,\n                        outer sep=0pt,\n                        anchor=east},\n                      xaxislabel/.style={\n                        text=black!70,\n                        inner sep=0pt,\n                        font=\\footnotesize,\n                        outer sep=0pt},\n                       bar/.style={\n                        draw=white,\n                        line width= 0.4pt,\n                        line join=round,\n                        line cap=round,\n                        fill=fillColor,\n                        rounded corners = 1.5pt}                ]\n\n% X-axis\n\\node[xaxislabel] at (100, 42) {0};\n\\node[xaxislabel] at (200, 42) {1};\n\\node[xaxislabel] at (300, 42) {2};\n\\node[black!70] at (204.07, 28) {Number of Heads};\n\n% Y-axis\n\\path[axisline] ( 42, 50) -- ( 42,250);\n\\path[axisline] ( 38, 50) -- ( 42, 50);\n\\path[axisline] ( 38,150) -- ( 42,150);\n\\path[axisline] ( 38,250) -- ( 42,250);\n\\node[axislabel] at ( 36, 50) {0};\n\\node[axislabel] at ( 36,150) {.25};\n\\node[axislabel] at ( 36,250) {.50};\n\n% Bars\n\\path[bar] (50, 50.00) rectangle (150,150);\n\\path[bar] (150, 50.00) rectangle (250,250);\n\\path[bar] (250, 50.00) rectangle (350,150);\n\\node at (100,  80) {\\includegraphics [width=36pt]{../QuarterTails.png}};\n\\node at (100, 120) {\\includegraphics [width=36pt]{../QuarterTails.png}};\n\\node at (200,  80) {\\includegraphics [width=36pt]{../QuarterHeads.png}};\n\\node at (200, 120) {\\includegraphics [width=36pt]{../QuarterTails.png}};\n\\node at (200, 180) {\\includegraphics [width=36pt]{../QuarterTails.png}};\n\\node at (200, 220) {\\includegraphics [width=36pt]{../QuarterHeads.png}};\n\\node at (300,  80) {\\includegraphics [width=36pt]{../QuarterHeads.png}};\n\\node at (300, 120) {\\includegraphics [width=36pt]{../QuarterHeads.png}};\n\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\nThe probability distribution of the number of heads observed when two coins are tossed at the same time is a member of the binomial distribution family. The binomial distribution occurs when independent random variables with the same Bernoulli distribution are added together. In fact, Bernoulli discovered the binomial distribution as well as the Bernoulli distribution.Two random variable are said to be independent if the outcome of one variable does not alter the probability of any outcome in the other variable.\nImagine that a die is rolled 10 times and we count how often a 6 occurs.4 Each roll of the die is called a trial. The sample space of this random variable is \\{0,1,2,...,10\\}. What is the probability that a 6 will occur 5 times? or 1 time? or not at all? Such questions are answered by the binomial distribution’s probability mass function:4 Wait! Hold on! I thought that throwing dice resulted in a (discrete) uniform distribution. Well, it still does. However, now we are asking a different question. We are only concerned with two outcomes each time the die is thrown: 6 and not 6. This is a Bernoulli distribution, not a uniform distribution, because the probability of the two events is unequal: {⅙,⅚}Every time a random variable generates a number, that instance of the variable is called a trial, which is also known as an experiment.\n\nf_X(x;n,p)=\\frac{n!}{x!\\left(n-x\\right)!}p^x\\left(1-p\\right)^{n-x}\n\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\nX\nThe random variable (the number of sixes from 10 throws of the die)\n\n\nx\nAny particular member of the sample space (i.e., x \\in \\{0,1,2,...,10\\})\n\n\nn\nThe number of times that the die is thrown (i.e., n=10)\n\n\np\nThe probability that a six will occur on a single throw of the die (i.e., p=\\frac{1}{6})\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.14: The probability distribution of the number of sixes observed when a six-sided die is thrown 10 times.\n\n\n\nBecause n=10 and p=\\frac{1}{6}, the probability mass function simplifies to:\n\\begin{equation*}\nf_X(x)=\\frac{n!}{x!\\left(n-x\\right)!}\\left(\\frac{1}{6}\\right)^x\\left(\\frac{5}{6}\\right)^{10-x}\n\\end{equation*}\nIf we take each element x of the sample space from 0 to 10 and plug it into the equation above, the probability distribution will look like Figure 3.14.\n\n\n R Code for Figure 3.14\n\n\n# Probability of sixes\n\ntibble(sample_space = 0:10) %>% \n  mutate(\n    probability = dbinom(\n      sample_space, \n      size = max(sample_space), \n      prob = 1 / 6),\n    probabiltiy_label = prob_label(probability, \n                                   digits = 2)) %>% \n  ggplot(aes(factor(sample_space), probability)) +\n  geom_col(fill = myfills[1]) + \n  ggtext::geom_richtext(\n    aes(label = probabiltiy_label),\n    size = ggtext_size(30),\n    angle = 90, \n    hjust = 0,\n    family = bfont, \n    label.margin = unit(c(0,0,0,1), \"mm\"),\n    label.padding = unit(c(1.6,0,0,0),\"mm\"), \n    label.colour = NA,\n    color = \"gray40\") +\n  theme_minimal(base_family = bfont, base_size = 30) + \n  scale_y_continuous(\"Probability\", \n                     expand = expansion(c(0,.09)), \n                     labels = prob_label) + \n  scale_x_discrete(\"Sample Space (Number of Sixes)\") + \n  theme(panel.grid.major.x = element_blank()) \n\n\n\n\n\n3.5.8.1 Clinical Applications of the Binomial Distribution\nWhen would a binomial distribution be used by a clinician? One particularly important use of the binomial distribution is in the detection of malingering. Sometimes people pretend to have memory loss or attention problems in order to win a lawsuit or collect insurance benefits. There are a number of ways to detect malingering but a common method is to give a very easy test of memory in which the person has at least a 50% chance of getting each test item correct even if the person guesses randomly.A person who malingers is pretending to be sick to avoid work or some other responsibility.\nSuppose that there are 20 questions. Even if a person has the worst memory possible, that person is likely to get about half the questions correct. However, it is possible for someone with a legitimate memory problem to guess randomly and by bad luck answer fewer than half of the questions correctly. Suppose that a person gets 4 questions correct. How likely is it that a person would, by random guessing, only answer 4 or fewer questions correctly?\nWe can use the binomial distribution’s cumulative distribution function. However, doing so by hand is rather tedious. Using R, the answer is found with the pbinom function:\n\np <- pbinom(4,20,0.5)\n\nWe can see that the probability of randomly guessing and getting 4 or fewer items correct out of 20 items total is approximately 0.006, which is so low that the hypothesis that the person is malingering seems plausible. Note here that there is a big difference between these two questions:\n\nIf the person is guessing at random (i.e., not malingering), what is the probability of answering correctly 4 questions or fewer out of 20?\nIf the person answers 4 out of 20 questions correctly, what is the probability that the person is guessing at random (and therefore not malingering)?\n\nHere we answer only the first question. It is an important question, but the answer to the second question is probably the one that we really want to know. We will answer it in another chapter when we discuss positive predictive power. For now, we should just remember that the questions are different and that the answers can be quite different.\n\n\n3.5.8.2 Graphing the binomial distribution\nSuppose that there are n=10 trials, each of which have a probability of p=0.8. The sample space is the sequence of integers from 0 to 10, which can be generated with the seq function (i.e., seq(0,10)) or with the colon operator 0:10. First, the sample space is generated (a sequence from 0 to 10.), using the seq function. The associated probability mass function probabilities are found using the dbinom function. The cumulative distribution function probabilities are found using the pbinom function.\n\n# Make a sequence of numbers from 0 to 10\nSampleSpace <- seq(0, 10)\n# Probability mass distribution for \n# binomial distribution (n = 10, p = 0.8)\npmfBinomial <- dbinom(SampleSpace, \n                      size = 10, \n                      prob = 0.8)\n# Generate a basic plot of the \n# probability mass distribution\nplot(pmfBinomial ~ SampleSpace, \n     type = \"b\")\n# Cumulative distribution function \n# for binomial distribution (n = 10, p = 0.8)\ncdfBinomial <- pbinom(SampleSpace, \n                      size = 10, \n                      prob = 0.8)\n\n\n\n\nFigure 3.15: Basic plot of a binomial probability mass function\n\n\n\n\n\n# Cumulative distribution function \n# for binomial distribution (n = 10, p = 0.8)\ncdfBinomial <- pbinom(SampleSpace, \n                      size = 10, \n                      prob = 0.8)\n# Generate a basic plot of the\n# binomial cumulative distribution function\nplot(cdfBinomial ~ SampleSpace, \n     type = \"b\")\n\n\n\n\nFigure 3.16: Basic plot of a binomial cumulative distribution function\n\n\n\n\nHowever, making the graph look professional involves quite a bit of code that can look daunting at first. However, the results are often worth the effort. Try running the code below to see the difference. For presentation-worthy graphics, export the graph to the .pdf or .svg format. An .svg file can be imported directly into MS Word or MS PowerPoint.\n\n\n\n\n\nFigure 3.17: Probability Mass Function and Cumulative Distribution Function of the Binomial Distribution (n = 10,~p = 0.8)\n\n\n\n\n\n\n R Code for Figure 3.17\n\n\n# Probability Mass Function and Cumulative Distribution \n# Function of the Binomial Distribution\ntibble(SampleSpace = 0:10,\n       pmf = dbinom(SampleSpace, 10, 0.8),\n       cdf = pbinom(SampleSpace, 10, 0.8)) %>%\n  pivot_longer(-SampleSpace, \n               names_to = \"Function\", \n               values_to = \"Proportion\") %>% \n  mutate(Function = factor(Function, \n                           levels = c(\"pmf\", \"cdf\")) ) %>%\n  arrange(desc(Function)) %>%\n  ggplot(aes(x = SampleSpace,\n             y = Proportion,\n             color = Function)) +\n  geom_pointline(lty = \"dotted\", distance = unit(3, \"mm\")) +\n  geom_point(aes(size = Function)) +\n  theme_minimal(base_family = \"serif\",\n                base_size = 18) +\n  scale_color_manual(values = myfills) +\n  scale_x_continuous(\"Sample Space\",\n                     breaks = seq(0, 10, 2),\n                     expand = expansion(0.02)) +\n  scale_y_continuous(expand = expansion(0.02),\n                     labels = prob_label) +\n  scale_size_manual(values = c(3, 4.5)) +\n  theme(legend.position = \"none\") +\n  annotate(\n    x = 10 - 0.16,\n    y = dbinom(10, 10, 0.8),\n    geom = \"label\",\n    label = \"Probability Mass Function\",\n    hjust = 1,\n    size = 4.75,\n    color = myfills[1],\n    label.size = 0,\n    family = \"serif\",\n    label.padding = unit(0, \"mm\")\n  ) +\n  annotate(\n    x = 10 - 0.2,\n    y = 1,\n    geom = \"label\",\n    label = \"Cumulative Distribution Function\",\n    hjust = 1,\n    size = 4.75,\n    color = myfills[2],\n    label.size = 0,\n    family = \"serif\",\n    label.padding = unit(0, \"mm\")\n  )\n\n\n\n\n\n\n\n3.5.9 Poisson Distributions\n\n\n\n\n\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\n\nParameter\n\\lambda \\in (0,\\infty)\n\n\nSample Space\nx\\in \\{0,1,2,\\ldots\\}\n\n\nMean\n\\mu = \\lambda\n\n\nVariance\n\\sigma^2 = \\lambda\n\n\nSkewness\n\\gamma_1 = \\frac{1}{\\sqrt{\\lambda}}\n\n\nKurtosis\n\\gamma_2 = \\frac{1}{\\lambda}\n\n\nProbability Mass Function\nf_X(x;\\lambda) = \\frac{\\lambda^x}{e^{\\lambda} x!}\n\n\nCumulative Distribution Function\nF_X(x;\\lambda) = \\sum_{i=0}^{x}{\\frac{\\lambda^i}{e^{\\lambda} i!}}\n\n\n\nTable 3.4: Features of Poisson DistributionsNotation note: The notation (0,\\infty) means all real numbers greater than 0.\n\n\n\n\n\n\n\n\nFigure 3.18: Siméon Denis Poisson (1781–1840)Image Credits\n\n\n\nImagine that an event happens sporadically at random and we measure how often it occurs in regular time intervals (e.g., events per hour). Sometimes the event does not occur in the interval, sometimes just once, and sometimes more than once. However, we notice that over many intervals, the average number of events is constant. The distribution of the number of events in each interval will follow a Poisson distribution. Although “Poisson” means “fish” in French, fish have nothing to do with it. This distribution was named after Siméon Denis Poisson, whose work on the distribution made it famous.The Poisson distribution is a discrete distribution used to model how often an event will occur during a particular interval of time.\nThe Poisson distribution has a single parameter \\lambda, the average number of events per time interval. Interestingly, \\lambda is both the mean and the variance of this distribution. The distribution shape will differ depending on how long our interval is. If an event occurs on average 30 times per hour, \\lambda = 30. If we count how often the event occurs in 10-minute intervals, the same event will occur about 5 times per interval, on average (i.e., \\lambda = 1). If we choose to count how often the same event occurs every minute, then \\lambda = 0.5.\n\n\n\n\n\nFigure 3.19: The shape of the Poisson Distribution depends on the interval used for counting events. Here, the event occurs once per minute, on average.\n\n\n\n\n\n\n R Code for Figure 3.19\n\n\n# The shape of the Poisson Distribution\ncrossing(lambda = c(0.5, 5, 30), x = 0:60) %>%\n  mutate(p = dpois(x, lambda)) %>%\n  mutate(lambda = factor(lambda, labels = paste0(\n    \"Every \",\n    c(\"30 Seconds\", \"5 Minutes\", \"Half Hour\"),\n    \" (\\u03BB = \",\n    c(0.5, 5, 30),\n    \")\"\n  ))) %>%\n  ggplot(aes(x, p, color = lambda)) +\n  geom_line(linetype = 3, linewidth = 0.1) +\n  geom_point(size = 2.5, color = \"white\") +\n  geom_point(size = 1) +\n  facet_grid(lambda ~ ., scales = \"free\") +\n  scale_x_continuous(\"Number of Events\", breaks = seq(0, 120, 20)) +\n  scale_color_manual(values = c(myfills[1], \"darkorchid\", myfills[2])) +\n  theme_light(base_size = 20, base_family = bfont) +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(\"Probability\", labels = prob_label)\n\n\n\n\n\n3.5.9.1 A clinical application of the the Poisson distribution\nSuppose that you begin treating an adult male client who has panic attacks that come at unpredictable times. Some weeks there are no panic attacks and some weeks there are many, but on average he has 2 panic attacks each week. The client knows this because he has kept detailed records in a spreadsheet for the last 5 years. The client had sought treatment once before, but terminated early and abruptly because, according to him, “It wasn’t working.” After sensitive querying, you discover that he expected that treatment should have quickly reduced the frequency of panic attacks to zero. When that did not happen, he became discouraged and stopped the treatment.\nBecause your client is well educated and quantitatively inclined, you decide to to use the data he has collected as part of the intervention and also to help set a more realistic set of expectations. Obviously, you and your client both would prefer 0 panic attacks per week, but sometimes it takes more time to get to the final goal. We do not want to terminate treatment that is working just because the final goal has not yet been achieved.\nYou plot the frequency of how often he had 0 panic attacks in a week, 1 panic attack in a week, 2 panic attacks in a week, and so forth, as shown in red in Figure 3.20. Because you have read this book, you immediately recognize that this is a Poisson distribution with \\lambda = 2. When you graph an actual Poison distribution and compare it with your client’s data, you see that it is almost a perfect match.5 Then you explain that although the goal is permanent cessation of the panic attacks, sometimes an intervention can be considered successful if the frequency of panic attacks is merely reduced. For example, suppose that in the early stages of treatment the frequency of panic attacks were reduced from twice per week to once every other week (\\lambda = 0.5), on average. If such a reduction were achieved, there would still be weeks in which two or more panic attacks occur. According to Figure 3.23, this will occur about 9% of the time.5 Note that I am not claiming that all clients’ panic attack frequencies have this kind of distribution. It just so happens to apply in this instance.\n\n\n\n\n\nFigure 3.20: The variability of a hypothetical client’s panic attack frequency before and after treatment\n\n\n\n\n\n\n R Code for Figure 3.20\n\n\n# The variability of a hypothetical \n# client's panic attack frequency\nd_label <- tibble(\n  x = c(0, 2),\n  Time = c(\"After\", \"Before\"),\n  Proportion = dpois(x, lambda = c(0.5, 2)),\n  Label = c(\"After Treatment: \\u03BB = 0.5\",\n            \"Before Treatment: \\u03BB = 2\")\n)\n\ntibble(\n  x = seq(0, 7),\n  Before = dpois(x, lambda = 2),\n  After = dpois(x, lambda = 0.5)\n) %>%\n  gather(key = Time, value = Proportion,-x) %>%\n  ggplot(aes(x, Proportion, color = Time)) +\n  geom_pointline(linetype = \"dotted\", linesize = .5, size = 3, distance = unit(2, \"mm\")) +\n  # geom_point(size = 3) +\n  geom_label(\n    data = d_label,\n    aes(label = Label),\n    hjust = 0,\n    nudge_x = 0.2,\n    family = bfont,\n    label.padding = unit(0, \"lines\"),\n    label.size = 0,\n    size = 6\n  ) +\n  scale_color_manual(values = myfills) +\n  scale_x_continuous(\"Panic Attacks Per Week\",\n                     breaks = 0:7,\n                     minor_breaks = NULL) +\n  scale_y_continuous(labels = . %>% prob_label(., 0.1)) +\n  theme_minimal(base_size = 20, base_family = bfont) +\n  theme(legend.position = \"none\")\n\n\n\n\nIn R, you can use the dpois function to plot the Poisson probability mass function. For example, if the average number of events per time period is λ = 2, then the probability that there will be 0 events is dpois(x = 0, lambda = 2), which evaluates to 0.1353.\nTo calculate the cumulative distribution function of Poisson distribution in R, use the ppois function. For example, if we want to estimate the probability of having 4 panic attacks or more in a week if λ = 2, we must subtract the probability of having 3 panic attacks or less from 1, like so:\n\n1 - ppois(q = 3, lambda = 2)\n\np = 0.143\nHere is a simple way to plot the probability mass function and the cumulative distribution function using the dpois and ppois functions:\n\n# Make a sequence of integers from 0 to 7\nPanicAttacks <- seq(0, 7)\n\n# Generate the probability mass function with lambda = 2\nProbability <- dpois(PanicAttacks, 2)\n\n# Basic plot of the Poisson \n# distribution's probability mass function\nplot(Probability ~ PanicAttacks, type = \"b\") \n\n\n\n\nFigure 3.21: Poisson Probability Mass Function (\\lambda=2)\n\n\n\n\n\n# Generate the cumulative \n# distribution function with lambda = 2\nCumulativeProbability <- ppois(PanicAttacks, 2)\n\n# Basic plot of the Poisson distribution's \n# cumulative distribution function\nplot(CumulativeProbability ~ PanicAttacks, type = \"b\") \n\n\n\n\nFigure 3.22: Poisson Cumulative Distribution Function (\\lambda=2)\n\n\n\n\nWith an additional series with \\lambda = 0.5, the plot can look like Figure 3.23.\n\n\n\n\n\nFigure 3.23: The cumulative distribution function of a hypothetical client’s panic attack frequency before and after treatment\n\n\n\n\n\n\n R Code for Figure 3.23\n\n\n# The cumulative distribution function \n# of a hypothetical client's\n# panic attack frequency before \n# and after treatment\n\nd_label <- tibble(\n  x = c(0, 0),\n  Time = c(\"After\", \"Before\"),\n  Proportion = ppois(x, lambda = c(0.5, 2)),\n  Label = c(\"After Treatment: *&lambda;* = 0.5\",\n            \"Before Treatment: *&lambda;* = 2\")\n)\n\ntibble(\n  x = seq(0, 7),\n  Before = ppois(x, lambda = 2),\n  After = ppois(x, lambda = 0.5)\n) %>%\n  gather(key = Time, value = Proportion,-x) %>%\n  ggplot(aes(x, Proportion, color = Time)) +\n  geom_pointline(linetype = \"dotted\", linesize = .5, size = 3, distance = unit(2, \"mm\")) +\n  geom_richtext(\n    data = d_label,\n    aes(label = Label),\n    hjust = 0,\n    nudge_x = 0.2,\n    family = bfont,\n    label.padding = unit(0, \"lines\"),\n    label.size = 0,\n    size = ggtext_size(20)\n  ) +\n  scale_color_manual(values = myfills) +\n  scale_y_continuous(breaks = seq(0, 1, 0.2), \n                     limits = c(0, 1),\n                     labels = . %>% prob_label(., 0.1)) +\n  scale_x_continuous(\"Panic Attacks Per Week\",\n                     breaks = 0:7,\n                     minor_breaks = NULL) +\n  theme_minimal(base_size = 20, base_family = bfont) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n3.5.10 Geometric Distributions\n\n\n\n\n\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\n\nProbability of success in each trial\np\\in[0,1]\n\n\nSample Space\nx \\in \\{1,2,3,\\ldots\\}\n\n\nMean\n\\mu = \\frac{1}{p}\n\n\nVariance\n\\sigma^2 = \\frac{1-p}{p^2}\n\n\nSkewness\n\\gamma_1 = \\frac{2-p}{\\sqrt{1-p}}\n\n\nKurtosis\n\\gamma_2 = 6 + \\frac{p^2}{1-p}\n\n\nProbability Mass Function\nf_X(x;p) = (1-p)^{x-1}p^x\n\n\nCumulative Distribution Function\nF_X(x;p) = 1-(1-p)^x\n\n\n\nTable 3.5: Features of Geometric Distributions\n\n\nAtul Gawande (2007, pp. 219–223) tells a marvelous anecdote about how a doctor used some statistics to help a young patient with cystic fibrosis to return to taking her medication more regularly. Because the story is full of pathos and masterfully told, I will not repeat a clumsy version of it here. However, unlike Gawande, I will show how the doctor’s statistics were calculated.\n\nGawande, A. (2007). Better: A surgeon’s notes on performance. Metropolitan Books.\nAccording to the story, if a patient fails to take medication, the probability that a person with cystic fibrosis will develop a bad lung illness on any particular day is .005. If medication is taken, the risk is .0005. Although these probabilities are both close to zero, over the the course of a year, they result in very different levels of risk. Off medication, the patient has about an 84% chance of getting sick within a year’s time. On medication, the patient’s risk falls to 17%. As seen in Figure 3.24, the cumulative risk over the course of 10 years is quite different. Without medication, the probability of becoming seriously ill within 10 years at least once is almost certain. With medication, however, a small but substantial percentage (~16%) of patients will go at least 10 years without becoming ill. \n\n\n\n\n\nFigure 3.24: The cumulative risk of serious lung disease with and without medication\n\n\n\n\n\n\n R Code for Figure 3.24\n\n\n# The cumulative risk of serious lung \n# disease with and without medication\n\ntotal_years <- 10\n\ntibble(Days = seq(0, total_years * 365, 10),\n       WithoutMeds = pgeom(Days, 0.005),\n       WithMeds = pgeom(Days, 0.0005)) %>% \n  gather(Meds, p, -Days) %>% \n  mutate(Years = Days / 365) %>% \n  ggplot(aes(Years, p, color = Meds)) + \n  geom_line(linewidth = 1) + \n  theme_minimal(base_size = 18, base_family = bfont) + \n  theme(legend.position = \"none\") +\n  scale_x_continuous(breaks = seq(0,total_years,2)) + \n  scale_y_continuous(\"Cumulative Risk\", breaks = seq(0,1,0.2),\n                     labels = . %>% prob_label(., 0.1)) +\n  scale_color_manual(values = myfills) +\n  coord_fixed(ratio = 10) +\n  annotate(x = 4, y = 0.93, \n           label = \"Without Medication\\nDaily Risk = .005\",\n           geom = \"label\", \n           color = myfills[2],\n           label.padding = unit(0,\"lines\"),\n           label.size = 0,\n           family = bfont,\n           size = 5.2\n           ) +\n  annotate(x = 6.25, y = 0.535, \n           label = \"With Medication\\nDaily Risk = .0005\",\n           geom = \"label\", \n           color = myfills[1],\n           label.padding = unit(0,\"lines\"),\n           label.size = 0,\n           family = bfont,\n           size = 5.2\n           )\n\n\n\n\nSuch calculations make use of the geometric distribution. Consider a series of Bernoulli trials in which an event has a probability p of occurring on any particular trial. The probability mass function of the geometric distribution will tell us the probability that the xth trial will be the first time the event occurs.\n\nf_X(x;p)=(1-p)^{x-1}p^x\n\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\nX\nA random variable with a geometric distribution\n\n\nf_X\nThe probability mass function of X\n\n\nx\nThe number of Bernoulli trials on which the event first occurs\n\n\np\nThe probability of an event occurring on a single Bernoulli trial\n\n\n\n\n\nIn R, the probability mass function of the geometric distribution is calculated with the dgeom function:\n\n# Make a sequence of integers from 1 to 10\nx <- seq(1, 10)\n\n# Generate the probability mass \n# function with p = 0.6\nProbability <- dgeom(x, prob = 0.6)\n\n# Basic plot of the geometric \n# distribution's probability mass function\nplot(Probability ~ x, type = \"b\") \n\n\n\n\nFigure 3.25: Geometric Probability Mass Function (p=.6)\n\n\n\n\nThe cumulative distribution function of the geometric distribution was used to create Figure 3.24. It tells us the probability that the event will occur on the x^{th} trial or earlier:\n\nF_X(x;p)=1-(1-p)^x\n\nIn R, the cumulative distribution function of the geometric distribution uses the pgeom function:\n\n# Generate the cumulative \n# distribution function with p = 0.6\nCumulativeProbability <- pgeom(x, prob = 0.6)\n\n# Basic plot of the geometric\n# distribution's cumulative distribution function\nplot(CumulativeProbability ~ x, type = \"b\") \n\n\n\n\nFigure 3.26: Geometric Cumulative Distribution Function (p=.6)"
  },
  {
    "objectID": "distributions.html#continuous-distributions",
    "href": "distributions.html#continuous-distributions",
    "title": "3  Distributions",
    "section": "3.6 Continuous Distributions",
    "text": "3.6 Continuous Distributions\n\n3.6.1 Probability Density Functions\nAlthough there are many more discrete distribution families, we will now consider some continuous distribution families. Most of what we have learned about discrete distributions applies to continuous distributions. However, there is a need of a name change for the probability mass function. In a discrete distribution, we can calculate an actual probability for a particular value in the sample space. In continuous distributions, doing so can be tricky. We can always calculate the probability that a score in a particular interval will occur. However, in continuous distributions, the intervals can become very small, approaching a width of 0. When that happens, the probability associated with that interval also approaches 0. Yet, some parts of the distribution are more probable than others. Therefore, we need a measure of probability that tells us the probability of a value relative to other values: the probability density functionThe probability density function is function that can show relative likelihoods of sample space elements of a continuous random variable.\nConsidering the entire sample space of a discrete distribution, all of the associated probabilities from the probability mass function sum to 1. In a probability density function, it is the area under the curve that must sum to 1. That is, there is a 100% probability that a value generated by the random variable will be somewhere under the curve. There is nowhere else for it to go!\nHowever, unlike probability mass functions, probability density functions do not generate probabilities. Remember, the probability of any value in the sample space of a continuous variable is infinitesimal. We can only compare the probabilities to each other. To see this, compare the discrete uniform distribution and continuous uniform distribution in Figure 3.4. Both distributions range from 1 to 4. In the discrete distribution, there are 4 points, each with a probability of ¼. It is easy to see that these 4 probabilities of ¼ sum to 1. Because of the scale of the figure, it is not easy to see exactly how high the probability density function is in the continuous distribution. It happens to be ⅓. Why? First, it does not mean that each value has a ⅓ probability. There are an infinite number of points between 1 and 4 and it would be absurd if each of them had a ⅓ probability. The distance between 1 and 4 is 3. In order for the rectangle to have an area of 1, its height must be ⅓. What does that ⅓ mean, then? In the case of a single value in the sample space, it does not mean much at all. It is simply a value that we can compare to other values in the sample space. It could be scaled to any value, but for the sake of convenience it is scaled such that the area under the curve is 1.\nNote that some probability density functions can produce values greater than 1. If the range of a continuous uniform distribution is less than 1, at least some portions of the curve must be greater than 1 to make the area under the curve equal 1. For example, if the bounds of a continuous distribution are 0 and ⅓, the average height of the probability density function would need to be 3 so that the total area is equal to 1.\n\n\n3.6.2 Continuous Uniform Distributions\n\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\n\nLower Bound\na \\in (-\\infty,\\infty)\n\n\nUpper Bound\nb \\in (a,\\infty)\n\n\nSample Space\nx \\in \\lbrack a,b\\rbrack\n\n\nMean\n\\mu = \\frac{a+b}{2}\n\n\nVariance\n\\sigma^2 = \\frac{(b-a)^2-1}{12}\n\n\nSkewness\n\\gamma_1 = 0\n\n\nKurtosis\n\\gamma_2 = -\\frac{6}{5}\n\n\nProbability Density Function\nf_X(x;a,b) = \\frac{1}{b-a}\n\n\nCumulative Distribution Function\nF_X(x;a,b) = \\frac{x-a}{b-a}\n\n\n\nTable 3.6: Features of Continuous Discrete Distributions\n\n\nUnlike the discrete uniform distribution, the uniform distribution is continuous.6 In both distributions, there is an upper and lower bound and all members of the sample space are equally probable.6 For the sake of clarity, the uniform distribution is often referred to as the continuous uniform distribution.\n\n3.6.2.1 Generating random samples from the continuous uniform distribution\nTo generate a sample of n numbers with a continuous uniform distribution between a and b, use the runif function like so:\n\n# Sample size\nn <- 1000\n# Lower and upper bounds\na <- 10\nb <- 30\n# Sample\nx <- runif(n, min = a, max = b)\n\n\n\n\n\n\n\nFigure 3.27: Random sample (n = 1000) of a continuous uniform distribution between 10 and 30. Points are randomly jittered to show the distribution more clearly.\n\n\n\n\n\n R Code for Figure 3.27\n\n\n# Plot\ntibble(x) %>% \nggplot(aes(x, y = 0.5)) + \n  geom_jitter(size = 0.5, \n              pch = 16,\n              color = myfills[1], \n              height = 0.45) +\n  scale_x_continuous(NULL) +\n  scale_y_continuous(NULL, \n                     breaks = NULL, \n                     limits = c(0,1), expand = expansion()) + \n  theme_minimal(base_family = bfont, base_size = bsize)\n\n\n\n\n\n\n3.6.2.2 Using the continuous uniform distribution to generate random samples from other distributions\nUniform distributions can begin and end at any real number but one member of the uniform distribution family is particularly important—the uniform distribution between 0 and 1. If you need to use Excel instead of a statistical package, you can use this distribution to generate random numbers from many other distributions.\nThe cumulative distribution function of any continuous distribution converts into a continuous uniform distribution. A distribution’s quantile function converts a continuous uniform distribution into that distribution. Most of the time, this process also works for discrete distributions. This process is particularly useful for generating random numbers with an unusual distribution. If the distribution’s quantile function is known, a sample with a continuous uniform distribution can easily be generated and converted.\nFor example, the RAND function in Excel generates random numbers between 0 and 1 with a continuous uniform distribution. The BINOM.INV function is the binomial distribution’s quantile function. Suppose that n (number of Bernoulli trials) is 5 and p (probability of success on each Bernoulli trial) is 0.6. A randomly generated number from the binomial distribution with n=5 and p=0.6 is generated like so:\n=BINOM.INV(5,0.6,RAND())\nExcel has quantile functions for many distributions (e.g., BETA.INV, BINOM.INV, CHISQ.INV, F.INV, GAMMA.INV, LOGNORM.INV, NORM.INV, T.INV). This method of combining RAND and a quantile function works reasonably well in Excel for quick-and-dirty projects, but when high levels of accuracy are needed, random samples should be generated in a dedicated statistical program like R, Python (via the numpy package), Julia, STATA, SAS, or SPSS.\n\n\n\n3.6.3 Normal Distributions\n(Unfinished)\n\n\n\n\n\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\n\nSample Space\nx \\in (-\\infty,\\infty)\n\n\nMean\n\\mu = \\mathcal{E}\\left(X\\right)\n\n\nVariance\n\\sigma^2 = \\mathcal{E}\\left(\\left(X - \\mu\\right)^2\\right)\n\n\nSkewness\n\\gamma_1 = 0\n\n\nKurtosis\n\\gamma_2 = 0\n\n\nProbability Density Function\nf_X(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma ^ 2}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n\n\nCumulative Distribution Function\nF_X(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} {\\displaystyle \\int_{-\\infty}^{x} e ^ {-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx}\n\n\n\nTable 3.7: Features of Normal Distributions\n\n\n\n\n\n\n\n\nFigure 3.28: Carl Friedrich Gauss (1777–1855)Image Credits\n\n\n\nThe normal distribution is sometimes called the Gaussian distribution after its discoverer, Carl Friedrich Gauss Figure 3.28. It is a small injustice that most people do not use Gauss’s name to refer to the normal distribution. Thankfully, Gauss is not exactly languishing in obscurity. He made so many discoveries that his name is all over mathematics and statistics.\nThe normal distribution is probably the most important distribution in statistics and in psychological assessment. In the absence of other information, assuming that an individual difference variable is normally distributed is a good bet. Not a sure bet, of course, but a good bet. Why? What is so special about the normal distribution?\nTo get a sense of the answer to this question, consider what happens to the binomial distribution as the number of events (n) increases. To make the example more concrete, let’s assume that we are tossing coins and counting the number of heads (p=0.5). In Figure 3.29, the first plot shows the probability mass function for the number of heads when there is a single coin (n=1)). In the second plot, n=2 coins. That is, if we flip 2 coins, there will be 0, 1, or 2 heads. In each subsequent plot, we double the number of coins that we flip simultaneously. Even with as few as 4 coins, the distribution begins to resemble the normal distribution, although the resemblance is very rough. With 128 coins, however, the resemblance is very close.\n\n\n\n\n\nFigure 3.29: The binomial distribution begins to resemble the normal distribution when the number of events is large.\n\n\n\n\nThis resemblance to the normal distribution in the example is not coincidental to the fact that p=0.5, making the binomial distribution symmetric. If p is extreme (close to 0 or 1), the binomial distribution is asymmetric. However, if n is large enough, the binomial distribution eventually becomes very close to normal.\nMany other distributions, such as the Poisson, Student’s T, F, and \\chi^2 distributions, have distinctive shapes under some conditions but approximate the normal distribution in others (See Figure 3.30). Why? In the conditions in which non-normal distributions approximate the normal distribution, it is because, like in Figure 3.29, many independent events are summed.\n\n\n\n\n\nFigure 3.30: Many distributions become nearly normal when their parameters are high.\n\n\n\n\n\n3.6.3.1 Notation for Normal Variates\nStatisticians write about variables with normal distributions so often that a compact notation for specifying a normal variable’s parameters was useful to develop. If I want to specify that X is a normally variable with a mean of \\mu and a variance of \\sigma^2, I will use this notation:\nX \\sim \\mathcal{N}(\\mu, \\sigma^2)\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\nX\nA random variable.\n\n\n\\sim\nIs distributed as\n\n\n\\mathcal{N}\nHas a normal distribution\n\n\n\\mu\nThe population mean\n\n\n\\sigma^2\nThe population variance\n\n\n\n\n\nMany authors list the standard deviation \\sigma instead of the variance \\sigma^2. When I specify normal distributions with specific means and variances, I will avoid ambiguity by always showing the variance as the standard deviation squared. For example, a normal variate with a mean of 10 and a standard deviation of 3 will be written as X \\sim \\mathcal{N}(10,3^2).\n\n\n\n\n\nFigure 3.31: Percentiles convert a distribution into a uniform distribution\n\n\n\n\n\n\n\n\n\nFigure 3.32: Evenly spaced percentile ranks are associated with unevenly spaced scores.\n\n\n\n\n\n\n\n\n\nFigure 3.33: Evenly spaced scores are associated with unevenly spaced percentiles\n\n\n\n\n\n\n3.6.3.2 Half-Normal Distribution\n(Unfinished)\n\n\n\n\n\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\n\nSample Space\nx \\in [\\mu,\\infty)\n\n\nMu\n\\mu \\in (-\\infty,\\infty)\n\n\nSigma\n\\sigma \\in [0,\\infty)\n\n\nMean\n\\mu + \\sigma\\sqrt{\\frac{2}{\\pi}}\n\n\nVariance\n\\sigma^2\\left(1-\\frac{2}{\\pi}\\right)\n\n\nSkewness\n\\sqrt{2}(4-\\pi)(\\pi-2)^{-\\frac{3}{2}}\n\n\nKurtosis\n8(\\pi-3)(\\pi-2)^{-2}\n\n\nProbability Density Function\nf_X(x;\\mu,\\sigma) = \\sqrt{\\frac{2}{\\pi \\sigma ^ 2}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n\n\nCumulative Distribution Function\nF_X(x;\\mu,\\sigma) = \\sqrt{\\frac{2}{\\pi\\sigma}} {\\displaystyle \\int_{\\mu}^{x} e ^ {-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx}\n\n\n\nTable 3.8: Features of Half-Normal Distributions\n\n\n\n\n\n\n\nFigure 3.34: The half-normal distribution is the normal distribution with the left half of the distribution stacked on top of the right half of the distribution.\n\n\n\n\n\n\n R Code for Figure 3.34\n\n\n# Half normal distribution\nxlim <- 4\nn <- length(seq(-xlim, 0, 0.01))\nt1 <- tibble(\n  x = c(0,-xlim,\n        seq(-xlim, 0, 0.01),\n        0,\n        0,\n        seq(0, xlim, 0.01),\n        xlim,\n        0),\n  y = c(0,\n        0,\n        dnorm(seq(-xlim, 0, 0.01)),\n        0,\n        0,\n        dnorm(seq(0, xlim, 0.01)),\n        0,\n        0),\n  side = c(rep(F, n + 3), rep(T, n + 3)),\n  Type = 1\n)\nt2 <- t1 %>%\n  mutate(y = if_else(side, y, 2 * y)) %>%\n  mutate(x = abs(x),\n         Type = 2)\n\nbind_rows(t1, t2) %>%\n  mutate(Type = factor(Type)) %>%\n  ggplot(aes(x, y, fill = side)) +\n  geom_polygon() +\n  geom_text(\n    data = tibble(\n      x = 0,\n      y = dnorm(0) * c(1, 2) + 0.14,\n      Type = factor(c(1,2)),\n      label = c(\n        \"Normal\",\n        \"Half-Normal\"),\n      side = T),\n    aes(label = label),\n    family = bfont, fontface = \"bold\",\n    size = ggtext_size(30), \n    vjust = 1\n  ) +\n  geom_richtext(\n    data = tibble(\n      x = 0,\n      y = dnorm(0) * c(1, 2) + 0,\n      Type = factor(c(1,2)),\n      label = c(\n        paste0(\"*X* ~ \",\n               span_style(\"N\", style = \"font-family:'Lucida Calligraphy'\"),\n               \"(*\",\n               span_style(\"&mu;\"),\n               \"*, *\",\n               span_style(\"&sigma;\"),\n               \"*<sup>2</sup>)\"),\n        paste0(\"*X* ~ |\",\n               span_style(\"N\", style = \"font-family:'Lucida Calligraphy'\"),\n               \"(0, *\",\n               span_style(\"&sigma;\"),\n               \"*<sup>2</sup>)| + *\",\n               span_style(\"&mu;\"),\n               \"*\")),\n      side = T),\n    aes(label = label),\n    family = c(\"Equity Text A\"),\n    size = ggtext_size(30), \n    vjust = 0, \n    label.padding = unit(0,\"lines\"), \n    label.color = NA,\n    fill = NA) +\n  theme_void(base_size = 30,\n                base_family = bfont) +\n  theme(\n    legend.position = \"none\",\n    strip.text = element_blank()\n  ) +\n  scale_fill_manual(values = myfills) +\n  facet_grid(rows = vars(Type), space = \"free_y\", scales = \"free_y\") \n\n\n\n\nSuppose that X is a normally distributed variable such that\nX \\sim \\mathcal{N}(\\mu, \\sigma^2)\nVariable Y then has a half-normal distribution such that Y = |X-\\mu|+\\mu. In other words, imagine that a normal distribution is folded at the mean with the left half of the distribution now stacked on top of the right half of the distribution (See Figure 3.34).\n\n\n3.6.3.3 Truncated Normal Distributions\n(Unfinished)\n\n\n3.6.3.4 Multivariate Normal Distributions\n(Unfinished)\n\n\n\n3.6.4 Chi Square Distributions\n(Unfinished)\n\n\n\n\n\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\n\nSample Space\nx \\in [0,\\infty)\n\n\nDegrees of freedom\n\\nu \\in [0,\\infty)\n\n\nMean\n\\nu\n\n\nVariance\n2\\nu\n\n\nSkewness\n\\sqrt{8/\\nu}\n\n\nKurtosis\n12/\\nu\n\n\nProbability Density Function\nf_X(x;\\nu) = \\frac{x^{\\nu/2-1}}{2^{\\nu/2}\\;\\Gamma(\\nu/2)\\,\\sqrt{e^x}}\n\n\nCumulative Distribution Function\nF_X(x;\\nu) = \\frac{\\gamma\\left(\\frac{\\nu}{2},\\frac{x}{2}\\right)}{\\Gamma(\\nu/2 )}\n\n\n\nTable 3.9: Features of Chi-Square Distributions\n\n\nI have always thought that the \\chi^2 distribution has an unusual name. The chi part is fine, but why square? Why not call it the \\chi distribution?7 As it turns out, the \\chi^2 distribution is formed from squared quantities.7 Actually, there is a \\chi distribution. It is simply the square root of the \\chi^2 distribution. The half-normal distribution happens to be a \\chi distribution with 1 degree of freedom.\nNotation note: A \\chi^2 distribution with \\nu degrees of freedom can be written as \\chi^2_\\nu or \\chi^2(\\nu).\nThe \\chi^2 distribution has a straightforward relationship with the normal distribution. It is the sum of multiple independent squared normal variates. That is, if z is a standard normal variate— z\\sim\\mathcal{N}(0,1^2)—then z^2 has a \\chi^2 distribution with 1 degree of freedom (\\nu):\nz^2\\sim \\chi^2_1\nIf z_1 and z_2 are independent standard normal variates, the sum of their squares has a \\chi^2 distribution with 2 degrees of freedom:\nz_1^2+z_2^2 \\sim \\chi^2_2 If \\{z_1,z_2,\\ldots,z_{\\nu} \\} is a series of \\nu independent standard normal variates, the sum of their squares has a \\chi^2 distribution with \\nu degrees of freedom:\n\\sum^\\nu_{i=1}{z_i^2} \\sim \\chi^2_\\nu\n\n3.6.4.1 Clinical Uses of the \\chi^2 distribution\nThe \\chi^2 distribution has many applications, but the mostly likely of these to be used in psychological assessment is the \\chi^2 Test of Goodness of Fit and the \\chi^2 Test of Independence.\nSuppose we suspect that a child’s temper tantrums are more likely to occur on weekdays than on weekends. The child’s mother has kept a record of each tantrum for the past year and was able to count the frequency of tantrums. If tantrums were equally likely to occur on any day, 5 of 7 tantrums should occur on weekdays, and 2 of 7 tantrums should occur on weekends. The observed frequencies are compared with the expected frequencies below.\n\\begin{array}{r|c|c|c}\n& \\text{Weekday} & \\text{Weekend} & \\text{Total} \\\\\n\\hline\n\\text{Observed Frequency}\\, (o) & 14 & 13 & n=27\\\\\n\\text{Expected Proportion}\\,(p) & \\frac{5}{7} & \\frac{2}{7} & 1\\\\\n\\text{Expected Frequency}\\, (e = np)& 27\\times \\frac{5}{7}= 19.2857& 27\\times \\frac{2}{7}= 7.7143& 27\\\\\n\\text{Difference}\\,(o-e) & -5.2857 & 5.2857&0\\\\\n\\frac{(o-e)^2}{e} & 1.4487 & 3.6217 & \\chi^2 = 5.07\n\\end{array}\nIn the table above, if the observed frequencies (o_i) are compared to their respective expected frequencies (e_i), then:\n\\chi^2_{k-1}=\\sum_{i=1}^k{\\frac{(o_i-e_i)^2}{e_i}}=5.07\nUsing the \\chi^2 cumulative distribution function, we find that the probability of observing the frequencies listed is low under the assumption that tantrums are equally likely each day.\n\nobserved_frequencies <- c(14, 13)\nexpected_probabilities <- c(5,2) / 7\n\nfit <- chisq.test(observed_frequencies, p = expected_probabilities)\nfit\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed_frequencies\nX-squared = 5.0704, df = 1, p-value = 0.02434\n\n# View expected frequencies and residuals\nbroom::augment(fit)\n\n# A tibble: 2 × 6\n  Var1  .observed .prop .expected .resid .std.resid\n  <fct>     <dbl> <dbl>     <dbl>  <dbl>      <dbl>\n1 A            14 0.519     19.3   -1.20      -2.25\n2 B            13 0.481      7.71   1.90       2.25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\nB\n0\n1\n\n\n\n\n0\n40\n33\n\n\n1\n5\n22\n\n\n\n\n\n\n\n# A tibble: 4 × 9\n  A     B     .observed .prop .row.prop .col.prop .expected .resid .std.resid\n  <fct> <fct>     <int> <dbl>     <dbl>     <dbl>     <dbl>  <dbl>      <dbl>\n1 0     0            40  0.4      0.889     0.548      32.8   1.25       3.24\n2 1     0            33  0.33     0.6       0.452      40.2  -1.13      -3.24\n3 0     1             5  0.05     0.111     0.185      12.2  -2.05      -3.24\n4 1     1            22  0.22     0.4       0.815      14.8   1.86       3.24\n\n\n\n\n\n3.6.5 Student’s t Distributions\n\n\n\n\n\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\n\nSample Space\nx \\in (-\\infty,\\infty)\n\n\nDegrees of Freedom\n\\nu \\in (0,\\infty)\n\n\nMean\n\\left\\{ \\begin{array}{ll}  0 & \\nu > 1 \\\\  \\text{Undefined} & \\nu \\le 1 \\\\ \\end{array} \\right.\n\n\nVariance\n\\left\\{ \\begin{array}{ll}  \\frac{\\nu}{\\nu-2} & \\nu>2 \\\\  \\infty & 1<\\nu \\le 2\\\\  \\text{Undefined} & \\nu \\le 1 \\\\ \\end{array} \\right.\n\n\nSkewness\n\\left\\{ \\begin{array}{ll}  0 & \\nu > 3 \\\\  \\text{Undefined} & \\nu \\le 3 \\\\ \\end{array} \\right.\n\n\nKurtosis\n\\left\\{ \\begin{array}{ll}  \\frac{6}{\\nu-4} & \\nu>4 \\\\  \\infty & 2<\\nu \\le 4\\\\  \\text{Undefined} & \\nu \\le 2 \\\\ \\end{array} \\right.\n\n\nProbability Density Function\nf_X(x; \\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})} {\\sqrt{\\nu\\pi}\\,\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{x^2}{\\nu} \\right)^{-\\frac{\\nu+1}{2}}\n\n\nCumulative Distribution Function\nF_X(x; \\nu)=\\frac{1}{2} + x \\Gamma \\left( \\frac{\\nu+1}{2} \\right) \\frac{\\phantom{\\,}_{2}F_1 \\left(\\frac{1}{2},\\frac{\\nu+1}{2};\\frac{3}{2};-\\frac{x^2}{\\nu} \\right)} {\\sqrt{\\pi\\nu}\\,\\Gamma \\left(\\frac{\\nu}{2}\\right)}\n\n\n\nTable 3.10: Features of t DistributionsNotation note: \\Gamma is the gamma function. _2F_1 is the hypergeometric function.\n\n\n\n\n\n\n\n\nFigure 3.35: “Student” statistician, William Sealy Gosset (1876–1937)Image Credit\n\n\n\n(Unfinished)\nGuinness Beer gets free advertisement every time the origin story of the Student t distribution is retold, and statisticians retell the story often. The fact that the original purpose of the t distribution was to brew better beer seems too good to be true.\nWilliam Sealy Gosset (1876–1937), self-trained statistician and head brewer at Guinness Brewery in Dublin, continually experimented on small batches to improve and standardize the brewing process. With some help from statistician Karl Pearson, Gosset used then-current statistical methods to analyze his experimental results. Gosset found that Pearson’s methods required small adjustments when applied to small samples. With Pearson’s help and encouragement (and later from Ronald Fisher), Gosset published a series of innovative papers about a wide range of statistical methods, including the t distribution, which can be used to describe the distribution of sample means.\nWorried about having its trade secrets divulged, Guinness did not allow its employees to publish scientific papers related to their work at Guinness. Thus, Gosset published his papers under the pseudonym, “A Student.” The straightforward names of most statistical concepts need no historical treatment. Few of us who regularly use the Bernoulli, Pareto, Cauchy, and Gumbell distributions could tell you anything about the people who discovered them. But the oddly named “Student’s t distribution” cries out for explanation. Thus, in the long run, it was Gosset’s anonymity that made him famous.\n\n\n\n\n\n\nFigure 3.36: The t distribution approaches the standard normal distribution as the degrees of freedom (df) parameter increases.\n\n\n\n\n\n R Code for Figure 3.34\n\n\n# The t distribution approaches the normal distribution\nd <- crossing(x = seq(-6,6,0.02), \n         df = c(seq(1,15,1),\n                seq(20,45,5),\n                seq(50,100,10),\n                seq(200,700,100))) %>%\n  mutate(y = dt(x,df),\n         Normal = dnorm(x)) \n\nt_size <- 40\n\nd_label <- d %>% \n  select(df) %>% \n  unique() %>% \n  mutate(lb = qt(.025, df),\n         ub = qt(0.975, df)) %>% \n  pivot_longer(c(lb, ub), values_to = \"x\", names_to = \"bounds\") %>% \n  mutate(label_x = signs::signs(x, accuracy = .01),\n         y = 0,\n         yend = dt(x, df))\n\np <- ggplot(d, aes(x, y)) + \n  geom_area(aes(y = Normal), alpha = 0.25, fill = myfills[1]) +\n  geom_line() +\n  geom_area(data = . %>% filter(x >= 1.96), \n            alpha = 0.25, \n            fill = myfills[1],\n            aes(y = Normal)) +\n  geom_area(data = . %>% filter(x <= -1.96), \n            alpha = 0.25, \n            fill = myfills[1],\n            aes(y = Normal)) +\n  geom_text(data = d_label, \n            aes(label = label_x), \n            family = bfont, \n            vjust = 1.25,\n            size = ggtext_size(t_size)) + \n  geom_text(data = d_label %>% select(df) %>% unique,\n            aes(x = 0, y = 0, label = paste0(\"df = \", df)), \n            vjust = 1.25, \n            family = bfont,\n            size = ggtext_size(t_size)) + \n  geom_segment(data = d_label, aes(xend = x, yend = yend)) +\n  transition_states(states = df, \n                    transition_length =  1, \n                    state_length = 2) +\n  theme_void(base_size = t_size, base_family = bfont) +\n  # labs(title = \"df = {closest_state}\") +\n  annotate(x = qnorm(c(0.025, 0.975)), \n           y = 0, \n           label = signs::signs(qnorm(c(0.025, 0.975)), accuracy = .01), \n           geom = \"text\", \n           size = ggtext_size(t_size),\n           color = myfills[1],\n           vjust = 2.6, \n           family = bfont) + \n  coord_cartesian(xlim = c(-6,6), ylim = c(-.045, NA)) \n\nanimate(p, \n        renderer = magick_renderer(), \n        device = \"svglite\", \n        fps = 2, \n        height = 6, \n        width = 10)\ngganimate::anim_save(\"tdist_norm.gif\")\n\n\n\n\n\n3.6.5.1 The t distribution’s relationship Relationship to the normal distribution.\nSuppose we have two independent standard normal variates Z_0 \\sim \\mathcal{N}(0, 1^2) and Z_1 \\sim \\mathcal{N}(0, 1^2).\nA t distribution with one degree of freedom is created like so:\n\nT_1 = z_0\\sqrt{\\frac{1}{z_1^2}}\n\nA t distribution with two degrees of freedom is created like so:\n\nT_2 = z_0\\sqrt{\\frac{2}{z_1^2 + z_2^2}}\n\nWhere z_0, z_1 and z_2 are independent standard normal variates.\nA t distribution with \\nu degrees of freedom is created like so:\n\nT_v = z_0\\sqrt{\\frac{\\nu}{\\sum_{i=1}^\\nu z_i^2}}\n\nThe sum of \\nu squared standard normal variates \\left(\\sum_{i=1}^\\nu z_i^2\\right) has a \\chi^2 distribution with \\nu degrees of freedom, which has a mean of \\nu. Therefore, \\sqrt{\\frac{\\nu}{\\sum_{i=1}^\\nu z_i^2}}, on average, equals one. However, the expression \\sqrt{\\frac{\\nu}{\\sum_{i=1}^\\nu z_i^2}} has a variability approaches 0 as \\nu increases. When \\nu is high, z_0 is being multiplied by a value very close to 1. Thus, T_\\nu is nearly normal at high levels of nu."
  },
  {
    "objectID": "distributions.html#addititional-distritributions",
    "href": "distributions.html#addititional-distritributions",
    "title": "3  Distributions",
    "section": "3.7 Addititional Distritributions",
    "text": "3.7 Addititional Distritributions\n(Unfinished)\n\nF Distributions\nWeibull Distributions\nGumbel Distributions\nBeta Distributions\nExponential Distributions\nPareto Distributions"
  },
  {
    "objectID": "descriptives.html",
    "href": "descriptives.html",
    "title": "4  Descriptive Statistics",
    "section": "",
    "text": "5 Moments, Cumulants, and Descriptive Statistics\nWe can define random variables in terms of their probability mass/density functions. We can also define them in terms of their moments and cumulants."
  },
  {
    "objectID": "descriptives.html#frequency-distribution-tables",
    "href": "descriptives.html#frequency-distribution-tables",
    "title": "4  Descriptive Statistics",
    "section": "4.1 Frequency Distribution Tables",
    "text": "4.1 Frequency Distribution Tables\nA simple way to describe a distribution is to list how many times each value in the distribution occurs. For example, in this distribution: \\{10, 3, 4, 10, 6, 4, 6, 4\\}, there is 1 three, 3 fours, 2 sixes, and 2 tens. The value that occurs most often is four. A frequency distribution table displays the number of times each value occurs, as in Table 4.1.A frequency distribution table summarises a sample by showing the frequency counts of each member of the sample space.\n\n\n\n\n\nTable 4.1: Frequency Distribution TableThe median is 5, halfway between the two middle scores of 4 and 6.\n\n\n\n\n\n\n\n\n\nX\nFrequency\nCumulativeFrequency\nProportion\nCumulativeProportion\n\n\n\n\n3\n1\n1\n.125\n.125\n\n\n4\n3\n4\n.375\n.500\n\n\n6\n2\n6\n.250\n.750\n\n\n10\n2\n8\n.250\n1\n\n\n\n\n\n\n\n R Code for Table 4.1\n\n\nX <- c(3,4,6,10)\nreps <- c(1,3,2,2)\nd <- map2(X, reps, rep) %>% unlist %>% \n  tibble(X = .) %>% \n  group_by(X) %>% \n  summarise(f = n()) %>% \n  mutate(cf = cumsum(f),\n         p = (f / sum(f)) , \n         cp = cumsum(p) %>% \n           prob_label(accuracy = .001)) %>% \n  mutate(p = prob_label(p, accuracy = .001))\n\nsixes <- reps[X == 6]\nsample_size = sum(reps)\n\nknitr::kable(\n  d,\n  col.names = c(\n    \"*X*\",\n    \"Frequency\",\n    \"Cumulative<br>Frequency\" ,\n    \"Proportion\",\n    \"Cumulative<br>Proportion\"\n  ),\n  align = \"rrrrr\",\n  escape = F\n)\n\n\n\n\nIt is common to include alongside the frequencies of each value the proportion (or percentage) of times a value occurs. If the frequency of sample space element i is f_i, and the total sample size is n, then the proportion of sample space element i is\np_i = \\frac{f_i}{n}\nIn Table 4.1, the frequency of sixes is f=2 and there are n = 8 numbers in the distribution, thus the proportion of sixes is p = \\frac{2}{8} = .25.\nIt is also common to supplement frequency distribution tables with additional information such as the cumulative frequency. For each sample space element, the cumulative frequency (cf) is the sum of the frequencies (f) of the current and all previous sample space elements.The cumulative frequency tells us the number of scores in a distribution that are equal to or lower than a particular sample space element.\ncf_i= \\sum_{j=1}^{i}{f_j}\nOrdinal, interval, and ratio variables can have cumulative frequencies, but not nominal variables. To calculate cumulative frequencies, the sample space needs to be sorted in a meaningful way, which is not possible with true nominal variables. That is, there are no scores “below” any other scores in nominal variables.\nThe cumulative proportion (cp) is the proportion of scores less than or equal to a particular sample space element.\ncp_i = \\frac{cf_i}{n}\n\n4.1.1 Frequency Distribution Tables in R\nLet’s start with a data set from Garcia et al. (2010), which can accessed via the psych package.\n\nGarcia, D. M., Schmitt, M. T., Branscombe, N. R., & Ellemers, N. (2010). Women’s reactions to ingroup members who protest discriminatory treatment: The importance of beliefs about inequality and response appropriateness. European Journal of Social Psychology, 40(5), 733–745.\n\n# Get the Garcia data set from the psych package\nd <- psych::Garcia\n\nThe sjmisc package (Lüdecke, 2021) provides a quick and easy way to create a frequency distribution table with the frq function.\n\nLüdecke, D. (2021). Sjmisc: Data and variable transformation functions. https://strengejacke.github.io/sjmisc/\n\nsjmisc::frq(d$anger)\n\nx <numeric> \n# total N=129 valid N=129 mean=2.12 sd=1.66\n\nValue |  N | Raw % | Valid % | Cum. %\n-------------------------------------\n    1 | 73 | 56.59 |   56.59 |  56.59\n    2 | 24 | 18.60 |   18.60 |  75.19\n    3 |  4 |  3.10 |    3.10 |  78.29\n    4 |  8 |  6.20 |    6.20 |  84.50\n    5 | 12 |  9.30 |    9.30 |  93.80\n    6 |  7 |  5.43 |    5.43 |  99.22\n    7 |  1 |  0.78 |    0.78 | 100.00\n <NA> |  0 |  0.00 |    <NA> |   <NA>\n\n\nTypically we use frequency distribution tables to check whether the values of a variable are correct and that the distribution makes sense to us. Thus the frq function is all we need most of the time. However, if you need a publication-ready frequency distribution table, you will probably have to make it from scratch (See Table 4.2).\n\n\n\n\n\n\n\nX\nf\ncf\np\ncp\n\n\n\n\n\n1\n73\n73\n.57\n.57\n\n\n\n2\n24\n97\n.19\n.75\n\n\n\n3\n4\n101\n.03\n.78\n\n\n\n4\n8\n109\n.06\n.84\n\n\n\n5\n12\n121\n.09\n.94\n\n\n\n6\n7\n128\n.05\n.99\n\n\n\n7\n1\n129\n.01\n1.00\n\n\n\n\nTable 4.2: Frequency Distribution Table for Angerf = Frequency,cf = Cumulative Frequency, p = Proportion, and cp = Cumulative Proportion\n\n\n\n\n R Code for Table 4.2\n\n\n# Publication-quality frequency table\nd %>% \n  rename(X = anger) %>% \n  count(X, name = \"f\") %>% \n  mutate(cf = cumsum(f),\n         p = f / sum(f),\n         cp = cumsum(p)) %>% \n  mutate(across(\n    .cols = p:cp,\n    .fns = function(x) scales::number(x, .01) %>% \n      str_remove(\"^0\"))) %>% \n  rename_with(\n    .fn = function(x) paste0(\"*\",x,\"*\")) %>% \n  mutate(` ` = \"\") %>% \n  kable(\n    digits = 2,\n    align = \"r\"\n  ) %>%\n  html_table_width(c(30, rep(100, 4), 20))\n\n\n\n\n\n\n4.1.2 Frequency Distribution Bar Plots\n\n\n\n\n\n\nFigure 4.1: Frequency Distribution Bar Plot\n\n\n\n\n\n\n\n\n\nFigure 4.2: Cumulative Frequency Stacked Bar Plot\n\n\n\n\n\n\n\n\n\nFigure 4.3: Cumulative Frequency Step Plot\n\n\n\nIn Figure 4.1, the frequency distribution from Table 4.2 is translated into a standard bar plot in which each bar is proportional to the frequency of each response. A column bar plot allows for easy comparison of the frequency of each category. For example, in Figure 4.1, the most frequent response to the Anger question—1 (low)—draws your attention immediately. In contrast to the mental effort needed to scan frequencies listed in a table, the relative height of each frequency in the bar plot is perceived, compared, and interpreted almost effortlessly. With a single glance at Figure 4.1, no calculation is required to know that none of the other responses is even half as frequent as 1.\n\n\n R Code for Figure 4.1\n\n\n# Make frequency data\nd_freq <- d %>% \n  rename(Anger = anger) %>% \n  count(Anger, name = \"f\") %>% \n  mutate(cf = cumsum(f),\n         p = f / sum(f),\n         cp = cumsum(p))\n\n# Frequency Bar Plot\nd_freq %>% \n  ggplot(aes(Anger, f)) + \n  geom_col(fill = myfills[1]) + \n  geom_text_fill(aes(label = f), \n                 vjust = -0.5, \n                 size = ggtext_size(30)) +\n  scale_x_continuous(breaks = 1:7, \n                     minor_breaks = NULL) + \n  scale_y_continuous(\"Frequency\", \n                     expand = expansion(c(0, 0.09))) +\n  theme_minimal(base_size = 30, \n                base_family = bfont) + \n  theme(panel.grid.major.x = element_blank())       \n\n\n\n\n\n\n4.1.3 Frequency Distribution Stacked Bar Plots\nIn a standard bar plot, one may easily compare frequencies to each other, but that may not be what you wish the reader to notice first. A stacked bar plot emphasizes the proportions of each category as it relates to the whole. It also allows for the visual display of the cumulative frequencies and proportions. For example, in Figure 4.2, it is easy to see that more than half of participants have an anger level of 1, and three quarters have an anger level of 2 or less.\n\n\n R Code for Figure 4.2\n\n\n# Stacked Frequency Bar Plot\n\nd_freq %>% \n  ggplot(aes(factor(\"Anger\"), cf - f / 2)) + \n  geom_tile(aes(height = f, fill = factor(Anger)), \n            width = 1.2) +\n  geom_text(aes(label = paste0(\n    Anger, \n    \" (\", \n    f, \n    \", \",\n    scales::percent(p, accuracy = 1),\n    \")\"))) + \n  scale_y_continuous(\n    \"Cumulative Frequency\",\n    breaks = c(0, d_freq$cf),\n    minor_breaks = NULL,\n    expand = expansion(c(0, .04)),\n    sec.axis = sec_axis(\n      trans = ~ .x,\n      labels = scales::percent(c(0, d_freq$cp), \n                               accuracy = 1),\n      breaks = c(0, d_freq$cf),\n      name = \"Cumulative Percentage\"))  +\n  scale_fill_manual(values = tinter::tinter(myfills[1], 9)) +\n  scale_x_discrete(NULL) +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.text.y = element_text(vjust = c(rep(0.5, 7),-0.3))\n  ) \n\n\n\n\n\n\n4.1.4 Frequency Distribution Step Line Plots\nA step line plot can show the cumulative frequency’s relationship with the variable. For example, in Figure 4.3, it appears that the cumulative frequency rises quickly at first but then rises slowly and steadily thereafter.\n\n\n R Code for Figure 4.3\n\n\n# Frequency Step Plot\nd_freq %>%\n  ggplot(aes(Anger, cf)) +\n  geom_step(direction = \"mid\",\n            color = myfills[1],\n            linewidth = 0.5) +\n  geom_text_fill(\n    aes(label = cf),\n    vjust = -0.5,\n    color = myfills[1],\n    size = ggtext_size(30)\n  ) +\n  geom_text_fill(\n    aes(label = signs::signs(\n      f, accuracy = 1, add_plusses = T\n    )),\n    vjust = 1.5,\n    color = \"gray40\",\n    size = ggtext_size(30)\n  ) +\n  scale_x_continuous(breaks = 1:7,\n                     expand = expansion()) +\n  scale_y_continuous(\n    \"Cumulative Frequency\",\n    limits = c(0, NA),\n    breaks = 0,\n    minor_breaks = NULL,\n    expand = expansion(mult = c(0.001, 0.07))\n  ) +\n  theme_minimal(base_size = 30, base_family = bfont) +\n  theme(panel.grid.major.x = element_blank()) +\n  annotate(\n    geom = \"segment\",\n    x = 0.5,\n    y = 0,\n    xend = 0.5,\n    yend = 73,\n    color = myfills[1],\n    linewidth = 1\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = 0.5,\n    y = 73,\n    xend = 1,\n    yend = 73,\n    color = myfills[1],\n    linewidth = 0.5\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = 7,\n    y = 129,\n    xend = 7.5,\n    yend = 129,\n    color = myfills[1],\n    linewidth = 0.5\n  )"
  },
  {
    "objectID": "descriptives.html#measures-of-central-tendency",
    "href": "descriptives.html#measures-of-central-tendency",
    "title": "4  Descriptive Statistics",
    "section": "4.2 Measures of Central Tendency",
    "text": "4.2 Measures of Central Tendency\nIf we need to summarize an entire distribution with a single number with the least possible information loss, we use a measure of central tendency—usually the mode, the median, or the mean. Although these numbers are intended to represent the entire distribution, they often require accompaniment from other statistics to perform this role with sufficient fidelity.\nOur choice of which measure of central tendency we use to summarize a distribution depends on which type of variable we are summarizing (i.e., nominal, ordinal, interval, or ratio) and also a consideration of each central tendency measure’s strengths and weaknesses in particular situations.\n\n4.2.1 Mode\nThe mode is the most frequent score in a distribution. Suppose we have a distribution that looks like this:The mode is the value in a distribution that occurs most often.\n\\{1,2,2,2,2,3,3\\}\nBecause 2 occurs more frequently than the other values in the distribution, the mode is 2.\n\n\n\nIn a frequency distribution table, the mode is the value with the highest value in the f (frequency) column. In Table 4.2, the mode is 1 because it has the highest frequency (f = 73).\nIn a bar plot, histogram, or probability density plot, the mode is the value that corresponds to the highest point in the plot. For example, in Figure 4.1, the modal value is 1 because its frequency of 73 is the highest point in the bar plot. In Figure 4.4, the mode is −1 because that is the highest point in the density plot.\nIf two values tie, both values are the mode and the distribution is bimodal. Sometimes a distribution has two distinct clusters, each with its own local mode. The greater of these two modes is the major mode, and the lesser is the minor mode (See Figure 4.4).A bimodal distribution has two modes.\n\n\n\n\n\n\nFigure 4.4: A Bimodal Distribution\n\n\n\n\n\n R Code for Figure 4.4\n\n\n# A bimodal distribution\ntibble(x = seq(-3, 3.5, .01),\n       y = dnorm(x,-1, 0.5) / 0.8 + \n         dnorm(x, 1, 0.75)) %>%\n  ggplot(aes(x, y)) +\n  geom_area(fill = myfills[1]) +\n  geom_text(\n    data = . %>% dplyr::filter(x == -1),\n    vjust = -0.25,\n    label = \"Major\\nMode\",\n    size = 8,\n    lineheight = 0.9\n  ) +\n  geom_text(\n    data = . %>% dplyr::filter(x == 1),\n    vjust = -0.25,\n    label = \"Minor\\nMode\",\n    size = 8,\n    lineheight = 0.9\n  ) +\n  scale_x_continuous(NULL,\n                     minor_breaks = NULL,\n                     breaks = seq(-3, 3)) +\n  scale_y_continuous(NULL,\n                     breaks = NULL,\n                     expand = expansion(c(0, .25))) +\n  theme_minimal(base_size = 30,\n                base_family = bfont) +\n  theme(panel.grid.major.x = element_blank()) \n\n\n\n\nThe mode is the only measure of central tendency that be computed for all variable types and is the only choice for nominal variables (See Table 4.4).\nTo compute the mode of a variable, use the mfv (most frequent value) function from the modeest package (Poncet, 2019). In this example, the 2 occurs four times.\n\nPoncet, P. (2019). Modeest: Mode estimation. https://github.com/paulponcet/modeest\n\n# Data\nx <- c(1,2,2,2,2,3,3)\n# Mode\nmodeest::mfv(x)\n\n[1] 2\n\n\nThe mfv function will return all modes if there is more than one. In this example, the 1, 3, and 4 all occur twice.\n\n# Data\nx <- c(1,1,3,3,4,4,5,6)\n# Mode\nmodeest::mfv(x)\n\n[1] 1 3 4\n\n\n\n\n4.2.2 Median\nThe median is midpoint of a distribution, the point that divides the lower half of the distribution from the upper half. To calculate the median, you first need to sort the scores. If there is an odd number of scores, the median is the middle score. If there an even number of scores, it is the mean of the two middle scores. There are other definitions of the median that are a little more complex, but rarely is precision needed for calculating the median.The median is the point that divides the lower 50 percent of a distribution from the upper 50 percent.\nTo find the median using a frequency distribution table, find the first sample space element with a cumulative proportion greater than 0.5. For example, in the distribution shown in Table 4.3, the first cumulative proportion greater than 0.5 occurs at 5, which is therefore the median.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX\nFrequency\nCumulative Frequency\nProportion\nCumulative Proportion\n\n\n\n\n1\n1\n1\n.14\n.14\n\n\n5\n3\n4\n.43\n.57\n\n\n7\n1\n5\n.14\n.71\n\n\n9\n2\n7\n.29\n1.00\n\n\n\nTable 4.3: Finding the Median in a Frequency Distribution Table.In this case, the median is 5 because it has the first cumulative proportion that is greater than 0.5.\n\n\nIf a sample space element’s cumulative proportion is exactly 0.5, average that sample space element with the next highest value. For example, in the distribution in Table 4.1, the cumulative proportion for 4 is exactly 0.5 and the next value is 6. Thus the median is\n\\frac{4+6}{2}=5\nThe median can be computed for ordinal, interval, and ratio variables, but not for nominal variables (See Table 4.4). Because nominal variables have no order, no value is “between” any other value. Thus, because the median is the middle score and nominal variables have no middle, nominal variables cannot have a median.\nFor ordinal variables, the median is the preferred measure of central tendency because it is usually more stable from one sample to the next compared to the mode.\nIn R, the median function can compute the median:\n\nmedian(c(1,2,3))\n\n[1] 2\n\n\n\n\n4.2.3 Mean\nThe arithmetic mean is the sum of all values of a distribution divided by the size of the distribution.The arithmetic mean is the balance point of a disribution.\n\\mu_X = \\frac{\\sum_{i = 1}^n {X_i}}{n}\nWhere \\begin{align*}\n  \\mu_X &= \\text{The population mean of } X\\\\\n  n &= \\text{The number of values in } X\n\\end{align*}\nThe arithmetic mean can only be calculated with interval or ratio variables. Why? The formula for the mean requires adding numbers, and the operation of addition is not defined for ordinal and nominal values.\nThe arithmetic mean is usually the preferred measure of central tendency for interval and ration variables because it is usually more stable from sample to sample than the median and the mode. In Figure 4.5, it can be seen that the sampling distributions of the mean is narrower than that of the median and the mode. In other words, it has a smaller standard error.The standard error is the standard deviation of a sampling distribution.\n\n\n\n\n\n\nFigure 4.5: Sampling Distributions of Central Tendency Measures.The standard normal distribution, \\mathcal{N}(0,1), was used to generate 10,000 samples with a sample size of 100. The distribution of sample means is slightly narrower than the distribution of sample medians, meaning that the mean is slightly more stable than the median. The distribution of sample modes is very wide, meaning that the mode is much less stable than the mean and median.\n\n\n\n\n\n R Code for Figure 4.5\n\n\n# Central tendency function\nct <- function(sample_size = 100) {\n  x <- rnorm(sample_size)\n  mo <- DescTools::Mode(round(x,2))\n  \n  c(Mean = mean(x), \n       Median = median(x), \n       Mode = sample(mo, 1))\n}\n\n# Replicate samples\nd_ct <- replicate(10000, ct(100)) %>% \n  t() %>% \n  as_tibble() %>% \n  pivot_longer(cols = everything(), \n               names_to = \"CentralTendency\",\n               values_to = \"Value\") %>% \n  mutate(y = recode(CentralTendency, \n                    `Mode` = 0.03, \n                    `Median` = 0.07, \n                    `Mean` = 0.135))\n\n# Summary data\nd_sum <- d_ct %>% \n  filter(!is.na(Value)) %>% \n  group_by(CentralTendency) %>% \n  summarise(x = mean(Value),\n            y = mean(y),\n            ub = quantile(Value, 0.975),\n            lb = quantile(Value, 0.025)) %>% \n  rename(Value = x)\n\n# Plot\nd_ct %>% \n  filter(!is.na(Value)) %>% \n  ggplot(aes(Value, y)) +\n  stat_function(geom = \"area\", \n                fun = function(x) dnorm(x, 0, 1), \n                n = 1000, \n                color = NA, \n                fill = \"gray50\", \n                alpha = 0.2)  +\n  ggdist::stat_halfeye(aes(fill = CentralTendency), \n                       scale = 1.2, \n                       color = \"gray20\") +\n  geom_text(data = d_sum,\n            aes(label = CentralTendency),\n            vjust = 1.5,\n            size = ggtext_size(22),\n            color = \"gray20\") +\n  scale_x_continuous(name = NULL, \n                     limits = c(-3.5,3.5), \n                     breaks = seq(-4,4),\n                     labels = \\(x) signs::signs(x, accuracy = 1)) +\n  theme_minimal(base_size = 22, \n                base_family = bfont) +\n  theme(legend.position = \"none\", \n        panel.grid = element_blank()) +\n  scale_y_continuous(NULL, breaks = NULL, \n                     expand = expansion()) + \n  scale_fill_manual(values = scales::muted(\n    rep(myfills[1], 3),\n    l = c(65, 55, 40)))\n\n\n\n\nIn R, the mean function can compute the median:\n\nmean(c(1,2,3))\n\n[1] 2\n\n\nWatch out for missing values in R. If the distribution has even a single missning value, the mean function will return NA, as will most other summary functions in R (e.g., median, sd, var, and cor).\n\nmean(c(1,NA,3))\n\n[1] NA\n\n\nTo calculate the mean of all non-missing values, specify that all missing values shoule be removed prior to calculation like so:\n\nmean(c(1,NA,3), na.rm = TRUE)\n\n[1] 2\n\n\n\n\n4.2.4 Comparing Central Tendency Measures\nWhich measure of central tendency is best depends on what kind of variable is needed and also what purpuse it serves. Table 4.4 has a list of comparative features of each of the three major central tendency measures.\n\n\n\n\n\n\n\nFeature\nMode\nMedian\nMean\n\n\n\n\nComputable for Nominal Variables\nYes\nNo\nNo\n\n\nComputable for Ordinal Variables\nYes\nYes\nNo\n\n\nComputable for Interval Variables\nYes\nYes\nYes\n\n\nComputable for Ratio Variables\nYes\nYes\nYes\n\n\nAlgebraic Formula\nNo\nNo\nYes\n\n\nUnique Value\nNo\nYes\nYes\n\n\nSensitive to Outliers/Skewness\nNo\nNo\nYes\n\n\nStandard Error\nLarger\nSmaller\nSmallest\n\n\n\nTable 4.4: Comparing Central Tendency Measures"
  },
  {
    "objectID": "descriptives.html#expected-values",
    "href": "descriptives.html#expected-values",
    "title": "4  Descriptive Statistics",
    "section": "4.3 Expected Values",
    "text": "4.3 Expected Values\n(Unfinished)\nAt one level, the concept of the expected value of a random variable is really simple; it is just the population mean of the variable. So why don’t we just talk about population means and be done with this “expected value” business? It just complicates things! True. In this case, however, there is value in letting some simple things appear to become complicated for a while so that later we can show that some apparently complicated things are actually simple.The expected value of a random variable is the population mean of the values that the random variable generates.\nWhy can’t we just say that the expected value of a random variable is the population mean? You are familiar, of course, with the formula for a mean. You just add up the numbers and divide by the number of numbers n:\n\nm_X=\\frac{\\sum_{i=1}^{n} {x_i}}{n}\n\nFine. Easy. Except…hmm…random variables generate an infinite number of numbers. Dividing by infinity is tricky. We’ll have to approach this from a different angle…\nThe expected value of a random variable is a weighted mean. A mean of what? Everything in the sample space. How are the sample space elements weighted? Each element in the sample space is multiplied by its probability of occurring.\n\n\n\n\n\n\nFigure 4.6: Probability Distribution of a Hypothetical Random Variable\n\n\n\nSuppose that the sample space of a random variable X is {2, 4, 8} with respective probabilities of {0.3, 0.2, 0.5}, as shown in Figure 4.6.\n\n\n R Code for Figure 4.6\n\n\n\n\n\n\n\nThe notation for taking the expected value of a random variable X is \\mathcal{E}(X). Can we find the mean of this variable X even if we do not have any samples it generates? Yes. To calculate the expected value of X, multiply each sample space element by its associated probability and then take the sum of all resulting products. Thus,\n\n\\begin{align*}\n\\mathcal{E}(X)&=\\sum_{i=1}^{3}{p_i x_i}\\\\\n&= p_1x_1+p_2x_2+p_3x_3\\\\\n&= (.3\\times 2)+(.2\\times 4)+(.5\\times 8)\\\\\n&=5.4\n\\end{align*}\n\nThe term expected value might be a little confusing. In this case, 5.4 is the expected value of X but X never once generates a value of 5.4. So the expected value is not “expected” in the sense that we expect to see it often. It is expected to be close to the mean of any randomly selected sample of the variable that is sufficiently large:\n\n\n\n\n\n\nFigure 4.7: Slicing the Standard Normal Distribution into Ever Thinner Bins\n\n\n\n\n\\mathcal{E}(X)=\\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{i=1}^{n} {x_i}\n\nIf a random variable X is discrete, its expected value \\mathcal{E}(X) is the sum of each member of the sample space x_i multiplied by its probability of occurring p_i. The probability of occurring is the output of X’s probability density function at that location: p_i=f_X(x_i). Thus,\n\n\\mathcal{E}(X)=\\sum_{i=-\\infty}^{\\infty}{x_i f_X(x_i)}\n\nWith continuous variables, the number of elements in a sample is infinite. Fortunately, calculus was designed to deal with this kind of infinity. The trick is to imagine that the continuous variable is sliced into bins and that the bins are sliced ever more thinly. If a continuous random variable has probability density function f_X(x), the expected value is\n\n\\mathcal{E}(X)=\\int_{-\\infty}^{\\infty} {x f_X(x)\\,\\mathrm{d}x}\n\nIf we multiply each value of X by the height of its bin (p), we get the mean of the binned distribution. If the bins become ever thinner, as in Figure 4.7, the product of X and p approximates the expected value of the smooth continuous distribution.\n\n\n R Code for Figure 4.7\n\n\n# Slicing the standard normal distribution into ever thinner bins\nmake_bins <- function(binPower, binWidth, LowerBound, UpperBound) {\n  tibble(x = seq(LowerBound, UpperBound, binWidth), binPower, binWidth)\n}\n\npmap_df(tibble(binPower = 0:4, \n               binWidth = 2 ^ (-1 * binPower), \n               LowerBound = -4, \n               UpperBound = 4), \n        make_bins) %>% \n  mutate(p = pnorm(x + binWidth / 2) - pnorm(x - binWidth / 2),\n         width_label = factor(2 ^ binPower, \n                              levels = 2 ^ (0:4),\n                              labels = c(\"Width = 1\",\n                                         paste0(\"Width = 1/\",\n                                                2 ^ (1:4))))) %>%\n  ggplot(aes(x, p)) + \n  geom_col(aes(width = binWidth), \n           fill = myfills[1], \n           color = \"white\", \n           lwd = 0.1) + \n  facet_grid(width_label ~ .,  \n             scales = \"free\") + \n  theme_light(base_size = 24, \n              base_family = bfont) + \n  scale_x_continuous(NULL, \n                     breaks = -4:4, \n                     labels = function(x) signs::signs(x, accuracy = 1),\n                     # labels =  paste0(if_else(-4:4 < 0,\"−\",\"\"), abs(-4:4)),\n                     # sec.axis = dup_axis(),\n                     expand = c(0.01,0)) + \n  scale_y_continuous(NULL, \n                     breaks = NULL) +\n  theme(panel.grid = element_blank(), \n        # strip.text.y = element_blank(), \n        strip.placement = \"outside\",\n        strip.text.y = element_text(angle = 0),\n        axis.text.x = element_text(hjust = c(rep(.75,4),rep(0.5,5))))"
  },
  {
    "objectID": "descriptives.html#measures-of-variability",
    "href": "descriptives.html#measures-of-variability",
    "title": "4  Descriptive Statistics",
    "section": "4.4 Measures of Variability",
    "text": "4.4 Measures of Variability\n\n4.4.1 Variability of Nominal Variables\nFor most purposes, the visual inspection of a frequency distribution table or bar plot is all that is needed to understand a nominal variable’s variability. I have never needed a statistic that measures the variability of a nominal variable, but if you need one, there are many from which to choose. For example, Wilcox (1973) presented this analog to variance for nominal variables:\n\nWilcox, A. R. (1973). Indices of qualitative variation and political measurement. The Western Political Quarterly, 26(2), 325. https://doi.org/10.2307/446831\n\n\\text{VA} = 1-\\frac{1}{n^2}\\frac{k}{k-1}\\sum_{i=1}^k\\left(f_i-\\frac{n}{k}\\right)^2\n\nThe qualvar package (Gombin, 2018) can compute the primary indices of qualitative variation presented by Wilcox.\n\nGombin, J. (2018). Qualvar: Implements indices of qualitative variation proposed by wilcox (1973). http://joelgombin.github.io/qualvar/\n\n\n\n\n\n\nFigure 4.8: The Variance Analog (VA) index of qualitative variation ranges from 0 to 1. It equals 0 when every data point is assigned to the same category and 1 when each category has the same frequency.\n\n\n\n\nlibrary(qualvar)\n\n# Frequencies\nfrequencies =  c(A = 60, B = 10, C = 25, D = 5)\n\n# VA\nVA(frequencies)\n\n[1] 0.7533333\n\n\nIn all of these indices of qualitative variation, the lowest value is 0 when every data point belongs to the same category (See Figure 4.8, left panel). Also, the maximum value is 1 when the data points are equally distributed across categories (See Figure 4.8, right panel).\n\n\n R Code for Figure 4.8\n\n\n# The Variance Analog (VA) index of qualitative variation\nlow_var <- c(A = 100, B = 0, C = 0, D = 0)\nmid_var =  c(A = 60, B = 10, C = 25, D = 5)\nhigh_var = c(A = 25, B = 25, C = 25, D = 25)\n\n\ntibble(Variability = c(\"Low\", \"Middle\", \"High\"),\n       Frequency = list(low_var, mid_var, high_var),\n       VA = map_dbl(Frequency, VA)) %>% \n  mutate(Frequency = map(Frequency, function(d) as.data.frame(d) %>% \n                           tibble::rownames_to_column(\"Category\")),\n         Variability = paste0(Variability, \n                              \"\\nVA = \", \n                              prob_label(VA)) %>% \n           fct_inorder()) %>% \n  unnest(Frequency) %>%  \n  rename(Frequencies = d) %>% \n  ggplot(aes(Category, Frequencies)) +\n  geom_col(aes(fill = Variability)) + \n  geom_text_fill(aes(label = Frequencies), \n             vjust = -.3, \n             color = \"gray30\", \n             size = ggtext_size(30)) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.08)), \n                     breaks = seq(0,100,20), \n                     minor_breaks = seq(0,100,10)) +\n  scale_fill_manual(values = myfills) +\n  facet_grid(cols = vars(Variability)) + \n  theme_light(base_family = bfont, base_size = 30) + \n  theme(panel.grid.major.x = element_blank(), \n        legend.position = \"none\")\n\n\n\n\n\n\n4.4.2 Interquartile Range\nAs with nominal variables, a bar plot or frequency distribution table can tell you most of what you want to know about the variability of an ordinal variable. If you need a quantitative measure of how much an ordinal variable varies, you have many options. The most important of these is the interquartile range.The interquartile range (IQR) is the distance from the score at the 25th percentile to the score at the 75th percentile.\n\n\n\n\n\n\nFigure 4.9: In a normal distribution with a mean of 100 and a standard deviation of 15, the interquartile range is about 20.2, the distance between 89.9 and 110.1.\n\n\n\nWhen median is a good choice for our central tendency measure, the interquartile range is usually a good choice for our variability measure. Whereas the median is the category that contains the 50th percentile in a distribution, the interquartile range is the distance between the categories that contain the 25th and 75th percentile. That is, it is the range of the 50 percent of data in the middle of the distribution. For example, in Figure 4.9, the shaded region is the space between the 25th and 75th percentiles in a normal distribution. The IQR is the width of the shaded region, about 20.2.\n\n\n R Code for Figure 4.9\n\n\nIQR_bounds <- qnorm(c(.25, .75), mean = 100, sd = 15)\nl_height = .05\nggplot(data = tibble(x = c(40, 160), y = pnorm(x, 100, 15)), aes(x)) +\n  stat_function(fun = \\(x) dnorm(x, 100, 15),\n                geom = \"area\",\n                alpha = 0.1) +\n  stat_function(\n    fun = \\(x) dnorm(x, 100, 15),\n    geom = \"area\",\n    xlim = qnorm(c(.25, .75), mean = 100, sd = 15),\n    fill = myfills[1],\n    alpha = 0.5\n  ) +\n  scale_y_continuous(NULL, breaks = NULL, expand = expansion()) +\n  scale_x_continuous(NULL, breaks = seq(40, 160, 15)) +\n  theme_minimal(base_size = 28, base_family = bfont) +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    axis.ticks.x = element_line(color = \"gray30\"), plot.margin = margin()\n  ) +\n  annotate(\n    x = IQR_bounds,\n    y = 0,\n    size = ggtext_size(28),\n    geom = \"richtext\", label.color = NA, fill = NA,\n    label = paste0(\"**\", round(IQR_bounds, 1),\"**<br><span style='font-size:22pt'>\", c(25, 75), \"<sup>th</sup><br>percentile\"),\n    hjust = c(1, 0),\n    lineheight = .9,\n    vjust = 0\n  ) + \n  annotate(x = IQR_bounds[1] + 1.5, \n           y = 0, \n           xend = IQR_bounds[2] - 1.5, \n           yend = 0, \n           linewidth = 1.5, \n           linejoin = \"mitre\",\n           geom = \"segment\", arrow = arrow(length = unit(5, units = \"mm\"), \n                                           angle = 15,\n                                           ends = \"both\", \n                                           type = \"closed\")) + \n  annotate(x = 100, \n           y = 0,\n           size = ggtext_size(28),\n           label = paste0(\"*IQR*<br>=\", round(IQR_bounds[2] - IQR_bounds[1], 1)),\n           geom = \"richtext\", fill = NA, label.color = NA,\n           vjust = 0) + \n  coord_cartesian(clip = \"off\")\n\n\n\n\nIn ordinal data, there is no distance between categories, thus we cannot report the interquartile range per se. However, we can report the categories that contain the 25th and 75th percentiles. In Figure 4.10, the interquartile range has its lower bound at Disagree and its upper bound at Slightly Agree.”\n\n\n\n\n\n\nFigure 4.10: In this ordinal variable, the interquartile range has a lower bound at Disagree (which contains the 25th percentile) and an upper bound at Slightly Agree (which contains the 75th percentile).\n\n\n\n\n\n R Code for Figure 4.10\n\n\nd <- tibble(\n  Agreement = c(\n    \"Strongly Disagree\",\n    \"Disagree\",\n    \"Slightly Disagree\",\n    \"Slightly Agree\",\n    \"Agree\",\n    \"Strongly Agree\"\n  ),\n  n = c(23, 85, 93, 121, 20, 26),\n  p = n / sum(n),\n  cp = cumsum(p),\n  ymin = lag(cp, default = 0),\n  ytext = ymin + p / 2\n) \n\nd %>% \n  mutate(Agreement = fct_inorder(Agreement) %>% fct_rev()) %>% \n  ggplot(aes(p,cp)) + \n  geom_rect(aes(ymin = ymin, \n                ymax = cp, \n                xmin = 0, \n                xmax = 1, \n                fill = Agreement)) + \n  geom_label(aes(x = 1, label = paste0(round(cp * 100), \"%\")), hjust = 0, label.size = 0, color = \"gray30\") +\n  geom_text(aes(x = 0.5, y = ytext, \n                label = paste0(Agreement, \n                               \" (\", \n                               round(100 * p), \"%)\")),\n            size = WJSmisc::ggtext_size(18),\n            color = \"gray10\") + \n  # geom_hline(yintercept = c(0.25, 0.5, .75), size = 0.2, color = \"gray30\") +\n  scale_y_continuous(\"Cumulative Proportion\", \n                     minor_breaks = NULL, \n                     labels = \\(x) paste0(round(x * 100), \"%\"), \n                     expand = expansion(), limits = c(0,1)) + \n  scale_x_continuous(\"Agreement\", breaks = NULL, expand = expansion(add = c(0,.2))) + \n  scale_fill_manual(values = rev(c(rev(tinter(myfills[1], steps = 4)[1:3]), \n                               tinter(myfills[2], steps = 4)[1:3]))) + \n  theme(legend.position = \"none\", axis.ticks.y = element_line(\"gray30\"), panel.grid.major.y = element_blank()) +\n  coord_cartesian(clip = \"off\")\n\n\n\n\nThe median and the interquartile range are displayed in box and whiskers plots like Figure 4.11. The height of the box is the interquartile range, and the horizontal line is the median. The top “whisker” extends no higher than 1.5 × IQR above the 75th percentile. The bottom “whisker” extends no lower than 1.5 × IQR below the 25th percentile. Any data points outside the whiskers can be considered outliers.\n\n\n\n\n\nFigure 4.11: A Tukey-style Box and Whiskers Plot with Medians and Interquartile Ranges.\n\n\n\n\n\n\n R Code for Figure 4.11\n\n\nset.seed(2)\nd <-\n  tibble(A = rnorm(100, 50, 10),\n       C = 1.5 * rchisq(100, 4) + 50,\n       B = rbeta(100, 4.5, .5) * 80 ) %>% \n  pivot_longer(cols = everything(), names_to = \"grp\", values_to = \"x\") %>% \n  mutate(grp = factor(grp)) %>% \n  group_by(grp) %>% \n  mutate(md = median(x), IQR = IQR(x),\n         q25 = quantile(x, .25),\n         q75 = quantile(x, .75)) %>% \n    ungroup() %>% \n    mutate(outlier = ifelse(x > md, \n                            x - q75 > IQR * 1.5,\n                            q25 - x >  IQR * 1.5))\n\nd_stats <- d %>% \n  group_by(grp) %>% \n  summarise(md = median(x),\n            q25 = quantile(x, .25),\n            q75 = quantile(x, 0.75)) %>% \n  pivot_longer(cols = -grp, names_to = \"stats\", values_to = \"x\") %>% \n  mutate(st = case_when(stats == \"md\" ~ \" (Median)\",\n                        stats == \"q25\" ~ \" (1<sup>st</sup> Quartile)\",\n                        stats == \"q75\" ~ \" (3<sup>rd</sup> Quartile)\"))\n\nwidth = .3\n\nd %>% \n  ggplot(aes(grp, x)) + \n  geom_boxplot(aes(fill = grp), outlier.color = NA, width = width * 2) + \n  geom_richtext(data = d_stats, \n            aes(label = paste0(scales::number(x, .1), ifelse(grp == \"A\", st, \"\"))), \n            nudge_x = width + .01, \n            label.color = NA,\n            hjust = 0, \n            color = \"gray20\") + \n  geom_segment(data = select(d, grp, q25, q75) %>% unique(),\n               aes(yend = q75,\n                   y = q25,\n                   x = as.numeric(grp) - width - 0.05,\n                   xend = as.numeric(grp) - width - 0.05),\n               arrow = arrow(angle = 15, length = unit(3, \"mm\"), ends = \"both\", type = \"closed\")) +\n    geom_richtext(data = select(d, grp, q25, q75, IQR) %>% unique(),\n               aes(x = as.numeric(grp) - width - 0.05,\n                   y = (q25 + q75) / 2,\n                   label = paste0(ifelse(grp == \"A\", \"*IQR* = \",\"\"), scales::number(IQR, .1))), \n               angle = 90, \n               vjust = -0.2,\n               label.color = NA,\n               label.padding = margin(t = 3)) +\n  ggbeeswarm::geom_quasirandom(pch = 16, size = 1, aes(color = outlier), width = .3) +\n  scale_x_discrete(\"Group\") + \n  scale_y_continuous(NULL) +\n  scale_fill_manual(values = myfills %>% scales::alpha(.5)) + \n  scale_color_manual(values = c(\"gray20\", \"firebrick\")) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n4.4.3 Variance\nA deviation is computed by subtracting a score from its mean:A deviation is the distance of a score from the score’s mean.\nX-\\mu\nWe would like to know the typical size of the deviation X-\\mu. To do so, it might seem intuitively correct to take the average (i.e., expected value) of the deviation, but this quantity is always 0:\n\n\\begin{aligned}\n\\mathcal{E}(X-\\mu)&=\\mathcal{E}(X)-\\mathcal{E}(\\mu)\\\\\n&=\\mu-\\mu\\\\\n&=0\n\\end{aligned}\n\nBecause the average deviation is always 0, it has no utility as a measure of variability. It would be reasonable to take the average absolute value of the deviations, but absolute values often cause algebraic difficulties later when we want to use them to derive other statistics. A more mathematically tractable solution is to make each deviation positive by squaring them.\nVariance \\left(\\sigma^2\\right) is the expected value of squared deviations from the mean \\left(\\mu\\right):Variance is a measure of variability that gives the size of the average squared deviation from the mean.\n\n\\begin{aligned}\n\\sigma^2&=\\mathcal{E}\\!\\left(\\left(X-\\mu\\right)^2\\right)\\\\\n&=\\mathcal{E}\\!\\left(X^2-2X\\mu+\\mu^2\\right)\\\\\n&=\\mathcal{E}\\!\\left(X^2\\right)-\\mathcal{E}\\!\\left(2X\\mu\\right)+\\mathcal{E}\\!\\left(\\mu^2\\right)\\\\\n&=\\mathcal{E}\\!\\left(X^2\\right)-2\\mu\\,\\mathcal{E}\\!\\left(X\\right)+\\mu^2\\\\\n&=\\mathcal{E}\\!\\left(X^2\\right)-2\\mu\\mu+\\mu^2\\\\\n&=\\mathcal{E}\\!\\left(X^2\\right)-2\\mu^2+\\mu^2\\\\\n&=\\mathcal{E}\\!\\left(X^2\\right)-\\mu^2\\\\\n\\end{aligned}\n\nIf all elements of a population with mean \\mu are known, the population variance is calculated like so:\n\n\\sigma^2=\\frac{\\sum_i^n{\\left(x_i-\\mu\\right)^2}}{n}\n\nNotice that the population variance’s calculation requires knowing the precise value of the population mean. Most of the time, we need to estimate the population mean \\mu using a sample mean m. A sample variance \\left(s^2\\right) for a sample size n can be calculated like so:\n\ns^2=\\frac{\\sum_i^n{\\left(x_i-m\\right)^2}}{n-1}\n\n\n\n\n\n\n\nFigure 4.12: Friedrich Wilhelm Bessel (1784–1846)Image Credits\n\n\n\nUnlike with the population variance, we do not divide by the sample size n to calculate the sample variance. If we divided by n, the sample variance would be negatively biased (i.e., it is likely to underestimate the population variance). In what is known as Bessel’s correction (i.e, dividing by n-1 instead of by n), we get an unbiased estimate of the variance.\nVariance is rarely used for descriptive purposes because it is a squared quantity with no direct connection to the width of the distribution it describes. We mainly use variance as a stepping stone to compute other descriptive statistics (e.g., standard deviations and correlation coefficients) and as an essential ingredient in inferential statistics (e.g., analysis of variance, multiple regression, and structural equation modeling). However, Figure 4.13 attempts a visualization of what variance represents. Along the X-axis, the values of a normally distributed variable X are plotted as points. The Y-axis represents the deviations of variable X from \\mu, the mean of X. For each value of X, we can create a square with sides as long as the deviations from \\mu. The red squares have a positive deviation and the blue squares have a negative deviation. The darkness of the color represents the magnitude of the deviation. The black square has an area equal to the average area of all the squares. Its sides have a length equal to the standard deviation, the square root of variance.\n\n\n\n\n\nFigure 4.13: Visualizing Variance.The values of variable X are plotted with the deviations of X. Each square is a deviation from the mean of X. Darker squares have larger deviations. The area of the thick black square is the variance—the average size of the squared deviations.\n\n\n\n\n\n\n R Code for Figure 4.13\n\n\n# Visualizing Variance\n\nset.seed(1)\nx <- rnorm(70, 10, 3)\nmu <- mean(x)\nsigma <- sd(x)\nxbreaks <- pretty(x)\nybreaks <- pretty(x - mu)\n\ntick_width <- .04 * sigma\n\ntibble(x = x,\n       deviations = x - mu,\n       abval = abs(deviations)) |>\n  arrange(-abval) |>\n  ggplot(aes(x, deviations)) +\n  geom_rect(\n    aes(\n      xmax = x,\n      xmin = mu,\n      ymax = deviations,\n      ymin = 0,\n      fill = deviations\n    ),\n    color = \"gray95\",\n    linewidth = .1\n  ) +\n  annotate(\n    \"segment\",\n    yend = max(ybreaks) + tick_width,\n    y = min(ybreaks) - tick_width,\n    x = mu,\n    xend = mu,\n    color = \"gray30\"\n  ) +\n  annotate(\n    \"segment\",\n    xend = max(xbreaks) + tick_width,\n    x = min(xbreaks) - tick_width,\n    y = 0,\n    yend = 0,\n    color = \"gray30\"\n  ) +\n  coord_equal() +\n  annotate(\n    \"rect\",\n    xmax = mu + sigma,\n    xmin = mu,\n    ymax = sigma,\n    ymin = 0,\n    fill = NA,\n    color = \"gray10\",\n    linewidth = 1\n  ) + \n  annotate(\n    \"richtext\",\n    x = mu + sigma / 2,\n    y = sigma,\n    label = paste0(\"*\",span_style(\"&sigma;\"),\"* = \", \n                   scales::number(sigma, accuracy = .01)),\n    label.color = NA,\n    fill = NA,\n    vjust = 0,\n    family = bfont,\n    size = ggtext_size(16),\n    color = \"gray20\"\n  ) + \n  annotate(\n    \"richtext\",\n    x = mu + sigma,\n    y = sigma / 2,\n    angle = 90,\n    label = paste0(\"*\",span_style(\"&sigma;\"),\"* = \", \n                   scales::number(sigma, accuracy = .01)),\n    label.color = NA,\n    fill = NA,\n    vjust = 1.1,\n    family = bfont,\n    size = ggtext_size(16),\n    color = \"gray20\"\n  ) +\n  geom_richtext(\n    data = tibble(\n      x = xbreaks,\n      deviations = 0,\n      vjust = ifelse(xbreaks < mu, 0, 1.05)\n    ),\n    aes(label = x, vjust = vjust),\n    family = bfont,\n    label.color = NA,\n    label.padding = unit(c(.2, 0, .2, 0), \"lines\"),\n    fill = NA,\n    size = ggtext_size(16),\n    color = \"gray20\"\n  ) +\n  geom_richtext(\n    data = tibble(\n      deviations = ybreaks,\n      x = mu,\n      hjust = ifelse(ybreaks < 0, 0, 1.05)\n    ) |>\n      dplyr::filter(deviations != 0),\n    aes(\n      label = signs::signs(deviations, accuracy = 1),\n      hjust = hjust\n    ),\n    family = bfont,\n    label.color = NA,\n    fill = NA,\n    label.padding = unit(c(0, .4, 0, .4), \"lines\"),\n    size = ggtext_size(16),\n    color = \"gray20\"\n  ) + annotate(\"point\",\n               x = mu,\n               y = 0,\n               size = 3) +\n  annotate(\n    \"richtext\",\n    x = mu,\n    y = 0,\n    label = paste0(\n      \"*\",\n      span_style(\"&mu;\"),\n      \"* = \",\n      scales::number(mu, accuracy = .01),\n      \"\"\n    ),\n    label.color = NA,\n    fill = NA,\n    vjust = 0.5,\n    hjust = -.1,\n    angle = -45,\n    family = bfont,\n    size = ggtext_size(bsize),\n    color = \"gray20\"\n  ) +\n  geom_segment(\n    data = tibble(x = xbreaks),\n    aes(\n      x = xbreaks,\n      y = tick_width,\n      yend = -tick_width,\n      xend = xbreaks\n    ),\n    color = \"gray20\"\n  ) +\n  geom_segment(\n    data = tibble(y = ybreaks) |> dplyr::filter(y != 0),\n    aes(\n      y = y,\n      x = mu + tick_width,\n      xend = mu - tick_width,\n      yend = y\n    ),\n    color = \"gray20\"\n  ) +\n  theme_void(base_family = bfont, base_size = bsize) +\n  theme(legend.position = \"right\",\n        legend.title = element_text(angle = 90, \n                                    margin = margin(l = -15, r = -24))) +\n  scale_fill_gradient2(\n    \"Deviations\",\n    midpoint = 0,\n    low = myfills[1],\n    high = myfills[2],\n    mid = \"white\",\n    breaks = ybreaks,\n    limits = c(-8, 8),\n    labels = \\(x) signs(x, accuracy = 1)\n  ) +\n  guides(\n    fill = guide_colorbar(\n      title = \"Deviations\",\n      label.position = \"right\",\n      title.position = \"left\",\n      title.hjust = 0.5,\n      direction = \"vertical\",\n      barheight = unit(.9, \"npc\")\n    )\n  ) +\n  annotate(\n    \"text\",\n    x = min(xbreaks) - tick_width * 4,\n    y = 0,\n    label = \"X\",\n    family = bfont,\n    color = \"gray20\",\n    fontface = \"italic\",\n    size = ggtext_size(bsize, 1)\n  ) + \n  annotate(\"richtext\",\n           label.color = NA,\n           fill = NA,\n           x = mu + sigma / 2,\n           y = sigma / 2,\n           vjust = 0.5,\n           size = ggtext_size(bsize),\n           label = paste0(\"*\", \n                          span_style(\"&sigma;\"), \n                          \"*<sup>2</sup> = \", \n                          scales::number(sigma ^ 2, accuracy = .01)),\n           family = bfont,\n           color = \"gray20\") +\n  geom_point(pch = 16, size = 1.7, color = \"black\", y = 0, alpha = .8) \n\n\n\n\n\n\n4.4.4 Standard Deviation\nThe standard deviation is by far the most common measure of variability. The standard deviation \\sigma is the square root of the variance \\sigma^2.The standard deviation is a measure of variabily that estimates the typical distance of a score from its own mean.\n\n\\begin{aligned}\n\\sigma&=\\sqrt{\\sigma^2}=\\sqrt{\\frac{\\sum_i^n{\\left(x_i-\\mu\\right)^2}}{n}}\\\\\ns&=\\sqrt{s^2}=\\sqrt{\\frac{\\sum_i^n{\\left(x_i-m\\right)^2}}{n-1}}\n\\end{aligned}\n\nAlthough it is not an arithmetic average of the deviations, it can be thought of as representing the typical size of the deviations. Technically, it is the square root of the average squared deviation.\nIn a normal distribution, the standard deviation is the distance from the mean to the two inflection points in the probability density curve (see Figure 4.14).An inflection point in a curve is the point at which the curvature changes from upward to downward or vice versa.\n\n\n\n\n\nFigure 4.14: The inflection points in the normal curve are 1 standard deviation from the mean.\n\n\n\n\n\n\n R Code for Figure 4.14\n\n\n# The inflection points in the normal curve \n# are 1 standard deviation from the mean.\n\nd_text <- tibble(x = c(-1, 1,-1.4, 1.4, -.65,.65),\n                 y = dnorm(x),\n                 angle = c(0,0,68,-68, 67,-67),\n                 hjust = c(1,0,rep(.5, 4)),\n                 vjust = c(0.5,0.5, 0.1, 0.1, 0.05, 0.05),\n                 label = c(paste0(\"Inflection point at &minus;1<em>\", span_style(\"&sigma;\"), \"</em>\"),\n                           paste0(\"Inflection point at +1<em>\", span_style(\"&sigma;\"), \"</em>\"),\n                           \"Upward curve\",\n                           \"Upward curve\",\n                           \"Downward curve\",\n                           \"Downward curve\"),\n                 color = c(\"black\", \"black\", myfills[1], myfills[1], myfills[2], myfills[2])\n                 )\n\nggplot(tibble(x = c(-4,4)), aes(x)) + \n  stat_function(fun = dnorm, geom = \"area\", alpha = 0.4, fill = myfills[1], xlim = c(-4,-1)) +\n  stat_function(fun = dnorm, geom = \"area\", alpha = 0.4, fill = myfills[2], xlim = c(-1,1)) +\n  stat_function(fun = dnorm, geom = \"area\", alpha = 0.4, fill = myfills[1], xlim = c(1,4)) +\n  annotate(x = c(-1,1), y = dnorm(c(-1,1)), geom = \"point\") + \n  geom_richlabel(aes(color = color, label = label, y = y, angle = angle, hjust = hjust, vjust = vjust), data = d_text, text_size = 18) + \n  scale_color_identity() +\n  scale_x_continuous(\"Standard Deviation Units\", \n                     breaks = -4:4, \n                     labels = paste0(signs::signs(-4:4, \n                                                  accuracy = 1, \n                                                  label_at_zero = \"blank\", \n                                                  add_plusses = T),\n                                     \"<em>\",\n                                     span_style(c(rep(\"&sigma;\", 4), \"&mu;\", rep(\"&sigma;\", 4))), \n                                     \"</em>\")) +\n  scale_y_continuous(NULL, breaks = NULL, limits = c(0,NA), expand = expansion()) +\n  coord_fixed(12)  +\n  geom_vline(xintercept = 0, color = \"gray30\") +\n  theme(panel.grid = element_blank(), \n        axis.text.x = element_markdown())\n\n\n\n\n\n\n4.4.5 Average Absolute Deviations\nAverage absolute deviations summarize the absolution values of deviations from a central tendency measure. There are many kinds of average absolute deviations, depending which central tendency each value deviates from and then which measure of central tendency summarizes the absolute deviations. With three central tendency measures, we can imagine nine different “average” absolute deviations:\n\\text{The}~\\begin{bmatrix}mean\\\\ median\\\\ modal\\end{bmatrix}~\\text{absolute deviation around the}~\\begin{bmatrix}mean\\\\ median\\\\ mode\\end{bmatrix}.\nThat said, two of these average absolute values are used more than the others: the median deviation around the median and mean deviation around the mean. One struggles to imagine what uses one might have for the others (e.g, the modal absolute deviation around the median or the mean absolute deviation around the mode).\n\n4.4.5.1 Median Absolute Deviation (around the Median)\nSuppose that we have a random variable X, which has a median of \\tilde{X}. The median absolute deviation (MAD) is the median of the absolute values of the deviations from the median:\n\n\\text{Median Absolute Deviation (around the Median)}=\\mathrm{median}\\left(\\left|X-\\tilde{X}\\right|\\right)\n\nA primary advantage of the MAD over the standard deviation is that it is robust to the presence of outliers. For symmetric distributions, the MAD is half the distance of the interquartile range.\n\n\n4.4.5.2 Mean Absolute Deviation (around the Mean)\nIf the mean of random variable X is \\mu_X, then:\n\n\\text{Mean Absolute Deviation (around the Mean)}=\\mathrm{mean}\\left(\\left|X-\\mu_X\\right|\\right)\n\nThe primary advantage of this statistic is that it is easy to explain to people with no statistical training. In a straightforward manner, it tells us how far each score is from the mean, on average."
  },
  {
    "objectID": "descriptives.html#skewness",
    "href": "descriptives.html#skewness",
    "title": "4  Descriptive Statistics",
    "section": "4.5 Skewness",
    "text": "4.5 Skewness\n(Unfinished)"
  },
  {
    "objectID": "descriptives.html#kurtosis",
    "href": "descriptives.html#kurtosis",
    "title": "4  Descriptive Statistics",
    "section": "4.6 Kurtosis",
    "text": "4.6 Kurtosis\n(Unfinished)"
  },
  {
    "objectID": "descriptives.html#raw-moments",
    "href": "descriptives.html#raw-moments",
    "title": "4  Descriptive Statistics",
    "section": "5.1 Raw Moments",
    "text": "5.1 Raw Moments\nThe first raw moment \\mu'_1 of a random variable X is its expected value:\n\\mu'_1=\\mathcal{E}(X)=\\mu_X\nThe second raw moment \\mu'_2 is the expected value of X^2:\n\\mu'_2=\\mathcal{E}(X^2)\nThe nth raw moment \\mu'_n is the expected value of X^n:\n\\mu'_n=\\mathcal{E}(X^n)"
  },
  {
    "objectID": "descriptives.html#central-moments",
    "href": "descriptives.html#central-moments",
    "title": "4  Descriptive Statistics",
    "section": "5.2 Central Moments",
    "text": "5.2 Central Moments\nThe first raw moment, the mean, has obvious significance and is easy to understand. The remaining raw moments do not lend themselves to easy interpretation. We would like to understand the higher moments after accounting for the lower moments. For this reason, we can discuss central moments, which are like raw moments after subtracting mean.\nOne can evaluate a moment “about” a constant c like so:11 Alternately, we can say that this is the nth moment referred to c.\n\n\\text{The }n\\text{th moment of }X\\text{ about }c=\\mathcal{E}\\left(\\left(X-c\\right)^n\\right)\n\nA central moment \\mu_n is a moment about the mean (i.e., the first raw moment):A central moment is a raw moment after the variable’s mean has been subracted.\n\n\\mu_n=\\mathcal{E}\\left(\\left(X-\\mu_X\\right)^n\\right)\n\nThe first central moment \\mu_1 is not very interesting, because it always equals 1:\n\n\\begin{aligned}\\mu_1&=\\mathcal{E}\\left(\\left(X-\\mu_X\\right)^1\\right)\\\\\n&=\\mathcal{E}\\left(\\left(X-\\mu_X\\right)\\right)\\\\\n&=\\mathcal{E}\\left(X\\right)-\\mathcal{E}\\left(\\mu_X\\right)\\\\\n&=\\mu_X-\\mu_X\\\\\n&=0\n\\end{aligned}\n\nOf note, the second central moment \\mu_2 is the variance:\n\n\\mu_2=\\mathcal{E}\\left(\\left(X-\\mu_X\\right)^2\\right)=\\sigma_X^2"
  },
  {
    "objectID": "descriptives.html#standardized-moments",
    "href": "descriptives.html#standardized-moments",
    "title": "4  Descriptive Statistics",
    "section": "5.3 Standardized Moments",
    "text": "5.3 Standardized Moments\nA standardized moment2 is the raw moment of a variable after it has been “standardized” (i.e., converted to a z-score):2 Standardized moments are also called normalized central moments.\n\n\nStandardizing a variable by converting it to z-score is accomplished like so: \nz=\\frac{X-\\mu_X}{\\sigma_X}\n\n\n\\text{The }n\\text{th standardized moment} = \\mathcal{E}\\left(\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right)^n\\right)=\\frac{\\mu_n}{\\sigma^n}\n\nAlternatively, we could say that a standardized moment is a moment after accounting for the first raw moment \\mu'_1 and the second central moment \\mu_2:"
  },
  {
    "objectID": "compositescores.html#the-mean-of-the-sum-is-the-sum-of-the-means",
    "href": "compositescores.html#the-mean-of-the-sum-is-the-sum-of-the-means",
    "title": "5  Composite Scores",
    "section": "5.1 The Mean of the Sum Is the Sum of the Means",
    "text": "5.1 The Mean of the Sum Is the Sum of the Means\nThis section is going to make a simple idea look complicated. If you get lost, this is what I am trying to say:\n\nIf we create a new random variable by adding a bunch of random variables together, the mean of that new variable is found by adding together the means of all the variables we started with.\n\nI’m sorry for what comes next, but the work we put into it will pay off later. Okay, now let’s make that simple idea formal and hard to understand:\nWe can start with the example of two variables: X_1 and X_2. The sum of these variables, which we will call X_S, has a mean, which is the sum of the means of X_1 and X_2. That is, \\mu_{S}=\\mu_{1}+\\mu_{2}.\nWhat if we have three variables? or four? or five? It would be tedious to illustrate each case one by one. We need a way of talking about the sum of a bunch of random variables but without worrying about how many variables there are. Here we go:\n\n5.1.1 Calculating a Sum\nSuppose k is a positive integer greater than 1. So if there are k random variables, the notation for the set of all them is \\{X_1,...,X_k\\}. However, it is even more compact to use matrix notation such that \\boldsymbol{X}=\\{X_1,...,X_k\\}.\nNow, \\boldsymbol{X} is a set of random variables in their entirety, without referencing any particular values those variables might generate. A set of particular values of these variables would be shown as \\boldsymbol{x} or \\{x_1,...,x_k\\}. In regular notation, the sum of these particular values would be:\nx_S=\\sum_{i=1}^{k}{x_i}\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\nx_S\nThe sum of all k scores in \\{x_1,...,x_k\\}\n\n\nx_i\nA particular score generated by variable X_i\n\n\nk\nThe number of variables in \\{X_1,...,X_k\\}, (k \\in \\mathbb{N}_1)\n\n\n\n\n\nThe same formula is more compact in matrix notation:\n\nx_S=\\boldsymbol{x'1}\n\nWhere\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\boldsymbol{x}\nA k \\times 1 vector of scores \\{x_1,x_2,...,x_k\\}\n\n\n\\boldsymbol{1}\nA k \\times 1 vector of ones \\{1_1,1_2,...,1_k\\}\n\n\n\n\n\nThe \\boldsymbol{1} symbol may be a bit confusing. It is a column of ones that has the same length (number of elements) as \\boldsymbol{x}. Suppose that \\boldsymbol{x} has a length of three. In this case:\n\\boldsymbol{1}_3=\\begin{bmatrix}1\\\\ 1\\\\ 1 \\end{bmatrix}\nAlso, \\boldsymbol{x'}, is \\boldsymbol{x} transposed.To transpose means to make all columns of a matrix into rows (or all rows into columns).\n\n\nTransposition is noted with a prime symbol (\\boldsymbol{^\\prime}). If\n\n\\boldsymbol{A}= \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n\\end{bmatrix}\n then\n\n\\boldsymbol{A'}= \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\\\\\n\\end{bmatrix}\n\nA column of 3 ones:\n\n\\boldsymbol{1}_3=\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\n\nTransposed, a column of 2 ones is a row of 2 ones.\n\n\\boldsymbol{{1'}}_2=\\begin{bmatrix}1&1\\end{bmatrix}\n\nTypically, the number of ones is implied such that the length of the column or row will be compatible with the adjacent matrix. For example, post-multiplying by a vector ones will create a vector of row totals:\n\n\\begin{align*}\n\\boldsymbol{A1}&=\\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n\\end{bmatrix}\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\\\\\n&=\\begin{bmatrix}1+2+3\\\\4+5+6\\end{bmatrix}\\\\\n&=\\begin{bmatrix}6\\\\15\\end{bmatrix}\n\\end{align*}\n\nPre-multiplying by a vector of ones will create column totals:\n\n\\begin{align*}\n\\boldsymbol{1'A}&=\\begin{bmatrix}1\\\\1\\end{bmatrix}'\\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}1&1\\end{bmatrix}\\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}1+4 & 2+5 & 3+6\\end{bmatrix}\\\\&=\\begin{bmatrix}5 & 7 & 9\\end{bmatrix}\n\\end{align*}\n\nTo create a sum of the entire matrix, multiply by \\boldsymbol{1} on both sides:\n\\boldsymbol{1'A1}=21\nTherefore, \\boldsymbol{x'} is a row vector. A row vector multiplied by a column vector is the sum of the product of each analogous element in the pair of vectors. Thus,\n\n\\begin{align*}\n\\boldsymbol{x'1}&=\\begin{bmatrix} x_1\\\\ x_2\\\\ x_3 \\end{bmatrix}'\n\\begin{bmatrix}1\\\\ 1\\\\ 1 \\end{bmatrix} \\\\\n&=\\begin{bmatrix} x_1 & x_2 & x_3\\end{bmatrix}\n\\begin{bmatrix}1\\\\ 1\\\\ 1\\end{bmatrix}\\\\\n&=x_1 \\times 1 + x_2 \\times 1 + x_3 \\times 1\\\\\n&=  x_1 + x_2 + x_3\n\\end{align*}\n\nLet’s do some calculations in R with a particular example. Suppose that there are three variables: \\boldsymbol{X}=\\{X_1, X_2, X_3\\}. In R, we will create a vector of the names of variables in \\boldsymbol{X}:\n\nXnames <- c(\"X1\",\"X2\",\"X3\")\n# Notes:\n# Xnames is a vector of variable names\n# The c function combines numbers (or other objects) into a vector.\n\nNow suppose that there are three particular scores: \\boldsymbol{x}=\\{100,120,118\\}\n\n# x = vector of particular scores from variables X1, X2, X3\nx <- c(110,120,118)\n\n# Applying Xnames to x (to make output easier to read)\nnames(x) <- Xnames \n\n# Notes: \n# The `names` function returns or sets the names of vector elements.\n# We can set names and values in a single line like so:\nx <- c(X1 = 110, X2 = 120, X3 = 118)\n\nThe sum of these three scores can be calculated in a variety of ways. Here is the easiest:\n\n# x_S = The sum of scores x1, x2, and x3\nx_S <- sum(x)\n\nHowever, if we want to be matrix algebra masochists (and, apparently, at least one of us does!), we could do this:\n\n# A vector of ones the same length as x\nones <- rep(1,length(x))\n\n# Notes: \n# The rep function creates vectors of repeated elements. \n# For example, rep(5,3) is the same as c(5,5,5).\n# \n# The length function returns the number of elements in a vector. \n# For example, length(c(3,3)) returns `2`.\n\n\n# Calculating x_S with matrix algebra\nx_S <- t(x) %*% ones\n\n# Notes: \n# The t function transposes a vector or matrix.\n# The operator %*% multiplies compatible matrices.\n\nEither way that we calculate it, x_S = 348.\n\n\n5.1.2 Calculating the Mean of a Sum\nThe mean of X_S is:\n\n\\mu_{S}=\\sum_{i=1}^{k}{\\mu_i}=\\boldsymbol{\\mu'1}\n\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\mu_S\nThe mean of X_S\n\n\n\\mu_i\nThe mean of X_i\n\n\n\\boldsymbol{\\mu}\nA k \\times 1 vector of means of the variables in \\boldsymbol{X}\n\n\n\n\n\nSuppose that the means of X_1, X_2, and X_3 are all 100.\n\n# m = vector of means of X\nm <- c(100,100,100)\n\nAgain, the mean of X_S (\\mu_S) can be calculated in two ways:\n\n# m_S = The mean of S\n# The easy way\nm_S <- sum(m)\n\n# With matrix algebra\nm_S <- t(m) %*% ones\n\nRunning this code, we can see that \\mu_S = 300.\n\n\n5.1.3 Calculating the Mean of a Weighted Sum\nThe mean of a weighted sum is the weighted sum of the means. That is, if\n\\begin{equation}\nx_S=\\sum_{i=1}^{k}{x_i w_i}=\\boldsymbol{x'w}\n\\end{equation}\nWhere\nthen\n\\begin{equation}\n\\mu_S=\\sum_{i=1}^{k}{\\mu_i w_i}=\\boldsymbol{\\mu'w}\n\\end{equation}\n\n\nNote that the calculation of X_S and \\mu_S with matrix algebra is the same as it was with an equally weighted sum except that instead of post-multiplying by a vector of ones (\\boldsymbol{x^\\prime 1}), we post-multiply by a vector of weights (\\boldsymbol{x^\\prime w}). In truth, an equally weighted sum is a special case of a weighted sum in which \\boldsymbol{w} consists entirely of ones.\nSuppose that \\boldsymbol{w} = \\{0.5,1,2\\}. That is, the weight for X_1 is 0.5, the weight for X_2 is 1, and the weight for X_3 is 2. We will continue to use the same values for \\boldsymbol{x} and \\boldsymbol{\\mu} as before:\n\n# w = The vector of weight for variables X1, X2, and X3\nw = c(0.5, 1, 2)\n\n# The easy way\nx_S_weighted <- sum(x * w)\nm_S_weighted <- sum(m * w)\n\n# Notes:\n# The multiplication operator * multiplies analogous elements \n# of vectors  and matrices. In the example, `x * w` is \n# c(110 * 0.5, 120 * 1, 118 * 2)\n\n# With matrix algebra\nx_S_weighted <- t(x) %*% w\nm_S_weighted <- t(m) %*% w\n\nRunning the code shows that x_S = 411 and that \\mu_S = 350."
  },
  {
    "objectID": "compositescores.html#the-variance-of-the-sum-is-the-sum-of-the-covariance-matrix",
    "href": "compositescores.html#the-variance-of-the-sum-is-the-sum-of-the-covariance-matrix",
    "title": "5  Composite Scores",
    "section": "5.2 The Variance of the Sum Is the Sum of the Covariance Matrix",
    "text": "5.2 The Variance of the Sum Is the Sum of the Covariance Matrix\n\n\nUnfortunately, the notation for a covariance matrix is a bold capital sigma \\boldsymbol{\\Sigma}, which is easily confused with the summation symbol, which is generally larger and not bold: \\sum.\nIf variables are uncorrelated, the variance of their sum is the sum of their variances. However, this is not true when variables are substantially correlated. The formula for the variance of a sum looks more complex than it is. It is just the sum of the covariance matrix.\n\n\\sigma_{X_S}^2=\\sum_{i=1}^{k}{\\sum_{j=1}^{k}{\\sigma_{ij}}}=\\boldsymbol{1'\\Sigma 1}\n(\\#eq:VarianceOfASum)\n\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\sigma_{X_S}^2\nThe variance of X_S\n\n\n\\sigma_{ij}\nThe covariance between X_i and X_j (\\sigma_{ij}=\\sigma_i^2 if i=j)\n\n\n\\boldsymbol{\\Sigma}\nThe k \\times k covariance matrix of all the variables in \\boldsymbol{X}\n\n\n\n\n\n\n\nThe symbol for a sample correlation is the Roman lowercase r, and a matrix of such correlations is an uppercase \\boldsymbol{R}. Therefore, the population correlation coefficient is a Greek lowercase rho: \\rho. This, unfortunately means that a matrix of correlations should be an uppercase rho: \\boldsymbol{P}. Somehow, statisticians are okay with \\rho looking a lot like an italicized Roman letter p. However, using an uppercase rho (\\boldsymbol{P}) for a correlation matrix is too weird even for statisticians! You know, hobgoblins of little minds and all…\nSuppose that the standard deviations of X_1, X_2, and X_3 are all 15. Thus, \\sigma=\\{15,15,15\\}. The correlations among the three variables are shown in matrix \\boldsymbol{R}:\n\n\\boldsymbol{R} = \\begin{bmatrix}\n1 & 0.5 & 0.6\\\\\n0.5 & 1 & 0.7\\\\\n0.6 & 0.7 & 1\n\\end{bmatrix}\n\nTo create this matrix in R:\n\n# R = correlation matrix of variables in X\nR <- matrix(c(1, 0.5, 0.6, \n              0.5, 1, 0.7,  \n              0.6, 0.7, 1),\n    nrow = 3,\n    ncol = 3,\n    byrow = TRUE)\n\nR\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.5  0.6\n[2,]  0.5  1.0  0.7\n[3,]  0.6  0.7  1.0\n\n\nThe covariance matrix \\boldsymbol{\\Sigma} can be computed from the correlation matrix and the standard deviations like so:\n\n\\sigma_{ij} = \\sigma_{i} \\sigma_{j} \\rho_{ij}\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\sigma_{ij}\nThe covariance between X_i and X_j\n\n\n\\rho_{ij}\nThe correlation between X_i and X_j\n\n\n\\sigma_{i}\nThe standard deviation of variable X_i\n\n\n\\sigma_{j}\nThe standard deviation of variable X_j\n\n\n\n\n\nUnfortunately, computing a covariance matrix like this in a computer program is inelegant because we have to make use of looping:\n\n# R = Correlation matrix of variables in X\nR <- matrix(c(1,0.5,0.6,\n              0.5,1,0.7,\n              0.6,0.7,1), nrow = 3)\nrownames(R) <- colnames(R) <- Xnames #Apply names\n\n# s = The standard deviations of variables in X\ns <- c(15,15,15)\n\n# k = The number of variables\nk <- length(s)\n\n# CM = Covariance Matrix\n# Initialize k by k matrix of zeroes\nCM <- matrix(0, k, k)\nrownames(CM) <- colnames(CM) <- Xnames\nfor (i in seq(1, k)) {\n  for (j in seq(1, k)) {\n    CM[i, j] = s[i] * s[j] * R[i, j]\n  }\n}\n\nCM\n\n      X1    X2    X3\nX1 225.0 112.5 135.0\nX2 112.5 225.0 157.5\nX3 135.0 157.5 225.0\n\n\n\n\nThe \\mathtt{diag} function has three purposes. First, it can extract the diagonal vector from a matrix: \nA_{kk} =\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  a_{k1} & a_{k2} & \\cdots & a_{kk}\n\\end{bmatrix}\n Then \\mathtt{diag}(\\boldsymbol{A}) = \\{a_{11},a_{22},...,a_{kk}\\} Second, the diag function inserts a vector into the the diagonal of a k \\times k matrix of zeros: \n\\mathtt{diag}(\\boldsymbol{a}) =\n\\begin{bmatrix}\n  a_{1} & 0 & \\cdots & 0 \\\\\n  0 & a_{2} & \\cdots & 0 \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  0 & 0 & \\cdots & a_{k}\n\\end{bmatrix}\n Third, the diag function converts integer k into a k \\times k identity matrix: \n\\mathtt{diag}(k) =\n\\begin{bmatrix}\n  1 & 0 & \\cdots & 0 \\\\\n  0 & 1 & \\cdots & 0 \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\nTo calculate the covariance matrix in matrix notation, we can make use of the \\mathtt{diag} function.\n\n\\begin{align*}\n\\boldsymbol{\\Sigma}&=\\mathtt{diag}(\\boldsymbol{\\sigma}) \\boldsymbol{R}\\, \\mathtt{diag}(\\boldsymbol{\\sigma})\\\\\n&=\\begin{bmatrix}\n  \\sigma_{1} & 0 & \\cdots & 0 \\\\\n  0 & \\sigma_{2} & \\cdots & 0 \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  0 & 0 & \\cdots & \\sigma_{k}\n\\end{bmatrix}\n  \\begin{bmatrix}\n  1 & \\rho_{12} & \\cdots & \\rho_{1k} \\\\\n  \\rho_{21} & 1 & \\cdots & \\rho_{2k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\rho_{k1} & \\rho_{k2} & \\cdots & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\sigma_{1} & 0 & \\cdots & 0 \\\\\n  0 & \\sigma_{2} & \\cdots & 0 \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  0 & 0 & \\cdots & \\sigma_{k}\n\\end{bmatrix}\\\\\n  &=  \\begin{bmatrix}\n  \\sigma_{1}^2 & \\sigma_{1}\\sigma_{2}\\rho_{12} & \\cdots & \\sigma_{1}\\sigma_{k}\\rho_{1k} \\\\\n  \\sigma_{2}\\sigma_{1}\\rho_{21} & \\sigma_{2}^2 & \\cdots & \\sigma_{2}\\sigma_{k}\\rho_{2k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k}\\sigma_{1}\\rho_{k1} & \\sigma_{k}\\sigma_{2}\\rho_{k2} & \\cdots & \\sigma_{k}^2\n\\end{bmatrix}\\\\\n   &=  \\begin{bmatrix}\n  \\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & \\sigma_{2}^2 & \\cdots & \\sigma_{2k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & \\sigma_{k}^2\n\\end{bmatrix}\n\\end{align*}\n\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\boldsymbol{\\Sigma}\nThe covariance matrix of variables in \\boldsymbol{X}\n\n\n\\boldsymbol{\\sigma}\nThe standard deviations of variables in \\boldsymbol{X}\n\n\n\\boldsymbol{R}\nThe correlation matrix of variables in \\boldsymbol{X}\n\n\n\n\n\nSo this is where matrix algebra starts to shine. We do not need to initialize the variable that contains the covariance matrix, or calculate k, or do any looping.\n\n# CM = Covariance matrix of variables in X\nCM <- diag(s) %*% R %*% diag(s)\n\nBeautiful!\nRunning the code, we get: \n\\boldsymbol{\\Sigma}=\\begin{bmatrix}\n225 & 112.5 & 135\\\\\n112.5 & 225 & 157.5\\\\\n135 & 157.5 & 225\n\\end{bmatrix}\n\nTo calculate the variance of X_S if it is an unweighted sum we apply Equation @ref(eq:VarianceOfASum):\n\n\n\nHere we see that \\sigma_X^2=1485"
  },
  {
    "objectID": "compositescores.html#calculating-the-variance-of-a-weighted-sum",
    "href": "compositescores.html#calculating-the-variance-of-a-weighted-sum",
    "title": "5  Composite Scores",
    "section": "5.3 Calculating the Variance of a Weighted Sum",
    "text": "5.3 Calculating the Variance of a Weighted Sum\nIf X_S is a weighted sum, its variance is the weighted sum of the covariance matrix.\n\n\\sigma_S^2=\\sum_{i=1}^{k}{\\sum_{j=1}^{k}{w_i w_j \\sigma_{ij}}}=\\boldsymbol{w'\\Sigma w}\n(\\#eq:VarianceOfAWeightedSum)\n\nContinuing with the same variables in our example, we see that things are starting to become clumsy and ugly without matrix algebra:\n\n# First we initialize var_S as 0\nvar_S_weighted <- 0\n# Now we loop through k rows and k columns of CM\nfor (i in seq(1, k)) {\n  for (j in seq(1, k)) {\n    var_S_weighted <- var_S_weighted + w[i] * w[j] * CM[i, j]\n  }\n}\n\nWith matrix algebra, this all happens with a single line of code:\n\nvar_S_weighted <- t(w) %*% CM %*% w\n\nIf we don’t need to know the covariance matrix, we can skip its calculation:\n\nvar_S_weighted <- t(w * s) %*% R %*% (w * s)\n\nAll three methods give the same answer: \\sigma_X^2 = 2193.75"
  },
  {
    "objectID": "compositescores.html#calculating-a-composite-score",
    "href": "compositescores.html#calculating-a-composite-score",
    "title": "5  Composite Scores",
    "section": "5.4 Calculating a Composite Score",
    "text": "5.4 Calculating a Composite Score\nWe now have all of the information needed to make a composite score. First we will make an unweighted composite.\n\\begin{equation}\nC = \\frac{x_S-\\mu_S}{\\sigma_{X_S}}\\sigma_C + \\mu_C\n(\\#eq:Composite)\n\\end{equation}\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\nC\nThe composite variable C\n\n\nx_S\nThe sum of all k scores in \\{x_1,...,x_k\\}\n\n\n\\sigma_{X_S}\nThe standard deviation of X_S\n\n\n\\sigma_C\nThe standard deviation of C\n\n\n\\mu_C\nThe mean of C\n\n\n\n\n\nContinuing with our example, suppose that \\mu_C=100 and \\sigma_C=15.\n\n# m_C = Composite mean\nm_C <- 100\n# s_C = Composite standard deviation\ns_C <- 15\n# C = The composite score\nC <- ((x_S - m_S) / sqrt(var_S)) * s_C + m_C\n\n\nC=119\n\n\n5.4.1 Calculating a Weighted Composite Score\nIn matrix notation, an unweighted composite score is calculated like so:\n\nC=\\boldsymbol{\\frac{1'(x-\\mu_x)}{\\sqrt{1'\\Sigma 1}}}\\sigma_C+\\mu_C\n Replacing each vector of ones \\boldsymbol{1} with a weight vector \\boldsymbol{w} gives us the formula for computing a weighted composite score:\n\nC_w=\\boldsymbol{\\frac{w'(x-\\mu_x)}{\\sqrt{w'\\Sigma w}}}\\sigma_C+\\mu_C\n\n\n\nIn clinical practice, the most common kind of composite score is an equally weighted composite consisting of scores with the same mean and standard deviation. Sometimes scores are weighted by the degree to which they correlate with the construct the composite is intended to measure. Other weighting schemes are also possible. In most cases, it makes sense to first convert all scores to z-scores and then weight the z-scores. Failing to convert the scores to a common metric such as z-scores will result in an implicit weighting by standard deviations. That is, the score with the largest standard deviation will have the most weight in the composite score. Converting to z-scores first will equalize the each score’s influence on the composite score. We can convert all the scores in \\boldsymbol{x} to z-scores like so:\n\n\\boldsymbol{z} = \\frac{\\boldsymbol{x}-\\boldsymbol{\\mu}}{\\boldsymbol{\\sigma}}\n\nThe nice thing about z-scores is that their means are zeros, their standard deviations are ones, and their covariances are correlations. Thus, the formula for a weighted composite score consisting of z-scores is fairly simple, especially if the composite score is a z-score itself:\n\nC = \\frac{\\boldsymbol{w^\\prime z}}{\\sqrt{\\boldsymbol{w^\\prime Rw}}}\\sigma_C+\\mu_C\n\nContinuing with our example, computing a composite score with X_1=110, X_2=120, and X_3=118:\n\n# C = The composite score\nC <- (t(w) %*% (x - m) / sqrt(t(w) %*% CM %*% w)) * 15  + 100\n\n# With index scores, we round to the nearest integer\nC <- round(C)\n\n# Notes:\n# The round function rounds to the nearest integer by default,\n# but you can round to any number of digits you wish. For example, \n# rounding to 2 significant digits (i.e., the hundredths place), \n# would be round(x,2).\n\nHere we see, that the weighted composite score C = 120. If we had wanted an equally weighted composite score, the w vector would be set to equal ones. If we had done so, C would have been 119 instead of 120."
  },
  {
    "objectID": "compositescores.html#the-reliability-coefficient-of-a-composite",
    "href": "compositescores.html#the-reliability-coefficient-of-a-composite",
    "title": "5  Composite Scores",
    "section": "5.5 The Reliability Coefficient of a Composite",
    "text": "5.5 The Reliability Coefficient of a Composite\nCalculating the reliability coefficient of a composite score is much easier than it might appear at first. Remember that reliability is the ratio of a score’s true score variance to its total variance:\n\nr_{XX} = \\frac{\\sigma_T^2}{\\sigma_X^2}\n\nThe variance of a composite score is the sum of the covariance matrix of the composite’s component scores.\n\n\\sigma_X^2=\\boldsymbol{1}'\\begin{bmatrix}\n  \\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & \\sigma_{2}^2 & \\cdots & \\sigma_{2 k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & \\sigma_{k}^2\n\\end{bmatrix}\\boldsymbol{1}\n\nThe covariance matrix of the component true scores is exactly the same as the covariance matrix of the component observed scores, except that the variances on the diagonal are multiplied by the reliability coefficients of each of the component scores like so:\n\n\\sigma_T^2=\\boldsymbol{1}'\\begin{bmatrix}\n  {\\color{firebrick}r_{11}}\\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & {\\color{firebrick}r_{22}}\\sigma_{2}^2 & \\cdots & \\sigma_{2 k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & {\\color{firebrick}r_{kk}}\\sigma_{k}^2\n\\end{bmatrix}\\boldsymbol{1}\n\n\n# r_XX = Reliability coefficients for X1, X2, and X3\nr_XX <- c(0.88,0.80,0.76)\n\n# CM_T = Covariance matrix of true scores\nCM_T <- CM\n\n# Replace diagonal with true score variances\ndiag(CM_T) <- diag(CM) * r_XX\n\n# r_C = Reliability coefficient of composite score\nr_C <- sum(CM_T) / sum(CM)\n\nThe reliability coefficient of the unweighted composite is 0.92.\nFor a weighted composite, the reliability is calculated by relplacing each vector of ones \\boldsymbol{1} with a weight vector \\boldsymbol{w}:\n\n\\rho_{XX}=\\frac{\\sigma_T^2}\n               {\\sigma_X^2}\n         =\\frac{\\boldsymbol{w}^{\\prime}\n               \\begin{bmatrix}\n  {\\color{firebrick}r_{11}}\\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & {\\color{firebrick}r_{22}}\\sigma_{2}^2 & \\cdots & \\sigma_{2 k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & {\\color{firebrick}r_{kk}}\\sigma_{k}^2\n                \\end{bmatrix}\n                \\boldsymbol{w}}\n          {\\boldsymbol{w}^{\\prime}\n               \\begin{bmatrix}\n  \\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & \\sigma_{2}^2 & \\cdots & \\sigma_{2 k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & \\sigma_{k}^2\n\\end{bmatrix}\\boldsymbol{w}}\n\n\n# Reliability coefficient of the weighted composite\nr_C_w <- t(w) %*% CM_T %*% w / t(w) %*% CM %*% w\n\nThe reliability coefficient of the weighted composite is 0.88."
  },
  {
    "objectID": "compositescores.html#composite-scores-and-their-correlations",
    "href": "compositescores.html#composite-scores-and-their-correlations",
    "title": "5  Composite Scores",
    "section": "5.6 Composite Scores and Their Correlations",
    "text": "5.6 Composite Scores and Their Correlations\nIf the population correlations among all of the components are known, it is possible to calculate the correlations among composite scores made from these components. Such correlations can be used in many practical applications, including in prediction models and in the evaluation of difference scores.\nSuppose that Composite A is calculated from the sum of two component tests, A_1 and A_2. Composite B is calculated from the sum of B_1 and B_2. Suppose that the correlation matrix for the four components is:\n\n\n\n\n\\boldsymbol{R} = \\begin{array}{r|cccc}\n& \\color{royalblue}{A_1} & \\color{royalblue}{A_2} & \\color{firebrick}{B_1} & \\color{firebrick}{B_2}\\\\\n\\hline \\color{royalblue}{A_1} & \\color{royalblue}{1} & \\color{royalblue}{.30} & \\color{purple}{.35} & \\color{purple}{.40}\\\\\n\\color{royalblue}{A_2} & \\color{royalblue}{.30} & \\color{royalblue}{1} & \\color{purple}{.42} & \\color{purple}{.48}\\\\\n\\color{firebrick}{B_1} & \\color{purple}{.35} & \\color{purple}{.42} & \\color{firebrick}{1} & \\color{firebrick}{.56}\\\\\n\\color{firebrick}{B_2} & \\color{purple}{.40} & \\color{purple}{.48} & \\color{firebrick}{.56} & \\color{firebrick}{1}\n\\end{array}\n\n\n# Make correlation matrix R\nR <- matrix(c(1, .30, .35, .40,\n              .30, 1, .42, .48,\n              .35, .42, 1, .56,\n              .40, .48, .56, 1),\n            nrow = 4,\n            ncol = 4)\n\n# Display R\nR\n\n     [,1] [,2] [,3] [,4]\n[1,] 1.00 0.30 0.35 0.40\n[2,] 0.30 1.00 0.42 0.48\n[3,] 0.35 0.42 1.00 0.56\n[4,] 0.40 0.48 0.56 1.00\n\n\nThe correlation between Composite A and Composite B is calculating by adding up the numbers is all three shaded regions of the correlation matrix and then dividing the sum of “between” correlations in purple by the geometric mean of the sums from the “within” correlations in the blue and red regions in the correlation matrix \\boldsymbol{R} like so:\nr_{AB}=\\dfrac{\\color{purple}{\\text{Sum of Correlations between A and B}}}{\\sqrt{\\color{royalblue}{\\text{Sum of Correlations Within A}}\\times\\color{firebrick}{{\\text{Sum of Correlations Within B}}}}}\nTo calculate these sums using matrix algebra, we first construct a “weight matrix” \\boldsymbol{W} that tells us which tests are in which composite. The 1s in the first two rows of column 1 tell us that tests A_1 and A_2 belong to composite A. Likewise, the 1s in the last two rows of column 2 tell us that B_1 and B_2 belong to composite B.\n\n\\boldsymbol{W}=\n\\begin{array}{r|cc}\n& \\color{royalblue}{A} & \\color{firebrick}{B}\\\\\n\\hline \\color{royalblue}{A_1} & \\color{royalblue}{1} & 0\\\\\n\\color{royalblue}{A_2} & \\color{royalblue}{1} & 0\\\\\n\\color{firebrick}{B_1} & 0 & \\color{firebrick}{1}\\\\\n\\color{firebrick}{B_2} & 0 & \\color{firebrick}{1}\n\\end{array}\n\nHere is one way to make the weight matrix in R:\n\n# Make a 4 by 2 matrix of 0s\nW <- matrix(0, nrow = 4, ncol = 2)\n\n# Assign 1s to rows 1 and 2 to column 1\nW[1:2,1] <- 1\n# Assign 1s to rows 3 and 4 to column 2\nW[3:4,2] <- 1\n\n# Display W\nW\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    1    0\n[3,]    0    1\n[4,]    0    1\n\n\n\n\n\nThe covariance matrix \\boldsymbol{\\Sigma_{AB}} is calculated by pre- and post-mulitplying \\boldsymbol{R} by the wieght matrix .\n\n\\begin{aligned}\\boldsymbol{\\Sigma_{AB}} &= \\boldsymbol{W'RW}\\\\[2ex]\n&=\\left[\\begin{array}{cc}\n\\color{royalblue}{1} & 0\\\\\n\\color{royalblue}{1} & 0\\\\\n0 & \\color{firebrick}{1}\\\\\n0 & \\color{firebrick}{1}\n\\end{array}\\right]'\\left[\\begin{array}{cccc}\n\\color{royalblue}{1} & \\color{royalblue}{.30} & \\color{purple}{.35} & \\color{purple}{.40}\\\\\n\\color{royalblue}{.30} & \\color{royalblue}{1} & \\color{purple}{.42} & \\color{purple}{.48}\\\\\n\\color{purple}{.35} & \\color{purple}{.42} & \\color{firebrick}{1} & \\color{firebrick}{.56}\\\\\n\\color{purple}{.40} & \\color{purple}{.48} & \\color{firebrick}{.56} & \\color{firebrick}{1}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\color{royalblue}{1} & 0\\\\\n\\color{royalblue}{1} & 0\\\\\n0 & \\color{firebrick}{1}\\\\\n0 & \\color{firebrick}{1}\n\\end{array}\\right]\\\\[2ex]\n&=\\begin{array}{r|cc}\n& \\color{royalblue}{A} & \\color{firebrick}{B}\\\\\n\\hline \\color{royalblue}{A} & \\color{royalblue}{1 + .30 + .30 + 1} & \\color{purple}{.35 + .42 + .40 + .48}\\\\\n\\color{firebrick}{B} & \\color{purple}{.35 + .40 + .42 + .48} & \\color{firebrick}{1 + .56 + .56 + 1}\n\\end{array}\\\\[2ex]\n&=\\begin{array}{r|cc}\n& \\color{royalblue}{A} & \\color{firebrick}{B}\\\\\n\\hline \\color{royalblue}{A} & \\color{royalblue}{2.60} & \\color{purple}{1.65}\\\\\n\\color{firebrick}{B} & \\color{purple}{1.65} & \\color{firebrick}{3.12}\n\\end{array}\n\\end{aligned}\n\n\n# Covariance matrix of composites A and B\nSigma_AB <- t(W) %*% R %*% W\n\n# Display Sigma_AB\nSigma_AB\n\n     [,1] [,2]\n[1,] 2.60 1.65\n[2,] 1.65 3.12\n\n\nNow we need to extract the variance diagonal from the covariance matrix so that we can use them to convert the covariance matrix to a correlation matrix. The variances are put on a diagonal matrix, and then taking the square root converts the variances to standard deviations in matrix \\boldsymbol{\\sigma}.\n\n\\begin{aligned}\n\\boldsymbol{\\sigma}\n&=\\mathtt{diag}(\\mathtt{diag}(\\boldsymbol{\\Sigma_{AB}}))^{\\frac{1}{2}}\\\\\n&=\\mathtt{diag}\\left(\n\\mathtt{diag}\\left({\n\\left[\\begin{array}{cc}\n\\color{royalblue}{2.60} & \\color{purple}{1.65}\\\\\n\\color{purple}{1.65} & \\color{firebrick}{3.12}\n\\end{array}\\right]\n   }\\right)\n   \\right)^{\\frac{1}{2}}\\\\\n   &=\\mathtt{diag}\\left(\n   \\left[\\begin{array}{c}\n\\color{royalblue}{2.60}\\\\\n\\color{firebrick}{3.12}\n\\end{array}\\right]\n   \\right)^{\\frac{1}{2}}\\\\\n   &=\\left[\\begin{array}{cc}\n\\color{royalblue}{2.60} & 0\\\\\n0 & \\color{firebrick}{3.12}\n\\end{array}\\right]^{\\frac{1}{2}}\\\\\n   &=\\left[\\begin{array}{cc}\n\\color{royalblue}{1.6125} & 0\\\\\n0 & \\color{firebrick}{1.7664}\n\\end{array}\\right]\n\\end{aligned}\n\n\n# Standard deviations\nsigma <- diag(diag(Sigma_AB)^(0.5))\n\n# Display sigma\nsigma\n\n         [,1]     [,2]\n[1,] 1.612452 0.000000\n[2,] 0.000000 1.766352\n\n\nPre- and post-multiplying the covariance matrix \\boldsymbol{\\Sigma_{AB}} by the inverted standard devation matrix \\boldsymbol{\\sigma} to yield the correlations between composites A and B.\n\n\\begin{aligned}\n\\boldsymbol{R_{AB}}&=\\boldsymbol{\\sigma}^{-1}\\boldsymbol{\\Sigma_{AB}}\\boldsymbol{\\sigma}^{-1}\\\\[2ex]\n&={\\boldsymbol{\\sigma}^{-1} \\atop \\left[\\begin{array}{cc}\n\\color{royalblue}{1.6125} & 0\\\\\n0 & \\color{firebrick}{1.7664}\n\\end{array}\\right]^{-1}}\n   {\\boldsymbol{\\Sigma_{AB}} \\atop \\left[\\begin{array}{cc}\n\\color{royalblue}{2.60} & \\color{purple}{1.65}\\\\\n\\color{purple}{1.65} & \\color{firebrick}{3.12}\n\\end{array}\\right]}\n   {\\boldsymbol{\\sigma}^{-1} \\atop \\left[\\begin{array}{cc}\n\\color{royalblue}{1.6125} & 0\\\\\n0 & \\color{firebrick}{1.7664}\n\\end{array}\\right]^{-1}}\\\\[1ex]\n&=\\left[\\begin{array}{cc}\n\\color{royalblue}{.6202} & 0\\\\\n0 & \\color{firebrick}{.5661}\n\\end{array}\\right]\n   \\left[\\begin{array}{cc}\n\\color{royalblue}{2.60} & \\color{purple}{1.65}\\\\\n\\color{purple}{1.65} & \\color{firebrick}{3.12}\n\\end{array}\\right]\n   \\left[\\begin{array}{cc}\n\\color{royalblue}{.6202} & 0\\\\\n0 & \\color{firebrick}{.5661}\n\\end{array}\\right]\\\\[1ex]\n&=\\left[\\begin{array}{cc}\n\\color{royalblue}{1} & \\color{purple}{.58}\\\\\n\\color{purple}{.58} & \\color{firebrick}{1}\n\\end{array}\\right]\n\\end{aligned}\n\n\n\nThe solve function inverts a square matrix. Inverting a single number x gives its reciprocal. If you multiply the inverse of a number by the nunbe itself, you get 1, the identity for the multiplication operator. x^{-1}x=1, multiplying a matrix by its inverse creates an identity matrix (a matrix with ones on the diagonal and zeroes elsewhere):\n\\boldsymbol{A^{-1}A = I}\n\n# Correlations between composites A and B\nR_AB <- solve(sigma) %*% Sigma_AB %*% solve(sigma)\nR_AB\n\n          [,1]      [,2]\n[1,] 1.0000000 0.5793219\n[2,] 0.5793219 1.0000000\n\n\nThese calculations in R can be greatly simplified using the cov2cor function, which converts covariances to correlations:\n\n# Correlation matrix of composites A and B\nR_AB <- cov2cor(t(W) %*% R %*% W)\n\nR_AB\n\n          [,1]      [,2]\n[1,] 1.0000000 0.5793219\n[2,] 0.5793219 1.0000000"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Azzalini, A. (2022). Sn: The skew-normal and related distributions\nsuch as the skew-t and the SUN. http://azzalini.stat.unipd.it/SN/\n\n\nChang, W. (2023). Extrafont: Tools for using fonts. https://github.com/wch/extrafont\n\n\nCicchetti, D., Bronen, R., Spencer, S., Haut, S., Berg, A., Oliver, P.,\n& Tyrer, P. (2006). Rating scales, scales of measurement, issues of\nreliability: Resolving some critical issues for clinicians\nand researchers. The Journal of Nervous and Mental Disease,\n194(8), 557–564.\n\n\nClark, A. (2004). Natural-born cyborgs: Minds,\ntechnologies, and the future of human intelligence. Oxford\nUniversity Press, USA.\n\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003).\nApplied multiple regression/correlation analysis for the behavioral\nsciences. L. Erlbaum Associates.\n\n\nCrocker, L., & Algina, J. (2006). Introduction to classical and\nmodern test theory. Cengage Learning.\n\n\nEaton, M. L. (2007). Multivariate statistics: A vector space\napproach. Inst. of Mathematical Statistics.\n\n\nEmbretson, S. E., & Reise, S. P. (2000). Item response theory\nfor psychologists. Lawrence Erlbaum.\n\n\nFurr, R. (2017). Psychometrics: An introduction\n(3rd ed.). Sage.\n\n\nGarcia, D. M., Schmitt, M. T., Branscombe, N. R., & Ellemers, N.\n(2010). Women’s reactions to ingroup members who protest discriminatory\ntreatment: The importance of beliefs about inequality and\nresponse appropriateness. European Journal of Social\nPsychology, 40(5), 733–745.\n\n\nGawande, A. (2007). Better: A surgeon’s notes on\nperformance. Metropolitan Books.\n\n\nGombin, J. (2018). Qualvar: Implements indices of qualitative\nvariation proposed by wilcox (1973). http://joelgombin.github.io/qualvar/\n\n\nJensen, A. R. (2006). Clocking the mind: Mental\nchronometry and individual differences. Elsevier Science\nLimited.\n\n\nLey, P. (1972). Quantitative aspects of psychological assessment:\nAn introduction. Duckworth.\n\n\nLocher, R. (2020). IDPmisc: Utilities of institute of data analyses\nand process design (www.zhaw.ch/idp). https://CRAN.R-project.org/package=IDPmisc\n\n\nLüdecke, D. (2021). Sjmisc: Data and variable transformation\nfunctions. https://strengejacke.github.io/sjmisc/\n\n\nMichell, J. (1997). Quantitative science and the definition of\nmeasurement in psychology. British Journal of Psychology,\n88(3), 355–383. https://doi.org/10.1111/j.2044-8295.1997.tb02641.x\n\n\nMichell, J. (2004). Item response models, Pathological\nscience and the shape of error: Reply to\nBorsboom and Mellenbergh. Theory &\nPsychology, 14(1), 121–129.\n\n\nMichell, J. (2008). Is psychometrics pathological science?\nMeasurement, 6(1-2), 7–24.\n\n\nMüller, K., & Wickham, H. (2022). Tibble: Simple data\nframes. https://CRAN.R-project.org/package=tibble\n\n\nNunnally, J. C. (1967). Psychometric theory.\nMcGraw-Hill.\n\n\nPearson, K., & Heron, D. (1913). On Theories of\nAssociation. Biometrika, 9(1/2), 159–315.\nhttps://doi.org/10.2307/2331805\n\n\nPedersen, T. L. (2022). Ggforce: Accelerating ggplot2. https://CRAN.R-project.org/package=ggforce\n\n\nPedersen, T. L., & Robinson, D. (2022). Gganimate: A grammar of\nanimated graphics. https://CRAN.R-project.org/package=gganimate\n\n\nPoncet, P. (2019). Modeest: Mode estimation. https://github.com/paulponcet/modeest\n\n\nR Core Team. (2022). R: A language and environment for statistical\ncomputing. R Foundation for Statistical Computing. https://www.R-project.org/\n\n\nRaykov, T., & Marcoulides, G. A. (2011). Introduction to\npsychometric theory. Routledge.\n\n\nSchneider, W. J. (2023). Psycheval: A psychological evaluation\ntoolkit. https://github.com/wjschne/psycheval\n\n\nSharpsteen, C., & Bracken, C. (2020). tikzDevice: R graphics\noutput in LaTeX format. https://github.com/daqana/tikzDevice\n\n\nStevens, S. S. (1946). On the theory of scales of measurement.\nScience, 103(2684), 677–680. https://doi.org/10.1126/science.103.2684.677\n\n\nStrang, G. (2016). Introduction to linear algebra (5th\nedition). Cambridge press.\n\n\nWickham, H. (2022a). Stringr: Simple, consistent wrappers for common\nstring operations. https://CRAN.R-project.org/package=stringr\n\n\nWickham, H. (2022b). Tidyverse: Easily install and load the\ntidyverse. https://CRAN.R-project.org/package=tidyverse\n\n\nWickham, H. (2023). Forcats: Tools for working with categorical\nvariables (factors). https://CRAN.R-project.org/package=forcats\n\n\nWickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K.,\nWilke, C., Woo, K., Yutani, H., & Dunnington, D. (2022).\nggplot2: Create elegant data visualisations using the grammar of\ngraphics. https://CRAN.R-project.org/package=ggplot2\n\n\nWickham, H., François, R., Henry, L., Müller, K., & Vaughan, D.\n(2023). Dplyr: A grammar of data manipulation. https://CRAN.R-project.org/package=dplyr\n\n\nWickham, H., & Henry, L. (2023). Purrr: Functional programming\ntools. https://CRAN.R-project.org/package=purrr\n\n\nWickham, H., Vaughan, D., & Girlich, M. (2023). Tidyr: Tidy\nmessy data. https://CRAN.R-project.org/package=tidyr\n\n\nWilcox, A. R. (1973). Indices of qualitative variation and political\nmeasurement. The Western Political Quarterly, 26(2),\n325. https://doi.org/10.2307/446831\n\n\nWuertz, D., Setz, T., Theussl, S., & Chalabi, Y. (2022).\nfMultivar: Rmetrics - modeling of multivariate financial return\ndistributions. https://www.rmetrics.org\n\n\nXie, Y. (2023). Knitr: A general-purpose package for dynamic report\ngeneration in r. https://yihui.org/knitr/\n\n\nXie, Y., & Allaire, J. (2022). Tufte: Tufte’s styles for r\nmarkdown documents. https://github.com/rstudio/tufte\n\n\nZacharias, J. R. (1975). The trouble with IQ tests.\nNational Elementary Principal, 54(4), 23–29."
  }
]