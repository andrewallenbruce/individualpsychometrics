{
  "hash": "5ffac2452995358fb104b4613790109b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Distributions\nsubtitle: Continuous\ntoc-title: Distributions--Continuous\n---\n\n\n\n\n\n\n\n## Probability Density Functions {#sec:pdf}\n\nAlthough there are many more discrete distribution families, we will now consider some continuous distribution families. Most of what we have learned about discrete distributions applies to continuous distributions. However, there is a need of a name change for the probability mass function. In a discrete distribution, we can calculate an actual probability for a particular value in the sample space. In continuous distributions, doing so can be tricky. We can always calculate the probability that a score in a particular interval will occur. However, in continuous distributions, the intervals can become very small, approaching a width of 0. When that happens, the probability associated with that interval also approaches 0. Yet, some parts of the distribution are more probable than others. Therefore, we need a measure of probability that tells us the probability of a value *relative* to other values: the [probability density function]{.defword title=\"The **probability density function** is function that can show relative likelihoods of sample space elements of a continuous random variable.\"} \n\nConsidering the entire sample space of a discrete distribution, all of the associated probabilities from the probability mass function sum to 1. In a probability density function, it is the area under the curve that must sum to 1. That is, there is a 100\\% probability that a value generated by the random variable will be somewhere under the curve. There is nowhere else for it to go!\n\nHowever, unlike probability mass functions, probability density functions do not generate probabilities. Remember, the probability of any value in the sample space of a continuous variable is infinitesimal. We can only compare the probabilities to each other. To see this, compare the discrete uniform distribution and continuous uniform distribution in @fig-pdfIllustration. Both distributions range from 1 to 4. In the discrete distribution, there are 4 points, each with a probability of &frac14;. It is easy to see that these 4 probabilities of &frac14; sum to 1. Because of the scale of the figure, it is not easy to see exactly how high the probability density function is in the continuous distribution. It happens to be &frac13;. Why? First, it does not mean that each value has a &frac13; probability. There are an infinite number of points between 1 and 4 and it would be absurd if each of them had a &frac13; probability. The distance between 1 and 4 is 3. In order for the rectangle to have an area of 1, its height must be &frac13;. What does that &frac13; mean, then? In the case of a single value in the sample space, it does not mean much at all. It is simply a value that we can compare to other values in the sample space. It could be scaled to any value, but for the sake of convenience it is scaled such that the area under the curve is 1. \n\nNote that some probability density functions can produce values greater than 1. If the range of a continuous uniform distribution is less than 1, at least some portions of the curve must be greater than 1 to make the area under the curve equal 1. For example, if the bounds of a continuous distribution are 0 and &frac13;, the average height of the probability density function would need to be 3 so that the total area is equal to 1.\n\n## Continuous Uniform Distributions {#sec:Uniform}\n\n\n\n\n::: {#tbl-continuousuniformfeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-cap='Features of Continuous Discrete Distributions'}\n::: {.cell-output-display}\n\n\n|Feature                          |Symbol                            |\n|:--------------------------------|:---------------------------------|\n|Lower Bound                      |$a  \\in (-\\infty,\\infty)$         |\n|Upper Bound                      |$b \\in (a,\\infty)$                |\n|Sample Space                     |$x \\in \\lbrack a,b\\rbrack$        |\n|Mean                             |$\\mu = \\frac{a+b}{2}$             |\n|Variance                         |$\\sigma^2 = \\frac{(b-a)^2-1}{12}$ |\n|Skewness                         |$\\gamma_1 = 0$                    |\n|Kurtosis                         |$\\gamma_2 = -\\frac{6}{5}$         |\n|Probability Density Function     |$f_X(x;a,b) = \\frac{1}{b-a}$      |\n|Cumulative Distribution Function |$F_X(x;a,b) = \\frac{x-a}{b-a}$    |\n\n\n:::\n:::\n\n\n\n\n\nUnlike the [discrete uniform distribution](#sec:DiscreteUniform), the uniform distribution is [continuous](#sec:DiscreteVsContinuous).^[For the sake of clarity, the uniform distribution is often referred to as the *continuous uniform distribution*.] In both distributions, there is an upper and lower bound and all members of the sample space are equally probable.\n\n### Generating random samples from the continuous uniform distribution\n\nTo generate a sample of $n$ numbers with a continuous uniform distribution between $a$ and $b$, use the `runif` function like so:\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Sample size\nn <- 1000\n# Lower and upper bounds\na <- 10\nb <- 30\n# Sample\nx <- runif(n, min = a, max = b)\n```\n:::\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\"}\n::: {.cell-output-display}\n![Random sample (*n* = 1000) of a continuous uniform distribution between 10 and 30. Points are randomly jittered to show the distribution more clearly.](continuousdistributions_files/figure-html/fig-uniformdist-1.png){#fig-uniformdist fig-align='left' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-uniformdist\n\n```{.r .cell-code}\n# Plot\ntibble(x) %>% \nggplot(aes(x, y = 0.5)) + \n  geom_jitter(size = 0.5, \n              pch = 16,\n              color = myfills[1], \n              height = 0.45) +\n  scale_x_continuous(NULL) +\n  scale_y_continuous(NULL, \n                     breaks = NULL, \n                     limits = c(0,1), expand = expansion()) + \n  theme_minimal(base_family = bfont, base_size = bsize)\n```\n\n\n:::\n:::\n\n\n\n\n\n#### Using the continuous uniform distribution to generate random samples from other distributions\n\nUniform distributions can begin and end at any real number but one member of the uniform distribution family is particularly important---the uniform distribution between 0 and 1. If you need to use Excel instead of a statistical package, you can use this distribution to generate random numbers from many other distributions. \n\nThe cumulative distribution function of any continuous distribution converts into a continuous uniform distribution. A distribution's [quantile function](#sec:Quantile) converts a continuous uniform distribution into that distribution. Most of the time, this process also works for discrete distributions. This process is particularly useful for generating random numbers with an unusual distribution. If the distribution's quantile function is known, a sample with a continuous uniform distribution can easily be generated and converted.\n\nFor example, the `RAND` function in Excel generates random numbers between 0 and 1 with a continuous uniform distribution. The `BINOM.INV` function is the binomial distribution's quantile function. Suppose that $n$ (number of Bernoulli trials) is 5 and $p$ (probability of success on each Bernoulli trial) is 0.6. A randomly generated number from the binomial distribution with $n=5$ and $p=0.6$ is generated like so:\n\n`=BINOM.INV(5,0.6,RAND())`\n\nExcel has quantile functions for many distributions (e.g., `BETA.INV, BINOM.INV, CHISQ.INV, F.INV, GAMMA.INV, LOGNORM.INV, NORM.INV, T.INV`). This method of combining `RAND` and a quantile function works reasonably well in Excel for quick-and-dirty projects, but when high levels of accuracy are needed, random samples should be generated in a dedicated statistical program like R, Python (via the numpy package), Julia, STATA, SAS, or SPSS.\n\n\n\n## Normal Distributions {#sec:normal}\n\n(Unfinished)\n\n\n\n\n\n\n\n::: {#tbl-normalfeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-colwidths='[30,70]' tbl-cap='Features of Normal Distributions'}\n::: {.cell-output-display}\n\n\n|Feature                          |Symbol                                                                                                                                           |\n|:--------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------|\n|Sample Space                     |$x \\in (-\\infty,\\infty)$                                                                                                                         |\n|Mean                             |$\\mu = \\mathcal{E}\\left(X\\right)$                                                                                                                |\n|Variance                         |$\\sigma^2 = \\mathcal{E}\\left(\\left(X - \\mu\\right)^2\\right)$                                                                                      |\n|Skewness                         |$\\gamma_1 = 0$                                                                                                                                   |\n|Kurtosis                         |$\\gamma_2 = 0$                                                                                                                                   |\n|Probability Density Function     |$f_X(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma ^ 2}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$                                    |\n|Cumulative Distribution Function |$F_X(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} {\\displaystyle \\int_{-\\infty}^{x} e ^ {-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx}$ |\n\n\n:::\n:::\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\"}\n::: {.cell-output-display}\n![Carl Friedrich Gauss (1777--1855)<br>[Image Credits](https://en.wikipedia.org/wiki/File:Carl_Friedrich_Gauss_1840_by_Jensen.jpg)](images/Carl_Friedrich_Gauss2.jpg){#fig-GaussImage fig-align='left' width=300}\n:::\n:::\n\n\n\n\nThe normal distribution is sometimes called the *Gaussian* distribution after its discoverer, Carl Friedrich Gauss [@fig-GaussImage]. It is a small injustice that most people do not use Gauss's name to refer to the normal distribution. Thankfully, Gauss is not exactly languishing in obscurity. He made so many discoveries that his name is all over mathematics and statistics.\n\n\nThe normal distribution is probably the most important distribution in statistics and in psychological assessment. In the absence of other information, assuming that an individual difference variable is normally distributed is a good bet. Not a sure bet, of course, but a good bet. Why? What is so special about the normal distribution? \n\nTo get a sense of the answer to this question, consider what happens to the binomial distribution as the number of events ($n$) increases. To make the example more concrete, let's assume that we are tossing coins and counting the number of heads $(p=0.5)$. In @fig-ManyCoins, the first plot shows the probability mass function for the number of heads when there is a single coin $(n=1)$). In the second plot, $n=2$ coins. That is, if we flip 2 coins, there will be 0, 1, or 2 heads. In each subsequent plot, we double the number of coins that we flip simultaneously. Even with as few as 4 coins, the distribution begins to resemble the normal distribution, although the resemblance is very rough. With 128 coins, however, the resemblance is very close.\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![The binomial distribution begins to resemble the normal distribution when the number of events is large.](continuousdistributions_files/figure-html/fig-ManyCoins-1.png){#fig-ManyCoins fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n\nThis resemblance to the normal distribution in the example is not coincidental to the fact that $p=0.5$, making the binomial distribution symmetric. If $p$ is extreme (close to 0 or 1), the binomial distribution is asymmetric. However, if $n$ is large enough, the binomial distribution eventually becomes very close to normal.\n\nMany other distributions, such as the Poisson, Student's T, F, and $\\chi^2$ distributions, have distinctive shapes under some conditions but approximate the normal distribution in others (See @fig-nearlynormal). Why? In the conditions in which non-normal distributions approximate the normal distribution, it is because, like in @fig-ManyCoins, many independent events are summed.\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![Many distributions become nearly normal when their parameters are high.](continuousdistributions_files/figure-html/fig-nearlynormal-1.png){#fig-nearlynormal fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n### Notation for Normal Variates\n\nStatisticians write about variables with normal distributions so often that a compact notation for specifying a normal variable's parameters was useful to develop. If I want to specify that $X$ is a normally  variable with a mean of $\\mu$ and a variance of $\\sigma^2$, I will use this notation: \n\n$$X \\sim \\mathcal{N}(\\mu, \\sigma^2)$$ \n\n\n\n\n::: {#tbl-normal_notation .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-cap='Features of Half-Normal Distributions' tbl-colwidths='[40,60]'}\n::: {.cell-output-display}\n\n\n|Symbol        |Meaning                   |\n|:-------------|:-------------------------|\n|$X$           |A random variable.        |\n|$\\sim$        |Is distributed as         |\n|$\\mathcal{N}$ |Has a normal distribution |\n|$\\mu$         |The population mean       |\n|$\\sigma^2$    |The population variance   |\n\n\n:::\n:::\n\n\n\n\n\nMany authors list the standard deviation $\\sigma$ instead of the variance $\\sigma^2$. When I specify normal distributions with specific means and variances, I will avoid ambiguity by always showing the variance as the standard deviation squared. For example, a normal variate with a mean of 10 and a standard deviation of 3 will be written as $X \\sim \\mathcal{N}(10,3^2)$.\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![Percentiles convert a distribution into a uniform distribution](continuousdistributions_files/figure-html/fig-PercentileContinuous-1.png){#fig-PercentileContinuous fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![Evenly spaced percentile ranks are associated with unevenly spaced scores.](continuousdistributions_files/figure-html/fig-percentileuneven-1.png){#fig-percentileuneven fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![Evenly spaced scores are associated with unevenly spaced percentiles](continuousdistributions_files/figure-html/fig-scoreuneven-1.png){#fig-scoreuneven fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n### Half-Normal Distribution\n\n(Unfinished)\n\n\n\n\n\n\n\n::: {#tbl-halfnormalfeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-cap='Features of Half-Normal Distributions' tbl-colwidths='[40,60]'}\n::: {.cell-output-display}\n\n\n|Feature                          |Symbol                                                                                                                                  |\n|:--------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------|\n|Sample Space                     |$x \\in [\\mu,\\infty)$                                                                                                                    |\n|Mu                               |$\\mu \\in (-\\infty,\\infty)$                                                                                                              |\n|Sigma                            |$\\sigma \\in [0,\\infty)$                                                                                                                 |\n|Mean                             |$\\mu + \\sigma\\sqrt{\\frac{2}{\\pi}}$                                                                                                      |\n|Variance                         |$\\sigma^2\\left(1-\\frac{2}{\\pi}\\right)$                                                                                                  |\n|Skewness                         |$\\sqrt{2}(4-\\pi)(\\pi-2)^{-\\frac{3}{2}}$                                                                                                 |\n|Kurtosis                         |$8(\\pi-3)(\\pi-2)^{-2}$                                                                                                                  |\n|Probability Density Function     |$f_X(x;\\mu,\\sigma) = \\sqrt{\\frac{2}{\\pi \\sigma ^ 2}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$                               |\n|Cumulative Distribution Function |$F_X(x;\\mu,\\sigma) = \\sqrt{\\frac{2}{\\pi\\sigma}} {\\displaystyle \\int_{\\mu}^{x} e ^ {-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx}$ |\n\n\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![The half-normal distribution is the normal distribution with the left half of the distribution stacked on top of the right half of the distribution.](continuousdistributions_files/figure-html/fig-halfnormal-1.png){#fig-halfnormal fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-halfnormal\n\n```{.r .cell-code}\n# Half normal distribution\nxlim <- 4\nn <- length(seq(-xlim, 0, 0.01))\nt1 <- tibble(\n  x = c(0,-xlim,\n        seq(-xlim, 0, 0.01),\n        0,\n        0,\n        seq(0, xlim, 0.01),\n        xlim,\n        0),\n  y = c(0,\n        0,\n        dnorm(seq(-xlim, 0, 0.01)),\n        0,\n        0,\n        dnorm(seq(0, xlim, 0.01)),\n        0,\n        0),\n  side = c(rep(F, n + 3), rep(T, n + 3)),\n  Type = 1\n)\nt2 <- t1 %>%\n  mutate(y = if_else(side, y, 2 * y)) %>%\n  mutate(x = abs(x),\n         Type = 2)\n\nbind_rows(t1, t2) %>%\n  mutate(Type = factor(Type)) %>%\n  ggplot(aes(x, y, fill = side)) +\n  geom_polygon() +\n  geom_text(\n    data = tibble(\n      x = 0,\n      y = dnorm(0) * c(1, 2) + 0.14,\n      Type = factor(c(1,2)),\n      label = c(\n        \"Normal\",\n        \"Half-Normal\"),\n      side = T),\n    aes(label = label),\n    family = bfont, fontface = \"bold\",\n    size = ggtext_size(30), \n    vjust = 1\n  ) +\n  geom_richtext(\n    data = tibble(\n      x = 0,\n      y = dnorm(0) * c(1, 2) + 0,\n      Type = factor(c(1,2)),\n      label = c(\n        paste0(\"*X* ~ \",\n               span_style(\"N\", style = \"font-family:'Lucida Calligraphy'\"),\n               \"(*\",\n               span_style(\"&mu;\", \"font-family:serif;\"),\n               \"*, *\",\n               span_style(\"&sigma;\",\"font-family:serif;\"),\n               \"*<sup>2</sup>)\"),\n        paste0(\"*X* ~ |\",\n               span_style(\"N\", style = \"font-family:'Lucida Calligraphy'\"),\n               \"(0, *\",\n               span_style(\"&sigma;\",\"font-family:serif;\"),\n               \"*<sup>2</sup>)| + *\",\n               span_style(\"&mu;\",\"font-family:serif;\"),\n               \"*\")),\n      side = T),\n    aes(label = label),\n    family = c(\"Equity Text A\"),\n    size = ggtext_size(30), \n    vjust = 0, \n    label.padding = unit(0,\"lines\"), \n    label.color = NA,\n    fill = NA) +\n  theme_void(base_size = 30,\n                base_family = bfont) +\n  theme(\n    legend.position = \"none\",\n    strip.text = element_blank()\n  ) +\n  scale_fill_manual(values = myfills) +\n  facet_grid(rows = vars(Type), space = \"free_y\", scales = \"free_y\") \n```\n\n\n:::\n:::\n\n\n\n\nSuppose that $X$ is a normally distributed variable such that \n\n$$\nX \\sim \\mathcal{N}(\\mu, \\sigma^2)\n$$\n\nVariable $Y$ then has a half-normal distribution such that $Y = |X-\\mu|+\\mu$. In other words, imagine that a normal distribution is folded at the mean with the left half of the distribution now stacked on top of the right half of the distribution (See @fig-halfnormal).\n\n\n\n\n### Truncated Normal Distributions\n\n(Unfinished)\n\n### Multivariate Normal Distributions\n\n(Unfinished)\n\n\n## Chi Square Distributions\n\n(Unfinished)\n\n\n\n\n::: {#tbl-chisquarefeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-cap='Features of Chi-Square Distributions' tbl-colwidths='[50,50]'}\n::: {.cell-output-display}\n\n\n|Feature                          |Symbol                                                                             |\n|:--------------------------------|:----------------------------------------------------------------------------------|\n|Sample Space                     |$x \\in [0,\\infty)$                                                                 |\n|Degrees of freedom               |$\\nu \\in [0,\\infty)$                                                               |\n|Mean                             |$\\nu$                                                                              |\n|Variance                         |$2\\nu$                                                                             |\n|Skewness                         |$\\sqrt{8/\\nu}$                                                                     |\n|Kurtosis                         |$12/\\nu$                                                                           |\n|Probability Density Function     |$f_X(x;\\nu) = \\frac{x^{\\nu/2-1}}{2^{\\nu/2}\\;\\Gamma(\\nu/2)\\,\\sqrt{e^x}}$            |\n|Cumulative Distribution Function |$F_X(x;\\nu) = \\frac{\\gamma\\left(\\frac{\\nu}{2},\\frac{x}{2}\\right)}{\\Gamma(\\nu/2 )}$ |\n\n\n:::\n:::\n\n\n\n\nI have always thought that the $\\chi^2$ distribution has an unusual name. The *chi* part is fine, but why *square*? Why not call it the $\\chi$ distribution?^[Actually, there *is* a $\\chi$ distribution. It is simply the square root of the $\\chi^2$ distribution. The half-normal distribution happens to be a $\\chi$ distribution with 1 degree of freedom.] As it turns out, the $\\chi^2$ distribution is formed from squared quantities.\n\n\n[**Notation note**: A $\\chi^2$ distribution with $\\nu$ degrees of freedom can be written as $\\chi^2_\\nu$ or $\\chi^2(\\nu)$.]{.column-margin}\n\n\nThe $\\chi^2$ distribution has a straightforward relationship with the normal distribution. It is the sum of multiple independent squared normal variates. That is, suppose $z$ is a standard normal variate:\n\n$$z\\sim\\mathcal{N}(0,1^2)$$\n\nIn this case, $z^2$ has a $\\chi^2$ distribution with 1 degree of freedom $(\\nu)$:\n\n\n$$\nz^2\\sim \\chi^2_1\n$$\n\n\nIf $z_1$ and $z_2$ are independent standard normal variates, the sum of their squares has a $\\chi^2$ distribution with 2 degrees of freedom:\n\n$$\nz_1^2+z_2^2 \\sim \\chi^2_2\n$$\n\nIf $\\{z_1,z_2,\\ldots,z_{\\nu} \\}$ is a series of $\\nu$ independent standard normal variates, the sum of their squares has a $\\chi^2$ distribution with $\\nu$ degrees of freedom: \n\n$$\n\\sum^\\nu_{i=1}{z_i^2} \\sim \\chi^2_\\nu\n$$\n\n### Clinical Uses of the $\\chi^2$ distribution\n\nThe $\\chi^2$ distribution has many applications, but the mostly likely of these to be used in psychological assessment is the $\\chi^2$ Test of Goodness of Fit and the $\\chi^2$ Test of Independence. \n\nThe $\\chi^2$ Test of Goodness of Fit tells us if observed frequencies of events differ from expected frequencies. Suppose we suspect that a child's temper tantrums are more likely to occur on weekdays than on weekends. The child's mother has kept a record of each tantrum for the past year and was able to count the frequency of tantrums. If tantrums were equally likely to occur on any day, 5 of 7 tantrums should occur on weekdays, and 2 of 7 tantrums should occur on weekends. The observed frequencies are compared with the expected frequencies below.\n\n$$\n\\begin{array}{r|c|c|c}\n& \\text{Weekday} & \\text{Weekend} & \\text{Total} \\\\\n\\hline\n\\text{Observed Frequency}\\, (o) & 14 & 14 & n=28\\\\\n\\text{Expected Proportion}\\,(p) & \\frac{5}{7} & \\frac{2}{7} & 1\\\\\n\\text{Expected Frequency}\\, (e = np)& 28\\times \\frac{5}{7}= 20& 28\\times \\frac{2}{7}= 8& 28\\\\\n\\text{Difference}\\,(o-e) & -6 & 6\\\\\n\\frac{(o-e)^2}{e} & 1.8 & 4.5 & \\chi^2 = 6.3\n\\end{array}\n$$\n\nIn the table above, if the observed frequencies $(o_i)$ are compared to their respective expected frequencies $(e_i)$, then:\n\n$$\\chi^2_{k-1}=\\sum_{i=1}^k{\\frac{(o_i-e_i)^2}{e_i}}=6.3$$\n\nUsing the $\\chi^2$ cumulative distribution function, we find that the probability of observing the frequencies listed is low under the assumption that tantrums are equally likely each day. \n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\nobserved_frequencies <- c(Weekday = 14, Weekend = 14)\nexpected_probabilities <- c(Weekday = 5, Weekend = 2) / 7\n\nfit <- chisq.test(x = observed_frequencies, \n                  p = expected_probabilities)\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tChi-squared test for given probabilities\n\ndata:  observed_frequencies\nX-squared = 6.3, df = 1, p-value = 0.01207\n```\n\n\n:::\n\n```{.r .cell-code}\n# View expected frequencies and residuals\nbroom::augment(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  Var1    .observed .prop .expected .resid .std.resid\n  <fct>       <dbl> <dbl>     <dbl>  <dbl>      <dbl>\n1 Weekday        14   0.5        20  -1.34      -2.51\n2 Weekend        14   0.5         8   2.12       2.51\n```\n\n\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\nd_table <- tibble(A = rbinom(100, 1, 0.5)) |> \n  mutate(B = rbinom(100, 1, (A + 0.5) / 3)) |>\n  table() \n\nd_table |> \n  as_tibble() |> \n  pivot_wider(names_from = A,\n              values_from = n) |> \nknitr::kable(align = \"lcc\") |>\n  kableExtra::kable_styling(bootstrap_options = \"basic\") |>\n  kableExtra::collapse_rows() |> \n  kableExtra::add_header_above(header = c(` ` = 1, A = 2)) |> \n  html_table_width(400)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n<tr>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"1\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">A</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\"> B </th>\n   <th style=\"text-align:center;\"> 0 </th>\n   <th style=\"text-align:center;\"> 1 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 0 </td>\n   <td style=\"text-align:center;\"> 47 </td>\n   <td style=\"text-align:center;vertical-align: middle !important;\" rowspan=\"2\"> 21 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:center;\"> 11 </td>\n   \n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\nfit <- chisq.test(d_table)\n\nbroom::augment(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 9\n  A     B     .observed .prop .row.prop .col.prop .expected .resid .std.resid\n  <fct> <fct>     <int> <dbl>     <dbl>     <dbl>     <dbl>  <dbl>      <dbl>\n1 0     0            47  0.47     0.810     0.691      39.4   1.20       3.28\n2 1     0            21  0.21     0.5       0.309      28.6  -1.41      -3.28\n3 0     1            11  0.11     0.190     0.344      18.6  -1.75      -3.28\n4 1     1            21  0.21     0.5       0.656      13.4   2.06       3.28\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Student's *t* Distributions\n\n\n\n\n::: {#tbl-tfeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-cap='Features of Student\\'s *t* Distributions<br>**Notation note**: $\\Gamma$ is the gamma function. $_2F_1$ is the hypergeometric function.' tbl-colwidths='[50,50]'}\n::: {.cell-output-display}\n\n\n|Feature                          |Symbol                                                                                                                                                                                                                    |\n|:--------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|Sample Space                     |$x \\in (-\\infty,\\infty)$                                                                                                                                                                                                  |\n|Degrees of Freedom               |$\\nu \\in (0,\\infty)$                                                                                                                                                                                                      |\n|Mean                             |$\\left\\{\n\\begin{array}{ll}\n      0 & \\nu \\gt  1 \\\\\n      \\text{Undefined} & \\nu \\le 1 \\\\\n\\end{array} \n\\right.$                                                                                                                 |\n|Variance                         |$\\left\\{\n\\begin{array}{ll}\n      \\frac{\\nu}{\\nu-2} & \\nu\\gt 2 \\\\\n      \\infty & 1 \\lt \\nu \\le 2\\\\\n      \\text{Undefined} & \\nu \\le 1 \\\\\n\\end{array} \n\\right.$                                                                   |\n|Skewness                         |$\\left\\{\n\\begin{array}{ll}\n      0 & \\nu \\gt  3 \\\\\n      \\text{Undefined} & \\nu \\le 3 \\\\\n\\end{array} \n\\right.$                                                                                                                 |\n|Kurtosis                         |$\\left\\{\n\\begin{array}{ll}\n      \\frac{6}{\\nu-4} & \\nu \\gt 4 \\\\\n      \\infty & 2 \\lt \\nu \\le 4\\\\\n      \\text{Undefined} & \\nu \\le 2 \\\\\n\\end{array} \n\\right.$                                                                    |\n|Probability Density Function     |$f_X(x; \\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})} {\\sqrt{\\nu\\pi}\\,\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{x^2}{\\nu} \\right)^{-\\frac{\\nu+1}{2}}$                                                                                  |\n|Cumulative Distribution Function |$F_X(x; \\nu)=\\frac{1}{2} + x \\Gamma \\left( \\frac{\\nu+1}{2} \\right)  \\frac{\\phantom{\\,}_{2}F_1 \\left(\\frac{1}{2},\\frac{\\nu+1}{2};\\frac{3}{2};-\\frac{x^2}{\\nu} \\right)} {\\sqrt{\\pi\\nu}\\,\\Gamma \\left(\\frac{\\nu}{2}\\right)}$ |\n\n\n:::\n:::\n\n::: {.cell .fig-column-margin .fig-cap-location-top .tbl-cap-location-margin layout-align=\"left\"}\n::: {.cell-output-display}\n![\"Student\" statistician, William Sealy Gosset (1876--1937)<br>[Image Credit](https://en.wikipedia.org/wiki/File:William_Sealy_Gosset.jpg)](images/William_Sealy_Gosset.jpg){#fig-gosset fig-align='left' width=60%}\n:::\n:::\n\n\n\n\n(Unfinished)\n\nGuinness Beer gets free advertisement every time the origin story of the Student *t* distribution is retold, and statisticians retell the story often. The fact that the original purpose of the *t* distribution was to brew better beer seems too good to be true. \n\nWilliam Sealy Gosset (1876--1937), self-trained statistician and head brewer at Guinness Brewery in Dublin, continually experimented on small batches to improve and standardize the brewing process. With some help from statistician Karl Pearson, Gosset used then-current statistical methods to analyze his experimental results. Gosset found that Pearson's methods required small adjustments when applied to small samples. With Pearson's help and encouragement (and later from Ronald Fisher), Gosset published a series of innovative papers about a wide range of statistical methods, including the *t* distribution, which can be used to describe the distribution of sample means.\n\nWorried about having its trade secrets divulged, Guinness did not allow its employees to publish scientific papers related to their work at Guinness. Thus, Gosset published his papers under the pseudonym, \"A Student.\" The straightforward names of most statistical concepts need no historical treatment. Few of us who regularly use the Bernoulli, Pareto, Cauchy, and Gumbell distributions could tell you anything about the people who discovered them. But the oddly named \"Student's *t* distribution\" cries out for explanation. Thus, in the long run, it was Gosset's anonymity that made him famous. \n\n\n\n\n\n\n\n\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\"}\n::: {.cell-output-display}\n![The *t* distribution approaches the standard normal distribution as the degrees of freedom (*df*) parameter increases.](images/tdist_norm.gif){#fig-tnorm fig-align='left' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-tnorm\n\n```{.r .cell-code}\n# The t distribution approaches the normal distribution\nd <- crossing(x = seq(-6,6,0.02), \n         df = c(seq(1,15,1),\n                seq(20,45,5),\n                seq(50,100,10),\n                seq(200,700,100))) %>%\n  mutate(y = dt(x,df),\n         Normal = dnorm(x)) \n\nt_size <- 40\n\nd_label <- d %>% \n  select(df) %>% \n  unique() %>% \n  mutate(lb = qt(.025, df),\n         ub = qt(0.975, df)) %>% \n  pivot_longer(c(lb, ub), values_to = \"x\", names_to = \"bounds\") %>% \n  mutate(label_x = signs::signs(x, accuracy = .01),\n         y = 0,\n         yend = dt(x, df))\n\np <- ggplot(d, aes(x, y)) + \n  geom_area(aes(y = Normal), alpha = 0.25, fill = myfills[1]) +\n  geom_line() +\n  geom_area(data = . %>% filter(x >= 1.96), \n            alpha = 0.25, \n            fill = myfills[1],\n            aes(y = Normal)) +\n  geom_area(data = . %>% filter(x <= -1.96), \n            alpha = 0.25, \n            fill = myfills[1],\n            aes(y = Normal)) +\n  geom_text(data = d_label, \n            aes(label = label_x), \n            family = bfont, \n            vjust = 1.25,\n            size = ggtext_size(t_size)) + \n  geom_text(data = d_label %>% select(df) %>% unique,\n            aes(x = 0, y = 0, label = paste0(\"df = \", df)), \n            vjust = 1.25, \n            family = bfont,\n            size = ggtext_size(t_size)) + \n  geom_segment(data = d_label, aes(xend = x, yend = yend)) +\n  transition_states(states = df, \n                    transition_length =  1, \n                    state_length = 2) +\n  theme_void(base_size = t_size, base_family = bfont) +\n  # labs(title = \"df = {closest_state}\") +\n  annotate(x = qnorm(c(0.025, 0.975)), \n           y = 0, \n           label = signs::signs(qnorm(c(0.025, 0.975)), accuracy = .01), \n           geom = \"text\", \n           size = ggtext_size(t_size),\n           color = myfills[1],\n           vjust = 2.6, \n           family = bfont) + \n  coord_cartesian(xlim = c(-6,6), ylim = c(-.045, NA)) \n\nanimate(p, \n        renderer = magick_renderer(), \n        device = \"svglite\", \n        fps = 2, \n        height = 6, \n        width = 10)\ngganimate::anim_save(\"tdist_norm.gif\")\n```\n\n\n:::\n:::\n\n\n\n\n### The *t* distribution's relationship Relationship to the normal distribution.\n\nSuppose we have two independent standard normal variates $Z_0 \\sim \\mathcal{N}(0, 1^2)$ and $Z_1 \\sim \\mathcal{N}(0, 1^2)$. \n\nA *t* distribution with one degree of freedom is created like so:\n\n$$\nT_1 = z_0\\sqrt{\\frac{1}{z_1^2}}\n$$\n\nA *t* distribution with two degrees of freedom is created like so:\n\n$$\nT_2 = z_0\\sqrt{\\frac{2}{z_1^2 + z_2^2}}\n$$\n\nWhere $z_0$, $z_1$ and $z_2$ are independent standard normal variates.\n\nA *t* distribution with $\\nu$ degrees of freedom is created like so:\n\n$$\nT_v = z_0\\sqrt{\\frac{\\nu}{\\sum_{i=1}^\\nu z_i^2}}\n$$\n\nThe sum of $\\nu$ squared standard normal variates $\\left(\\sum_{i=1}^\\nu z_i^2\\right)$ has a $\\chi^2$ distribution with $\\nu$ degrees of freedom, which has a mean of $\\nu$. Therefore, $\\sqrt{\\frac{\\nu}{\\sum_{i=1}^\\nu z_i^2}}$, on average, equals one. However, the expression $\\sqrt{\\frac{\\nu}{\\sum_{i=1}^\\nu z_i^2}}$ has a variability approaches 0 as $\\nu$ increases. When $\\nu$ is high, $z_0$ is being multiplied by a value very close to 1. Thus, $T_\\nu$ is nearly normal at high levels of $nu$. \n\n\n\n## Additional Distributions\n\n### F Distributions\n\nSuppose that $X$ is the ratio of two independent $\\chi^2$ variates $U_1$ and $U_2$ scaled by their degrees of freedom $\\nu_1$ and $\\nu_2$, respectively:\n\n$$X=\\frac{\\frac{U_1}{\\nu_1}}{\\frac{U_2}{\\nu_2}}$$\n\nThe random variate $X$ will have an $F$ distribution with parameters, $\\nu_1$ and $\\nu_2$.\n\nThe primary application of the $F$ distribution is to test the equality of variances in ANOVA. I am unaware of any direct applications of the *F* distribution in psychological assessment.\n\n\n### Weibull Distributions\n\nHow long do we have to wait before an event occurs? With Weibull distributions, we model wait times in which the probability of the event changes depending on how long we have waited. Some machines are designed to last a long time, but defects in a part might cause it fail quickly. If the machine is going to fail, it is likely to fail early. If the machine works flawlessly in the early period, we worry about it less. Of course, all physical objects wear out eventually, but a good design and regular maintenance might allow a machine to operate for decades. The longer machine has been working well, the less risk that it will irreparably fail on any particular day.\n\nFor some things, the risk of failure on any particular day becomes increasingly likely the longer it has been used. Biological aging causes increasing risk of death over time such that the historical records have no instances of anyone living beyond \n\nFor some events, there is a constant probability that the event will occur. For others, the probability is higher at first but becomes steadily less likely over time\n\n\nthe longer we wait the greater the probability will occur. For example, as animals age the probability of death accelerates such that beyond a certain age no individual as been observed to survive. \n\n### Unfinished\n\n* Gumbel Distributions\n* Beta Distributions\n* Exponential Distributions\n* Pareto Distributions\n\n",
    "supporting": [
      "continuousdistributions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}