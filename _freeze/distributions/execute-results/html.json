{
  "hash": "4587d8473681f34352bfcc94f9a25961",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Distributions\"\n---\n\n\n\n\n\n\n\n## Random Variables\n\nBecause we first learn about variables in an algebra class, we tend to think of variables as having values that can be solved for---if we have enough information about them. If I say that $x$ is a variable and that $x+6=8$, we can use algebra to find that $x$ must equal 2.\n\n[Random variables]{.defword title=\"**Random variables** have values that are determined by a random process.\"} are not like algebraic variables. Random variables simply take on values because of some random process. If we say that the outcome of a throw of a six-sided die is a random variable, there is nothing to \"solve for.\" There is no equation that determines the value of the die. Instead, it is determined by chance and the physical constraints of the die. That is, the outcome must be one of the numbers printed on the die, and the six numbers are equally likely to occur. This illustrates an important point. The word *random* here does not mean \"anything can happen.\" On a six-sided die, you will never roll a 7, 3.5, $\\sqrt{\\pi}$, &minus;36,000, or any other number that does not appear on the six sides of the die. Random variables have outcomes that are subject to random processes, but those random processes *do*  have constraints on them such that some outcomes are more likely than others---and some outcomes never occur at all. \n\n\n\n\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\" fig.cap.location='top'}\n::: {.cell-output-display}\n![Rolling a six-sided die is a process that creates a randomly ordered series of integers from 1 to 6.](images/randomvariable.gif){#fig-randomvariable fig-align='left' width=100}\n:::\n:::\n\n\n\n\n\nWhen we say that the throw of a six-sided die is a random variable, we are not talking about any particular throw of a particular die but, in a sense, *every* throw (that has ever happened or ever could happen) of *every* die (that has ever existed or could exist). Imagine an immense, roaring, neverending, cascading flow of dice falling from the sky. As each die lands and disappears, a giant scoreboard nearby records the relative frequencies of ones, twos, threes, fours, fives, and sixes. That's a random variable.\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-randomvariable\n\n```{.r .cell-code}\n# Function to make dice\nmakedice <- function(i, id) {\n  x = switch(\n           i,\n           `1` = 0,\n           `2` = c(-1, 1),\n           `3` = c(-1, 1, 0),\n           `4` = c(-1, 1, -1, 1),\n           `5` = c(-1, 1, -1, 1, 0),\n           `6` = c(-1, 1, -1, 1, -1, 1)\n         )\n  y = switch(\n           i,\n           `1` = 0,\n           `2` = c(1,-1),\n           `3` = c(1,-1, 0),\n           `4` = c(1,-1,-1, 1),\n           `5` = c(1,-1,-1, 1, 0),\n           `6` = c(1,-1,-1, 1, 0, 0))\n  \n  tibble(id = id * 1,\n         i = i,\n         x = x,\n         y = y) %>% \n    add_case(id = id + 0.5,\n             i = 0,\n             x = NA,\n             y = NA)\n}\n\n# Die radius\nr <- 0.35\ndpos <- 1.5\n\n# Die round arcs\ndround <- tibble(x0 = c(-dpos, dpos, -dpos, dpos), \n                 y0 = c(dpos,dpos,-dpos,-dpos), \n                 r = r, \n                 start = c(-pi / 2, pi / 2, -pi, pi / 2) , \n                 end = c(0,0, -pi / 2,pi) )\n\n# Line segments\ndsegments <- tibble(x = c(-dpos, dpos + r, dpos, -dpos - r), \n                    y = c(dpos + r,dpos,-dpos - r,-dpos),\n                    xend = c(dpos, dpos + r, -dpos,-dpos - r), \n                    yend = c(dpos + r,-dpos,-dpos - r, dpos))\n# Number of throws\nk <- 50\n\n# Dot positions\nd <- map2_df(sample(1:6,k, replace = T), 1:k, makedice)  \n\n# Plot\np <- ggplot(d) + \n  geom_point(pch = 16, size = 50, aes(x,y))  + \n  geom_arc(aes(x0 = x0,\n               y0 = y0,\n               r = r,\n               start = start,\n               end = end,\n               linetype = factor(r)),\n           data = dround,\n           linewidth = 2) + \n  geom_segment(data = dsegments,\n               aes(x = x, y = y, xend = xend, yend = yend),\n               linewidth = 2) +\n  coord_equal() + \n  theme_void() + \n  theme(legend.position = \"none\") + \n  transition_manual(id) \n\n# Render animation\nanimate(p, \n        fps = 1,\n        device = \"svg\",\n        renderer = magick_renderer(), \n        width = 8,\n        height = 8)\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n## Sets\n\nA [set]{.defword title=\"A **set** is a collection of distinct objects.\"} refers to a collection of objects. Each distinct object in a set is an [element]{.defword title=\"An **element** is a distinct member of a set.\"}. \n\n### Discrete Sets\n\nTo show that a list of discrete elements is a [discrete set]{.defword title=\"A **discrete set** has numbers that are isolated, meaning that each number has a range in which it is the only number in the overall set.\"}, we can use curly braces. For example, the set of positive single-digit even numbers is $\\{2, 4, 6, 8\\}$. With large sets with repeating patterns, it is convenient to use an ellipsis (\"...\"), the punctuation mark signifying an omission or pause. For example, rather than listing every two-digit positive even number, we can show the pattern like so: \n\n$$\\{10, 12, 14,\\ldots, 98\\}$$\n\nIf we want the pattern to repeat forever, we can set an ellipsis on the left, right, or both sides. The set of odd integers extends to infinity in both directions:\n\n$$\\{\\ldots, -5, -3, -1, 1, 3, 5, \\ldots\\}$$\n\n\n### Interval Sets\n\nWith continuous variables, we can define sets in terms of [intervals]{.defword title=\"**Intervals** are a continous range of numbers.\"}. Whereas the discrete set $\\{0,1\\}$ refers just to the numbers 0 and 1, the interval set $(0,1)$ refers to all the numbers between 0 and 1. \n\n\n\n\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\" fig.cap.location='top'}\n::: {.cell-output-display}\n![Interval Notation](distributions_files/figure-html/fig-intervalnotation-1.png){#fig-intervalnotation fig-align='left' width=300}\n:::\n:::\n\n\n\n\n\n\nAs shown in @fig-intervalnotation, some intervals include their endpoints and others do not. Intervals noted with square brackets include their endpoints and intervals written with parentheses exclude them. Some intervals extend to positive or negative infinity: $(-\\infty,5]$ and $(-8,+\\infty)$. Use a parenthesis with infinity instead of a square bracket because infinity is not a specific number that can be included in an interval.\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-intervalnotation\n\n```{.r .cell-code}\n# Interval notation\ntibble(lb = 1L,\n       ub = 5L,\n       y = 1:4,\n       meaning = c(\"includes 1 and 5\",\n                   \"excludes 1 and 5\",\n                   \"includes 1 but not 5\",\n                   \"includes 5 but not 1\"),\n       l_bracket = c(\"[\", \"(\", \"[\", \"(\"),\n       u_bracket = c(\"]\", \")\", \")\", \"]\")) %>% \n  mutate(Interval = paste0(l_bracket, lb, \",\", ub, u_bracket) %>% \n           fct_inorder() %>% \n           fct_rev,\n         l_fill = ifelse(l_bracket == \"[\", myfills[1], \"white\"),\n         u_fill = ifelse(u_bracket == \"]\", myfills[1], \"white\")) %>% \n  ggplot(aes(lb, Interval)) + \n  geom_segment(aes(xend = ub, yend = Interval), \n               linewidth = 2, \n               color = myfills[1]) +\n  geom_point(aes(fill = l_fill), \n             size = 5, \n             pch = 21, \n             stroke = 2, color = myfills[1]) + \n  geom_point(aes(fill = u_fill, x = ub), \n             size = 5, \n             pch = 21, \n             stroke = 2, \n             color = myfills[1]) +\n  geom_label(aes(label = paste(Interval, meaning), x = 3), \n             vjust = -.75, \n             label.padding = unit(0,\"lines\"), \n             label.size = 0, \n             family = bfont, \n             size = ggtext_size(27)) +\n  scale_fill_identity() +\n  scale_y_discrete(NULL, expand = expansion(c(0.08, 0.25))) +\n  scale_x_continuous(NULL, minor_breaks = NULL) +\n  theme_minimal(base_size = 27, \n                base_family = bfont) + \n  theme(axis.text.y = element_blank(), \n        panel.grid.major = element_blank(),\n        axis.line.x = element_line(linewidth = .5),\n        axis.ticks.x = element_line(linewidth = .25), \n        plot.margin = margin())\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Sample Spaces {#sec:SampleSpace}\n\nThe set of all possible outcomes of a random variable is the [sample space]{.defword title=\"A **sample space** is the set of all possible values that a random variable can assume.\"}. Continuing with our example, the sample space of a single throw of a six-sided die is the set $\\{1,2,3,4,5,6\\}$. *Sample space* is a curious term. Why *sample* and why *space*? With random variables, [populations]{.defword title=\"A **population** consists of all entities under consideration.\"} are infinitely large, at least theoretically. Random variables just keep spitting out numbers forever! So any time we actually observe numbers generated by a random variable, we are always observing a [sample]{.defword title=\"A **sample** is a subset of a population.\"}; actual infinities cannot be observed in their entirety. A *space* is a set that has mathematical structure. Most random variables generate either integers or real numbers, both of which are structured in many ways (e.g., order).\n\nUnlike distributions having to do with dice, many distributions have a sample space with an infinite number of elements. Interestingly, there are two kinds of infinity we can consider. A distribution's sample space might be the set of whole numbers: $\\{0,1,2,...\\}$, which extends to positive infinity. The sample space of all integers extends to infinity in both directions: $\\{...-2,-1,0,1,2,...\\}$.\n\nThe sample space of continuous variables is infinitely large for another reason. Between any two points in a continuous distribution, there is an infinite number of other points. For example, in the beta distribution, the sample space consists of all real numbers between 0 and 1: $(0,1)$. Many continuous distributions have sample spaces that involve both kinds of infinity. For example, the sample space of the [normal distribution](#sec-normal) consists of all real numbers from negative infinity to positive infinity: $(-\\infty, +\\infty)$.\n\n## Probability Distributions {#sec:ProbabilityDistribution}\n\n\n\nEach element of a random variable's sample space occurs with a particular probability. When we list the probabilities of each possible outcome, we have specified the variable's [probability distribution]{.defword title=\"In a **probability distribution**, there is an assignment of a probability to each possible element in a variable's sample space.\"}. In other words, if we know the probability distribution of a variable, we know how probable each outcome is. In the case of a throw of a single die, each outcome is equally likely (@fig-dicepmf).\n\n\n\n\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\" fig.cap.location='top'}\n::: {.cell-output-display}\n![The probability distribution of a throw of a single die](images/dice.svg){#fig-dicepmf fig-align='left' width=275}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='$\\small\\rm\\LaTeX$ Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## $\\small\\rm\\LaTeX$ Code for @fig-dicepmf\n\n```{.tex .cell-code}\n% Dice PMF\n\n\\documentclass[tikz = true, border = 2pt]{standalone}\n\\usepackage{tikz}\n\\usepackage{xfrac}\n\\usetikzlibrary{shapes,calc}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}\n\\tikzset{\n\tdot hidden/.style={},\n\tline hidden/.style={},\n\tdot colour/.style={dot hidden/.append style={color=#1}},\n\tdot colour/.default=black,\n\tline colour/.style={line hidden/.append style={color=#1}},\n\tline colour/.default=black\n}\n\n\\usepackage{xparse}\n\\NewDocumentCommand{\\drawdie}{O{}m}{\n\t\\begin{tikzpicture}[x=1em,y=1em,radius=0.1,#1]\n\t\\draw[rounded corners=2,line hidden] (0,0) rectangle (1,1);\n\t\\ifodd#2\n\t\\fill[dot hidden] (0.5,0.5) circle;\n\t\\fi\n\t\\ifnum#2>1\n\t\\fill[dot hidden] (0.25,0.25) circle;\n\t\\fill[dot hidden] (0.75,0.75) circle;\n\t\\ifnum#2>3\n\t\\fill[dot hidden] (0.25,0.75) circle;\n\t\\fill[dot hidden] (0.75,0.25) circle;\n\t\\ifnum#2>5\n\t\\fill[dot hidden] (0.75,0.5) circle;\n\t\\fill[dot hidden] (0.25,0.5) circle;\n\t\\ifnum#2>7\n\t\\fill[dot hidden] (0.5,0.75) circle;\n\t\\fill[dot hidden] (0.5,0.25) circle;\n\t\\fi\n\t\\fi\n\t\\fi\n\t\\fi\n\t\\end{tikzpicture}\n}\n\n\\begin{document}\n\t\\begin{tikzpicture}\n\t\\foreach \\n in {1,...,6} {\n\t\t\\node at ($(0,7)-(0,\\n)$) {\\drawdie [scale = 2]{\\n}};\n\t\t\\node [fill=gray!50,\n\t\t       minimum height = 2cm,\n\t\t       minimum width = 0.1cm,\n\t\t       single arrow,\n\t\t       single arrow head extend =.15cm,\n\t\t       single arrow head indent =.08cm,\n\t\t       inner sep=1mm] at ($(1.55,7)-(0,\\n)$) {};\n\t\t\\node  (p1) at (3,\\n) {\\large{$\\sfrac{\\text{1}}{\\text{6}}$}};\n\t}\n\t\\node [text centered,\n\t       anchor=south,\n\t       text height = 1.5ex,\n\t       text depth = .25ex] (p3) at (0,6.6) {\\large{{Sample Space}}};\n\t\\node [text centered,\n\t       anchor = south,\n\t       text height = 1.5ex,\n\t       text depth = .25ex] (p4) at (3,6.6) {\\large{{Probability}}};\n\t\\end{tikzpicture}\n\\end{document}\n```\n\n\n:::\n:::\n\n\n\n\n\nThere is an infinite variety of probability distributions, but a small subset of them have been given names. Now, one can manage one's affairs quite well without ever knowing what a Bernoulli distribution is, or what a $\\chi{^2}$ distribution is, or even what a normal distribution is. However, sometimes life is a little easier if we have names for useful things that occur often. Most of the distributions with names are not really single distributions, but families of distributions. The various members of a family are unique but they are united by the fact that their probability distributions are generated by a particular mathematical function (more on that later). In such cases, the probability distribution is often represented by a graph in which the sample space is on the $X$-axis and the associated probabilities are on the $Y$-axis. In @fig-pdfIllustration, 16 probability distributions that might be interesting and useful to clinicians are illustrated. Keep in mind that what are pictured are only particular members of the families listed; some family members look quite different from what is shown in @fig-pdfIllustration.\n\n\n\n\n\n::: {.cell .fig-column-page-right .fig-cap-location-top .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![A gallery of useful distributions](images/pdfIllustration.svg){#fig-pdfIllustration fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-pdfIllustration\n\n```{.r .cell-code}\n# A gallery of useful distributions\n# Run output file pdfIllustration.tex in LaTeX\n# pdflatex --enable-write18 --extra-mem-bot=10000000 --synctex=1 pdfIllustration.tex\ntikzpackages <- paste(\n  \"\\\\usepackage{tikz}\",\n  \"\\\\usepackage{amsmath}\",\n  \"\\\\usepackage[active,tightpage,psfixbb]{preview}\",\n  \"\\\\PreviewEnvironment{pgfpicture}\",\n  collapse = \"\\n\"\n)\n\ntikzDevice::tikz('pdfIllustration.tex',\n                 standAlone = TRUE, \n                 packages = tikzpackages, \n                 width = 11, \n                 height = 11)\npar(\n  mar = c(1.75, 1.3, 1.75, 0),\n  mfrow = c(4, 4),\n  las = 1,\n  xpd = TRUE,\n  family = 'serif',\n  pty = \"s\",\n  mgp = c(2, 0.5, 0),\n  tcl = -0.3,\n  cex = 1.35\n)\n\n# Bernoulli\nplot(\n  c(0.2, 0.8) ~ seq(0, 1),\n  type = \"b\",\n  ylim = c(0, 1),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Bernoulli\",\n  lty = 3,\n  pch = 19,\n  xaxp = c(0, 1, 1),\n  xlim = c(-0.1, 1)\n)\ntext(x = .7, y = .8, \"$p=0.8$\")\n\n# Binomial\nplot(\n  dbinom(seq(0, 5), 5, 0.2) ~ seq(0, 5),\n  type = \"b\",\n  xlim = c(-0.1, 5),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Binomial\",\n  lty = 3,\n  pch = 19\n)\ntext(\n  x = c(3.5, 3.5),\n  y = c(.35, .25),\n  c(\"$p=0.2$\", \"$n=5$\"),\n  adj = 0\n)\n\n# Poisson\nplot(\n  dpois(0:10, 3) ~ seq(0, 10),\n  type = \"b\",\n  xlim = c(-0.1, 10),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Poisson\",\n  lty = 3,\n  pch = 19\n)\ntext(x = 7, y = .15, r\"($\\lambda=3$)\")\n\n# Geometric\nplot(\n  dgeom(0:4, prob = 0.8) ~ seq(0, 4),\n  type = \"b\",\n  ylim = c(0, .8),\n  xlim = c(-0.1, 4),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Geometric\",\n  lty = 3,\n  pch = 19\n)\ntext(x = 2, y = .6, \"$p=0.8$\")\n\n# Discrete Uniform\nplot(\n  rep(1 / 4, 4) ~ seq(1, 4),\n  type = \"b\",\n  ylim = c(0, 1),\n  xlim = c(0, 5),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Discrete Uniform\",\n  lty = 3,\n  pch = 19\n)\ntext(\n  x = c(1, 4),\n  y = c(.5, .5),\n  c(\"$a=1$\", \"$b=4$\"),\n  adj = 0.5\n)\n\n# Continuous\nplot(\n  c(0, 1 / 3, 1 / 3, 0) ~ c(1, 1, 4, 4),\n  type = \"n\",\n  ylim = c(0, 1),\n  xlim = c(0, 5),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 2,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Continuous Uniform\"\n)\npolygon(\n  c(1, 1, 4, 4),\n  c(0, 1 / 3, 1 / 3, 0),\n  col = myfills[1],\n  xpd = FALSE,\n  border = NA\n)\ntext(\n  x = c(1, 4),\n  y = c(.5, .5),\n  c(\"$a=1$\", \"$b=4$\"),\n  adj = 0.5\n)\n\n# Normal\nx <- seq(-4, 4, 0.02)\nplot(\n  dnorm(x) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Normal\",\n  lwd = 2,\n  bty = \"n\",\n  axes = F\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dnorm(x), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = FALSE,\n  border = NA\n)\naxis(2)\n\ntext(\n  x = c(-1.5, -1.5),\n  y = c(.35, .25),\n  c(\"$\\\\mu=0$\", \"$\\\\sigma^2=1$\"),\n  adj = 1\n)\n\n\ncenter_neg <- function(x) {\n  signs <- sign(x)\n  paste0(ifelse(signs < 0,\"$\",\"\"), x, ifelse(signs < 0,\"\\\\phantom{-}$\",\"\"))\n}\n\nall_tick_labels <- function(side = 1, at, labels = at) {\n  axis(side, labels = rep(\"\",length(at)), at = at)\nfor (i in 1:length(at)) {\n  axis(side, \n       at = at[i], \n       labels = labels[i],\n       tick = F)\n  }\n}\naxis_ticks <- seq(-4,4,2)\naxis_labs <- center_neg(axis_ticks)\nall_tick_labels(1, at = axis_ticks, labels = axis_labs)\n\n\n# Student t\nx <- seq(-6, 6, 0.02)\nplot(\n  dt(x, 2) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Student's $\\\\boldsymbol{t}$\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.4),\n  axes = F\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dt(x, 2), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = FALSE,\n  border = NA\n)\ntext(x = 3, y = .3, \"$\\\\nu=2$\")\naxis(2)\naxis_ticks <- seq(-6,6,2)\naxis_labs <- center_neg(axis_ticks)\nall_tick_labels(1, at = axis_ticks, labels = axis_labs)\n\n# Chi-Square\nx <- seq(0, 40, 0.05)\nplot(\n  dchisq(x, 13) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"$\\\\boldsymbol{\\\\chi^2}$\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.1)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dchisq(x, 13), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = FALSE,\n  border = NA\n)\ntext(x = 20, y = .08, \"$k=2$\", adj = 0)\n\n# F\nx <- seq(0, 6, 0.01)\nplot(\n  df(x, 3, 120) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"$\\\\boldsymbol{F}$\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.8)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, df(x, 3, 120), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(2, 2),\n  y = c(.6, .4),\n  c(\"$d_1=3$\", \"$d_2=120$\"),\n  adj = 0\n)\n\n# Weibull\nx <- seq(6.5, 11.5, 0.01)\nplot(\n  dweibull(x, 20, 10) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Weibull\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.8)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dweibull(x, 20, 10), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(7, 7),\n  y = c(.6, .4),\n  c(\"$k=20$\", \"$\\\\lambda=10$\"),\n  adj = 0\n)\n\n# Beta\nx <- seq(0, 1, 0.01)\nplot(\n  dbeta(x, 2, 2.5) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Beta\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 2),\n  xaxp = c(0, 1, 1)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dbeta(x, 2, 2.5), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(0.75, 0.75),\n  y = c(1.75, 1.25),\n  c(\"$\\\\alpha=2$\", \"$\\\\beta=2.5$\"),\n  adj = 0\n)\n\n# Log Normal\nx <- c(seq(0, .999, 0.001), seq(1, 15, .05))\nplot(\n  dlnorm(x, 2, .5) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Log Normal\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.4)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dlnorm(x, 1, 0.5), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(7, 7),\n  y = c(.3, .2),\n  c(\"$\\\\mu=2$\", \"$\\\\sigma=0.5$\"),\n  adj = 0\n)\n\n\n# Skew Normal\nx <- seq(-4, 4, 0.01)\nplot(\n  sn::dsn(x, 2, 2.5) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Skew Normal\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.5),\n  axes = F\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, sn::dsn(\n    x,\n    xi = 1,\n    omega = 1.5,\n    alpha = -4\n  ), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(-3.9, -3.9, -3.9),\n  y = c(.45, .35, .25),\n  c(\"$\\\\xi=1$\", \"$\\\\omega=1.5$\", \"$\\\\alpha=-4$\"),\n  adj = 0\n)\n\naxis(2)\naxis_ticks <- seq(-4,4,2)\naxis_labs <- center_neg(axis_ticks)\nall_tick_labels(1, at = axis_ticks, labels =axis_labs)\n\n# Normal Mixture\nx <- seq(-4, 4, 0.01)\ny <- (dnorm(x, -2, 0.5) * .25 + dnorm(x))\nplot(\n  y ~ x,\n  type = \"n\",\n  col = \"violet\",\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Normal Mixture\",\n  lwd = 4,\n  bty = \"n\",\n  ylim = c(0, 0.5),\n  axes = F\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, y, 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dnorm(x, -2, 0.5) * .25, 0),\n  col = myfills[2],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(-0.85, -0.85),\n  y = c(.45, .35),\n  c(\"$\\\\mu_1=-2$\", \"$\\\\sigma_1^2=0.5$\"),\n  adj = 1\n)\ntext(\n  x = c(3.5, 3.5),\n  y = c(.45, .35),\n  c(\"$\\\\mu_2=0$\", \"$\\\\sigma_2^2=1$\"),\n  adj = 1\n)\naxis(2)\naxis_ticks <- seq(-4,4,2)\naxis_labs <- center_neg(axis_ticks)\nall_tick_labels(1, at = axis_ticks, labels =axis_labs)\n# Bivariate Normal\n\nx = seq(-4, 4, 0.1)\nX = fMultivar::grid2d(x)\nz = fMultivar::dnorm2d(X$x, X$y, rho = 0.6)\nZ = list(x = x,\n         y = x,\n         z = matrix(z, ncol = length(x)))\npersp(\n  Z,\n  theta = 20,\n  phi = 25,\n  col = \"royalblue1\",\n  xlab = \"\\nX\",\n  ylab = \"\\nY\",\n  zlab = \"\",\n  zlim = c(0, .20),\n  border = NA,\n  expand = .7,\n  box = FALSE,\n  ticktype = \"simple\",\n  ltheta = 0,\n  shade = 0.5,\n  main = \"Bivariate Normal\",\n  lwd = 0.5\n)\ntext(c(-.11, .32), c(-.42, -.25), c(\"X\", \"Y\"))\ntext(0, 0.25, \"$\\\\rho_{XY}=0.6$\")\ndev.off()\n```\n\n\n:::\n:::\n\n\n\n\n\n## Discrete Distrubitions\n\nThe sample spaces in discrete distributions are discrete sets. Thus, in the x-axis of the probability distributions, you will see isolated numbers with gaps between each number (e.g., integers). \n\n### Discrete Uniform Distributions {#sec:DiscreteUniform}\n\nThe throw of a single die is a member of a family of distributions called the [discrete uniform distribution]{.defword title=\"A **discrete uniform distribution** is a family of random variable distributions in which the sample space is an evenly spaced sequence of numbers, each of which is equally likely to occur.\"}. It is \"discrete\" because the elements in the sample space are countable, with evenly spaced gaps between them. For example, there might be a sequence of 8, 9, 10, and 11 in the sample space, but there are no numbers in between. It is \"uniform\" because all outcomes are equally likely. With dice, the numbers range from a lower bound of 1 to an upper bound of 6. In the family of discrete uniform distributions, the lower and upper bounds are typically integers, mostly likely starting with 1. However, any real number $a$ can be the lower bound and the spacing $k$ between numbers can be any positive real number. For the sake of simplicity and convenience, I will assume that the discrete uniform distribution refers to consecutive integers ranging from a lower bound of $a$ and an upper bound of $b$. \n\nThis kind of discrete uniform distribution has a number of characteristics listed in @tbl-uniformfeatures. I will explain each of them in the sections that follow. As we go, I will also explain the mathematical notation. For example, $a \\in \\{\\ldots,-1,0,1,\\ldots\\}$ means that $a$ is an integer because $\\in$ means *is a member of* and $\\{\\ldots,-1,0,1,\\ldots\\}$ is the set of all integers.^[**Notation note**: Sometimes the set of all integers is referred to with the symbol $\\mathbb{Z}.$] $x \\in \\{a,a+1,\\ldots,b\\}$ means that the each member of the sample space $x$ is a member of the set of integers that include $a$, $b$, and all the integers between $a$ and $b$. The notation for the [probability mass function](#sec:pmf) and the [cumuluative distribution function](#sec:CumDist) function will be explained later in this chapter.\n\n\n\n\n\n\n::: {#tbl-uniformfeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-cap='Features of Discrete Uniform Distributions'}\n::: {.cell-output-display}\n\n\n|Feature                          |Symbol                                |\n|:--------------------------------|:-------------------------------------|\n|Lower Bound                      |$a \\in \\{\\ldots,-1,0,1,\\ldots\\}$      |\n|Upper Bound                      |$b \\in \\{a + 1, a + 2,  \\ldots\\}$     |\n|Sample Space                     |$x \\in\\{a, a + 1,\\ldots,b\\}$          |\n|Number of points                 |$n=b-a+1$                             |\n|Mean                             |$\\mu=\\frac{a+b}{2}$                   |\n|Variance                         |$\\sigma^2=\\frac{n^2-1}{12}$           |\n|Skewness                         |$\\gamma_1=0$                          |\n|Kurtosis                         |$\\gamma_2=-\\frac{6(n^2+1)}{5(n^2-1)}$ |\n|Probability Mass Function        |$f_X(x;a,b)=\\frac{1}{n}$              |\n|Cumulative Distribution Function |$F_X(x;a,b)=\\frac{x-a+1}{n}$          |\n\n\n:::\n:::\n\n\n\n\n\n### Parameters of Random Variables \n\nThe lower bound $a$ and the upper bound $b$ are the discrete uniform distribution's [parameters]{.defword title=\"A **parameter** is a defining feature of a random variable's probability distribution.\"}. The word *parameter* has many meanings, but here it refers to a characteristic of a distribution family that helps us identify precisely which member of the family we are talking about. Most distribution families have one, two, or three parameters. \n\nIf you have taken an algebra class, you have seen parameters before, though the word *parameter* many not have been used. Think about the formula of a line:\n\n$$\ny=mx+b\n$$\n\n\nBoth $x$ and $y$ are variables, but what are $m$ and $b$? Well, you probably remember that $m$ is the slope of the line and that $b$ is the $y$-intercept. If we know the slope and the intercept of a line, we know exactly which line we are talking about. No additional information is needed to graph the line. Therefore, $m$ and $b$ are the line's *parameters*, because they uniquely identify the line.^[What about other mathematical functions? Do they have parameters? Yes! Most do! For example, in the equation for a parabola $(y=ax^2+bx+c)$, $a$, $b$, and $c$ determine its precise shape.] All lines have a lot in common but there is an infinite variety of lines because the parameters, the slope and the intercept, can take on the value of any real number. Each unique combination of parameter values (slope and intercept) will produce a unique line. So it is with probability distribution families. All family members are alike in many ways but they also differ because of different parameter values.\n\nThe discrete uniform distribution (i.e., the typical variety consisting of consecutive integers) is defined by the lower and upper bound. Once we know the lower bound and the upper bound, we know exactly which distribution we are talking about.^[If we allow the lower bound to be any real number and the spacing to be any positive real number, the discrete uniform distribution can be specified by three parameters: the lower bound $a$, the spacing between numbers $k$ $(k>0)$, and the number of points $n$ $(n>1)$. The upper bound $b$ of such a distribution would be $b=a+k(n-1)$] Not all distributions are defined by their lower and upper bounds. Indeed, many distribution families are unbounded on one or both sides. Therefore, other features are used to characterize the distributions, such as the population mean $(\\mu)$.\n\n### Probability Mass Functions {#sec:pmf}\n\nMany distribution families are united by the fact that their probability distributions are generated by a particular mathematical function. For discrete distributions, those functions are called [probability mass functions]{.defword title=\"A **probability mass function** is a mathematical expression that gives the probability that a discrete random variable will equal a particular element of the variable's sample space.\"}. In general, a mathematical function is an expression that takes one or more constants (i.e., parameters) and one or more input variables, which are then transformed according to some sort of rule to yield a single number.\n\nA probability mass function transforms a random variable's sample space elements into probabilities. In @fig-dicepmf, the probability mass function can be thought of as the arrows between the sample space and the probabilities. That is, the probability mass function is the thing that was done to the sample space elements to calculate the probabilities. In @fig-dicepmf, each outcome of a throw of the the die was mapped onto a probability of &frac16;. Why &frac16;, and not some other number? The probability mass function of the discrete uniform distribution tells us the answer. \n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![Probability mass functions tell us how probable each sample space element is. That is, they are functions that convert samples spaces $(\\boldsymbol{x})$ into probabilities $(\\boldsymbol{p})$ according to specific parameters $(\\boldsymbol{\\theta}).$](images/pmf.svg){#fig-pmf fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='$\\small\\rm\\LaTeX$ Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## $\\small\\rm\\LaTeX$ Code for @fig-pmf\n\n```{.tex .cell-code}\n% Probability mass functions tell us how probable each sample space element is\n\n\\documentclass[tikz = true, border = 2pt]{standalone}\n\n\\usepackage{amsmath}\n\\usepackage{tikz}\n\\usetikzlibrary{decorations.text, arrows}\n\\usetikzlibrary{shapes}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}\n\n\\begin{document}\n\\begin{tikzpicture}[>=stealth,scale=0.9]\n\\definecolor{royalblue2}{RGB}{39,64,139};\n\\node [rectangle,\n       draw,\n       rounded corners= 2pt,\n       text depth=0.25ex,\n       minimum height = 7mm](ss) at (0,0) {$\\boldsymbol{x}=x_1,x_2,x_3,\\ldots,x_n$};\n\\node [rectangle,\n       draw,\n       rounded corners= 2pt,\n       text depth=0.25ex,\n       minimum height = 7mm](ps) at (7,0) {$\\boldsymbol{p}=p_1,p_2,p_3,\\ldots,p_n$};\n\\node [single arrow,\n       fill=royalblue2,\n       single arrow head extend=1.1ex,\n       transform shape,\n       minimum height=0.9cm,\n       text depth=0.25ex, text=white] (fx) at (3.5,0) {$\\quad\\;\\; f_X\\quad\\;\\;$};\n\\node [text depth=2.25ex,\n       text height= 5ex,\n       anchor=south,\n       yshift=-3.5ex](sst) at (0,0.75) {\\textbf{Sample Space}};\n\\node [text depth=2.25ex,\n       text height= 5ex,\n       anchor=south,\n       yshift=-3.5ex](pst) at (7,0.75) {\\textbf{Probabilities}};\n\\node [shape=rectangle,\n       text depth=2.25ex,\n       color=royalblue2,\n       align=center,\n       text height= 5ex,\n       anchor=south,\n       yshift=-3.5ex](pmf) at (3.5,0.75) {\\textbf{Probability}\\\\\n\t\\textbf{Mass Function}};\n\\node [text depth=2.25ex,\n       text height= 5ex,\n       anchor=south,\n       yshift=-3.5ex] (pt) at (3.5,-1.5) {\\textbf{Parameters}};\n\\node [rectangle,\n       draw,\n       rounded corners = 2pt,\n       text depth=0.25ex,\n       minimum height = 7mm] (pts) at (3.5,-2.25) {$\\boldsymbol{\\theta}=\\theta_1,\\theta_2,\\theta_3,\\ldots,\\theta_k$};\n%\\node [align=center,\n%       shape=rectangle,\n%       rounded corners = 3pt,\n%       draw](formula) at (3.5,4) {\\textbf{\\Large{Scary~Math!}}\\\\\n%\t$f_X\\!\\left(\\boldsymbol{x};\\boldsymbol{\\theta}\\right)=\\boldsymbol{p}$};\n%\\draw [rounded corners=5pt] (-2.5,-3) rectangle (9.5,2.65);\n\\node at (3.5,2) {$f_X\\!\\left(\\boldsymbol{x};\\boldsymbol{\\theta}\\right)=\\boldsymbol{p}$};\n\\draw[->,>=latex', very thick] (3.5,-1.15) to (3.5,-0.5);\n\\end{tikzpicture}\n\\end{document}\n```\n\n\n:::\n:::\n\n\n\n\n\nThe probability mass function of the discrete uniform distribution is fairly simple but the notation can be intimidating at first (@fig-pmf). By convention, a single random variable is denoted by a capital letter $X$. Any particular value of $X$ in its sample space is represented by a lowercase $x$. In other words, $X$ represents the variable in its totality whereas $x$ is merely one value that $X$ can take on. Confusing? Yes, statisticians work very hard to confuse us---and most of the time they succeed! \n\nThe probability mass function of random variable $X$ is denoted by $f_X(x)$. This looks strange at first. It means, \"When random variable $X$ generates a number, what is the probability that the outcome will be a particular value $x$?\" That is, $f_X(x)=P(X=x)$, where $P$ means \"What is the probability that...?\" Thus, $P(X=x)$ reads, \"What is the probability that random variable $X$ will generate a number equal to a particular value $x$?\" So, $f_X(7)$ reads, \"When random variable $X$ generates a number, what is the probability that the outcome will be 7?\"\n\nMost probability mass functions also have parameters, which are listed after a semi-colon. In the case of the discrete uniform distribution consisting of consecutive integers, the lower and upper bounds $a$ and $b$ are included in the function's notation like so: $f_X(x;a,b)$. This reads, \"For random variable $X$ with parameters $a$ and $b$, what is the probability that the outcome will be $x$?\" Some parameters can be derived from other parameters, as was the case with the number of points $n$ in the sample space of a discrete uniform distribution: $n=b-a+1$. The probability for each outcome in the sample space is the same and there are $n$ possible outcomes. Therefore, the probability associated with each outcome is $\\frac{1}{n}$.\n\nPutting all of this together, if $a$ and $b$ are integers and $a<b$, for all $n$ integers $x$ between $a$ and $b$, inclusive:\n\n$$\n\\begin{aligned}\nf_X\\left(x;a,b\\right)&=\\frac{1}{b-a+1}\\\\[2ex]\n&=\\frac{1}{n}\n\\end{aligned}\n$$\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|Symbol |Meaning                                                |\n|:------|:------------------------------------------------------|\n|$X$    |A random variable with a discrete uniform distribution |\n|$f_X$  |The probability mass function of $X$                   |\n|$x$    |Any particular member of the sample space of $X$       |\n|$a$    |The lower bound of the sample space                    |\n|$b$    |The upper bound of the sample space                    |\n|$n$    |$b-a+1$ (The number of points in the sample space)     |\n\n\n:::\n:::\n\n\n\n\n\n\nYou might notice that $x$ is not needed to calculate the probability. Why? Because this is a *uniform* distribution. No matter which sample space element $x$ we are talking about, the probability associated with it is always the same. In distributions that are not uniform, the position of $x$ matters and thus influences the probability of its occurrence.\n\n### Cumulative Distribution Functions {#sec:CumDist}\n\nThe [cumulative distribution function]{.defword title=\"A **cumulative distribution function** is a mathematical expression that gives the probability that a random variable will equal a particular element of the variable's sample space or less.\"} tells us where a sample space element ranks in a distribution. Whereas the probability mass function tells us the probability that a random variable will generate a particular number, the cumulative distribution function tells us the probability that a random variable will generate a particular number or less. \n\n$$\nF_X(x) = P(X \\le x)=p\n$$\n\n\n\n\n\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\" fig.cap.location='top'}\n::: {.cell-output-display}\n![The cumulative distribution function of the roll of a die is $F_X(x)=\\frac{x}{6}$](images/cdfDie.svg){#fig-cdfDie fig-align='left' width=90%}\n:::\n:::\n\n\n\n\n\nThe cumulative distribution function of the roll of a die (@fig-cdfDie) tells us that the probability of rolling at least a 4 is 4&frasl;6 (i.e., &frac23;).\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='$\\small\\rm\\LaTeX$ Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## $\\small\\rm\\LaTeX$ Code for @fig-cdfDie\n\n```{.tex .cell-code}\n% CDF Dice\n\n\\documentclass[tikz = true, border = 2pt]{standalone}\n\\usepackage{tikz}\n\\usepackage{xfrac}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}\n\n\\definecolor{firebrick}{RGB}{205,38,38}\n\\definecolor{royalblue}{RGB}{67,110,238}\n\n\\tikzset{\n\tdot hidden/.style={},\n\tline hidden/.style={},\n\tdot colour/.style={dot hidden/.append style={color=#1}},\n\tdot colour/.default=black,\n\tline colour/.style={line hidden/.append style={color=#1}},\n\tline colour/.default=black\n}\n\n\\NewDocumentCommand{\\drawdie}{O{}m}{\n\t\\begin{tikzpicture}[x=1em,y=1em,radius=0.1,#1]\n\t\\draw[rounded corners=2,line hidden] (0,0) rectangle (1,1);\n\t\\ifodd#2\n\t\\fill[dot hidden] (0.5,0.5) circle;\n\t\\fi\n\t\\ifnum#2>1\n\t\\fill[dot hidden] (0.25,0.25) circle;\n\t\\fill[dot hidden] (0.75,0.75) circle;\n\t\\ifnum#2>3\n\t\\fill[dot hidden] (0.25,0.75) circle;\n\t\\fill[dot hidden] (0.75,0.25) circle;\n\t\\ifnum#2>5\n\t\\fill[dot hidden] (0.75,0.5) circle;\n\t\\fill[dot hidden] (0.25,0.5) circle;\n\t\\ifnum#2>7\n\t\\fill[dot hidden] (0.5,0.75) circle;\n\t\\fill[dot hidden] (0.5,0.25) circle;\n\t\\fi\n\t\\fi\n\t\\fi\n\t\\fi\n\t\\end{tikzpicture}\n}\n\n\\begin{document}\n\n\\begin{tikzpicture}\n\\foreach \\i in {1,...,6} {\n\t\\node at (0.2,\\i){$\\sfrac{\\text{\\i}}{\\text{6}}$};\n\t\\node at (\\i,0.2){\\large{\\i}};\n\t\\foreach \\j in {1,...,6} {\n\t\t\\ifnum \\j>\\i\n\t\t\\node at (\\i,\\j) {\\drawdie [scale=2]{\\j}};\n\t\t\\else\n\t\t\\node at (\\i,\\j) {\\drawdie [scale=2,line colour=royalblue,dot colour=royalblue]{\\j}};\n\t\t\\fi\n\t}\n}\n\n\\draw [firebrick,\n       very thick,\n       rounded corners]\n       (0.55,0.5)-- ++\n       (0,1)-- ++\n       (1,0)-- ++\n       (0,1)-- ++\n       (1,0)-- ++\n       (0,1)-- ++\n       (1,0)-- ++\n       (0,1)-- ++\n       (1,0)-- ++\n       (0,1)-- ++\n       (1,0)-- ++\n       (0,1)-- ++\n       (1,0)-- ++\n       (0,-6)--cycle;\n\\node[rotate=90] at (-0.3,3.5) {{Probability}};\n\\node at (3.5,-0.2) {{Die roll is this value or less}};\n\\end{tikzpicture}\n\n\\end{document}\n```\n\n\n:::\n:::\n\n\n\n\n\nThe cumulative distribution function is often distinguished from the probability mass function with a capital $F$ instead of a lowercase $f$. In the case of a discrete uniform distribution consisting of $n$ consecutive integers from $a$ to $b$, the cumulative distribution function is:\n\n$$\n\\begin{align*}\nF_X(x;a,b)=\\frac{x-a+1}{b-a+1}\\\\[2ex]\n=\\frac{x-a+1}{n}\n\\end{align*}\n$$\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|Symbol |Meaning                                                |\n|:------|:------------------------------------------------------|\n|$X$    |A random variable with a discrete uniform distribution |\n|$F_X$  |The cumulative distribution function of $X$            |\n|$x$    |Any particular member of the sample space of $X$       |\n|$a$    |The lower bound of the sample space                    |\n|$b$    |The upper bound of the sample space                    |\n|$n$    |$b-a+1$ (The number of points in the sample space)     |\n\n\n:::\n:::\n\n\n\n\n\n\nIn the case of the the six-sided die, the cumulative distribution function is \n\n$$\n\\begin{aligned}\nF_X(x;a=1,b=6)&=\\frac{x-a+1}{b-a+1}\\\\[2ex]\n&=\\frac{x-1+1}{6-1+1}\\\\[2ex]\n&=\\frac{x}{6}\n\\end{aligned}\n$$\n\nThe cumulative distribution function is so-named because it adds all the probabilities in the probability mass function up to and including a particular member of the sample space. @fig-pmf2cdf shows how the each probability in the cumulative distribution function of the roll of a six-sided die is the sum of the current and all previous probabilities in the probability mass function.\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![The cumulative distribution function is the sum of the current and all previous elements of the probability mass function.](images/pmf2cdf.gif){#fig-pmf2cdf fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-pmf2cdf\n\n```{.r .cell-code}\n# The cumulative distribution function is the sum of the current \n# and all previous elements of the probability mass function\np <- crossing(id = 1:6,\n         x = 1:6) %>% \n  mutate(pmf = 1 / 6) %>% \n  mutate(cdf = ifelse(id < x, 1 / 6, x / 6)) %>% \n  ggplot(aes(x = x, y = cdf)) + \n  geom_segment(aes(yend = cdf - pmf, xend = x), \n               color = myfills[2]) + \n  geom_segment(aes(y = 0, yend = pmf, xend = x), \n               color = myfills[1]) + \n  geom_line(aes(y = pmf), lty = \"dotted\", color = myfills[1]) +\n  geom_line(data = . %>% dplyr::filter(x <= id), \n            lty = \"dotted\", \n            color = myfills[2] ) +\n  geom_point(aes(y = pmf),\n             color = myfills[1],\n             size = 5) +\n  geom_point(data = . %>% dplyr::filter(x <= id), \n             color = myfills[2],\n             size = 3.5) +\n  scale_x_continuous(\"Sample Space\", \n                     breaks = 1:6,\n                     expand = c(0.03,0),\n                     minor_breaks = NULL) +\n  scale_y_continuous(\"Probability\",\n                     breaks = 0:6 / 6, \n                     labels = c(0, paste0(1:5, \"/\", 6),1),\n                     expand = c(0.03,0),\n                     minor_breaks = NULL) +\n  theme_minimal(base_size = 18, base_family = bfont) +\n  coord_fixed(6) + \n  annotate(\"point\",\n           size = 5, \n           x = 1.2,  \n           y = 5.33 / 6, \n           color = myfills[1]) + \n  annotate(\"point\",\n           size = 3.5, \n           x = 1.2,  \n           y = 5.66 / 6, \n           color = myfills[2])  + \n  annotate(\"label\",\n           size = 6, \n           x = 1.33,  \n           y = 5.33 / 6, \n           color = myfills[1],\n           label = \"Probability Mass Function\",\n           hjust = 0,\n           family = bfont, \n           label.padding = unit(0, \"lines\"), \n           label.size = 0\n           ) + \n  annotate(\"label\",\n           size = 6, \n           x = 1.33,  \n           y = 5.66 / 6, \n           color = myfills[2],\n           label = \"Cumulative Distribution Function\",\n           hjust = 0,\n           family = bfont, \n           label.padding = unit(0, \"lines\"), \n           label.size = 0) +\n  transition_states(id,2,1) +\n  ease_aes(\"sine-in-out\")\n\nanimate(p, \n        device = \"svg\",\n        renderer = magick_renderer(), \n        width = 8,\n        height = 8)\n```\n\n\n:::\n:::\n\n\n\n\n\n### Quantile functions\n\nThe inverse of the cumulative distribution function is the [quantile function]{.defword title=\"A **quantile function** tells us which value in the sample space of a random variable is greater than a particular proportion of the values the random variable generates.\"}. The cumulative distribution starts with a value $x$ in the sample space and tells us $p$, the proportion of values in that distribution that are less than or equal to $x$. A quantile function starts with a proportion $p$ and tells us the value $x$ that splits the distribution such that the proportion $p$ of the distribution is less than or equal to $x$. \n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![The quantile function is the inverse of the cumulative distribution function: Just flip the X and Y axes!](distributions_files/figure-html/fig-cdfvquantile-1.png){#fig-cdfvquantile fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\nAs seen in @fig-cdfvquantile, if you see a graph of a continuous distribution function, just flip the $X$ and $Y$ axes, and you have a graph of a quantile function.\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-cdfvquantile\n\n```{.r .cell-code}\n# The quantile function is the inverse of the cumulative distribution function\n\nd <- tibble(x = seq(-4, 4, 0.01)) %>%\n  mutate(p = pnorm(x))\n\np1 <- ggplot(d, aes(x, p)) +\n  geom_arrow(\n    arrow_head = arrow_head_deltoid(),\n    arrow_fins = arrow_head_deltoid(),\n    color = myfills[1],\n    lwd = 1\n  ) +\n  scale_x_continuous(\"Sample Space (*x*)\", labels = \\(x) signs::signs(x, accuracy = 1)) +\n  scale_y_continuous(\"Proportion (*p*)\", labels = prob_label) +\n  theme_minimal(base_size = 26, base_family = bfont) +\n  coord_fixed(8) +\n  annotate(\n    \"richtext\",\n    x = -2,\n    y = .5 + .25 / 4,\n    label = paste0(\"*X* ~ \",\n                   span_style(\"N\", style = \"font-family:'Lucida Calligraphy'\"),\n                   \"(0, 1<sup>2</sup>)\"),\n    family = bfont,\n    label.size = 0,\n    label.padding = unit(0, \"mm\"),\n    size = ggtext_size(26)\n  ) +\n  ggtitle(\"Cumulative Distribution Function *F<sub>X</sub>*(*x*) = *p*\") +\n  theme(\n    plot.title = ggtext::element_markdown(size = 26 * 0.8), \n    plot.title.position = \"plot\",\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    axis.text.x = element_text(hjust = c(0.7, 0.7, 0.5, 0.5, 0.5))\n  )\n\np2 <- ggplot(d, aes(p, x)) +\n  geom_arrow(\n    arrow_head = arrow_head_deltoid(),\n    arrow_fins = arrow_head_deltoid(),\n    color = myfills[1],\n    lwd = 1\n  ) +\n  scale_y_continuous(\"Sample Space (*x*)\", labels = \\(x) signs::signs(x, accuracy = 1)) +\n  scale_x_continuous(\"Proportion (*p*)\", labels = prob_label) +\n  theme_minimal(base_size = 26, base_family = bfont) +\n  coord_fixed(1 / 8) +\n  annotate(\n    \"richtext\",\n    y = 0.5,\n    x = .25,\n    label = paste0(\"*X* ~ \",\n                   span_style(\"N\", style = \"font-family:'Lucida Calligraphy'\"),\n                   \"(0, 1<sup>2</sup>)\"),\n    family = bfont,\n    label.size = 0,\n    label.padding = unit(0, \"mm\"),\n    size = ggtext_size(26)\n  ) +\n  ggtitle(\"Quantile Function *Q*<sub>*X*</sub>(*p*) = *x*\") +\n  theme(plot.title = ggtext::element_markdown(size = 26 * 0.8), \n        plot.title.position = \"plot\",\n        axis.title.x = ggtext::element_markdown(),\n        axis.title.y = ggtext::element_markdown())\n\np1 + p2\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n### Generating a Random Sample in R\n\nIn R, the `sample` function generates numbers from the discrete uniform distribution. \n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# n = the sample size\nn <- 6000\n# a = the lower bound\na <- 1\n# b = the upper bound\nb <- 6\n# The sample space is the sequence of integers from a to b\nsample_space <- seq(a, b)\n# X = the sample with a discrete uniform distribution\n# The sample function selects n values\n# from the sample space with replacement at random\nX <- sample(sample_space, \n            size = n, \n            replace = TRUE)\n```\n:::\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\" fig.cap.location='top'}\n::: {.cell-output-display}\n![Frequency distribution of a discrete uniform random variable from 1 to 6 (*n* = 6,000)](distributions_files/figure-html/fig-discreteplot-1.png){#fig-discreteplot fig-align='left' width=400}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-discreteplot\n\n```{.r .cell-code}\n# Frequency distribution of a discrete uniform random variable from 1 to 6 \n\ntibble(X = factor(X)) %>% \n  group_by(X) %>% \n  summarise(Frequency = n()) %>% \n  ggplot(aes(X,Frequency, fill = X)) + \n  geom_col(width = 0.7, fill = myfills[1]) + \n  geom_label(aes(label = Frequency), \n             vjust = -0.3, \n             label.size = 0,\n             label.padding = unit(0,\"mm\"), \n             family = bfont,\n             size = 8,\n             color = \"gray30\",\n             fill = \"white\") + \n  theme_minimal(base_size = 28, \n                base_family = bfont) + \n  scale_y_continuous(\"Count\", \n                     expand = expansion(c(0,0.075)), \n                     breaks = seq(0,1000,200)) + \n  scale_x_discrete(NULL) +\n  theme(panel.grid.major.x = element_blank(), \n        legend.position = \"none\") \n```\n\n\n:::\n:::\n\n\n\n\n\nThe frequencies of the random sample can be seen in @fig-discreteplot. Because of [sampling error]{.defword title=\"Samples imperfectly represent the population from which they are drawn. **Sampling error** refers to differences between sample statistics and population parameters.\"}, the frequencies are approximately the same, but not exactly the same. If the sample is larger, the sampling error is smaller, meaning that the sample's characteristics will tend to more closely resemble the population characteristics. In this case, a larger sample size will produce frequency counts that will appear more even in their magnitude. However, as long as the sample is smaller than the population, sampling error will always be present. With random distributions, the population is assumed to be infinitely large, and thus sampling error at best becomes negligibly small.\n\n\n### Bernoulli Distributions {#sec:BernoulliDist}\n\n\n\n\n\n::: {#tbl-bernoullifeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-cap='Features of Bernoulli Distributions<br>**Notation note:** Whereas $\\{a,b\\}$ is the set of just two numbers, $a$ and $b$, $[a,b]$ is the set of all real numbers between $a$ and $b$.' tbl-colwidths='[50,50]'}\n::: {.cell-output-display}\n\n\n|Feature                          |Symbol                                |\n|:--------------------------------|:-------------------------------------|\n|Sample Space:                    |$x \\in \\{0,1\\}$                       |\n|Probability that $x=1$           |$p \\in {[0,1]}$                       |\n|Probability that $x=0$           |$q = 1 - p$                           |\n|Mean                             |$\\mu = p$                             |\n|Variance                         |$\\sigma^2 = pq$                       |\n|Skewness                         |$\\gamma_1 = \\frac{1 - 2p}{\\sqrt{pq}}$ |\n|Kurtosis                         |$\\gamma_2 = \\frac{1}{pq} - 6$         |\n|Probability Mass Function        |$f_X(x;p) = p^xq^{1 - x}$             |\n|Cumulative Distribution Function |$F_X(x;p) = x+p(1 - x)$               |\n\n\n:::\n:::\n\n\n\n\n\n\nThe toss of a single coin has the simplest probability distribution that I can think of---there are only two outcomes and each outcome is equally probable (@fig-coin). This is a special case of the [Bernoulli distribution]{.defword title=\"In the **Bernoulli distribution**, there are only two outcomes: a &ldquo;success&rdquo; (1) and a &ldquo;failure&rdquo; (0). If a success has a probability $p$ then a failure has a probability of $q = 1 - p$.\"}. Jakob Bernoulli (@fig-bernoullipic) was a famous mathematician from a famous family of mathematicians. The Bernoulli distribution is just one of the ideas that made Jakob and the other Bernoullis famous. \n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='$\\small\\rm\\LaTeX$ Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## $\\small\\rm\\LaTeX$ Code for @fig-coin\n\n```{.tex .cell-code}\n% Coin toss Bernoulli\n\\documentclass[tikz = true, border = 2pt]{standalone}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}\n\\usepackage{tikz}\n\\usetikzlibrary{shapes}\n\\usepackage{xfrac}\n\n\\begin{document}\n\t\t\\begin{tikzpicture}[scale=0.9]\n\t\t\\node (H) at (0,2) {\n\t\t\t\\includegraphics [width=1.5cm]{../images/QuarterHeads.png}\n\t\t};\n\t\t\\node (T) at (0,0) {\n\t\t\t\\includegraphics [width=1.5cm]{../images/QuarterTails.png}\n\t\t};\n\t\t\\node [fill=gray!50,\n\t\t       minimum height=1.5cm,\n\t\t       minimum width=0.1cm,\n\t\t       single arrow,\n\t\t       single arrow head extend=.15cm,\n\t\t       single arrow head indent=.08cm,\n\t\t       inner sep=1mm] (arrowtails1) at (1.9,2) {};\n\t\t\\node [fill=gray!50,\n\t\t       minimum height=1.5cm,\n\t\t       minimum width=0.1cm,\n\t\t       single arrow,\n\t\t       single arrow head extend=.15cm,\n\t\t       single arrow head indent=.08cm,\n\t\t       inner sep=1mm] (arrowheads2) at (1.9,0) {};\n\t\t\\node  (p1) at (3.4,2) {\\huge{$\\sfrac{\\text{1}}{\\text{2}}$}};\n\t\t\\node  (p2) at (3.4,0) {\\huge{$\\sfrac{\\text{1}}{\\text{2}}$}};\n\t\t\\node [text centered,\n\t\t       anchor=south,\n\t\t       text height=1.5ex,\n\t\t       text depth=.25ex] (p3) at (0,3) {\\large{Sample Space}};\n\t\t\\node [text centered,\n\t\t       anchor=south,\n\t\t       text height=1.5ex,\n\t\t       text depth=.25ex] (p4) at (3.4,3) {\\large{Probability}};\n\t\t\\end{tikzpicture}\n\n\\end{document}\n```\n\n\n:::\n:::\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\" fig.cap.location='top'}\n::: {.cell-output-display}\n![The probability distribution of a coin toss](images/coin.svg){#fig-coin fig-align='left' width=300}\n:::\n:::\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\" fig.cap.location='top'}\n::: {.cell-output-display}\n![Jakob Bernoulli (1654--1705)<br>[Image Credits](https://en.wikipedia.org/wiki/File:Jakob_Bernoulli.jpg)](images/Jakob_Bernoulli2.jpg){#fig-bernoullipic fig-align='left' width=200}\n:::\n:::\n\n\n\n\n\nThe Bernoulli distribution can describe any random variable that has two outcomes, one of which has a probability $p$ and the other has a probability $q=1-p$. In the case of a coin flip, $p=0.5$. For other variables with a Bernoulli distribution, $p$ can range from 0 to 1.\n\n\n\nIn psychological assessment, many of the variables we encounter have a Bernoulli distribution. In ability test items in which there is no partial credit, examinees either succeed or fail. The probability of success on an item (in the whole population) is $p$. In other words, $p$ is the proportion of the entire population that correctly answers the question. Some ability test items are very easy and the probability of success is high. In such cases, $p$ is close to 1. When $p$ is close to 0, few people succeed and items are deemed hard. Thus, in the context of ability testing, $p$ is called the [difficulty parameter]{.defword title=\"The **difficulty parameter** is the proportion of people who succeed on an ability item or endorse a yes/no questionnaire item.\"}. This is confusing because when $p$ is high, the item is easy, not difficult. Many people have suggested that it would make more sense to call it the \"easiness parameter\" but the idea has never caught on.\n\nTrue/False and Yes/No items on questionnaires also have Bernoulli distributions. If an item is frequently endorsed as true (\"I like ice cream.\"), $p$ is high. If an item is infrequently endorsed (\"I like black licorice and mayonnaise in my ice cream.\"), $p$ is very low. Oddly, the language of ability tests prevails even here. Frequently endorsed questionnaire items are referred to as \"easy\" and infrequently endorsed items are referred to as \"difficult,\" even though there is nothing particularly easy or difficult about answering them either way.\n\n#### Generating a Random Sample from the Bernoulli Distribution\n\n\n\n\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\" fig.cap.location='top'}\n::: {.cell-output-display}\n![Counts of a Random Variable with a Bernoulli Distribution $(p = 0.8, n = 1000)$](distributions_files/figure-html/fig-BernoulliSample-1.png){#fig-BernoulliSample fig-align='left' width=300}\n:::\n:::\n\n\n\n\n\nIn R, there is no specialized function for the Bernoulli distribution because it turns out that the Bernoulli distribution is a special case of the [binomial distribution](#sec:binomial), which will be described in the next section. With the function `rbinom`, we can generate data with a Bernoulli distribution by setting the `size` parameter equal to 1.\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# n = sample size\nn <- 1000\n# p = probability\np <- 0.8\n# X = sample\nX <- rbinom(n, size = 1, prob = p)\n# Make a basic plot\nbarplot(table(X))\n```\n:::\n\n\n\n\n\nIn @fig-BernoulliSample, we can see that the random variable generated a sequence that consists of about 80% ones and 20% zeroes. However, because of sampling error, the results are rarely exactly what the population parameter specifies. \n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-BernoulliSample\n\n```{.r .cell-code}\n# Counts of a Random Variable with a Bernoulli Distribution\nset.seed(4)\n# n = sample size\nn <- 1000\n# p = probability\np <- 0.8\n# X = sample\nX <- rbinom(n, size = 1, prob = p)\n# Make a basic plot\nggplot(tibble(X = factor(X)), aes(X)) +\n  geom_bar(fill = myfills[1]) +\n  scale_x_discrete(NULL) + \n  scale_y_continuous(NULL, expand = expansion(c(0.01,0.1))) +\n  geom_text(aes(label = after_stat(count)), \n            stat = \"count\", \n            vjust = -0.4, \n            size = ggtext_size(18),\n            family = bfont,\n            color = \"gray10\") + \n    theme_minimal(18, bfont) + \n  theme(panel.grid.major.x = element_blank())\n```\n\n\n:::\n:::\n\n\n\n\n\n### Binomial Distributions {#sec:binomial}\n\n\n\n\n\n::: {#tbl-binomialfeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-colwidths='[50,50]' tbl-cap='Features of Binomial Distributions'}\n::: {.cell-output-display}\n\n\n|Feature                              |Symbol                                                       |\n|:------------------------------------|:------------------------------------------------------------|\n|Number of Trials                     |$n \\in \\{1,2,3,\\ldots\\}$                                     |\n|Sample Space                         |$x \\in \\{0,...,n\\}$                                          |\n|Probability of success in each trial |$p \\in [0,1]$                                                |\n|Probability of failure in each trial |$q = 1 - p$                                                  |\n|Mean                                 |$\\mu = np$                                                   |\n|Variance                             |$\\sigma = npq$                                               |\n|Skewness                             |$\\gamma_1 = \\frac{1-2p}{\\sqrt{npq}}$                         |\n|Kurtosis                             |$\\gamma_2 = \\frac{1}{npq} - \\frac{6}{n}$                     |\n|Probability Mass Function            |$f_X(x;n,p)=\\frac{n!}{x!\\left(n-x\\right)!}p^x q^{n-x}$       |\n|Cumulative Distribution Function     |$F_X(x;n,p)=\\sum_{i=0}^{x}{\\frac{n!}{i!(n-i)!} p^i q^{n-i}}$ |\n\n\n:::\n:::\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\" fig.cap.location='top'}\n::: {.cell-output-display}\n![Probability distribution of the number of heads observed when two coins are tossed](images/twocoin.svg){#fig-twocoin fig-align='left' width=75%}\n:::\n:::\n\n\n\n\n\nLet's extend the idea of coin tosses and see where it leads. Imagine that two coins are tossed at the same time and we count how many heads there are. The outcome we might observe will be zero, one, or two heads. Thus, the sample space for the outcome of the tossing of two coins is the set $\\{0,1,2\\}$ heads. There is only one way that we will observe no heads (both coins tails) and only one way that we will observe two heads (both coins heads). In contrast, as seen in @fig-twocoin, there are two ways that we can observe one head (heads-tails \\& tails-heads).\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='$\\small\\rm\\LaTeX$ Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## $\\small\\rm\\LaTeX$ Code for @fig-twocoin\n\n```{.tex .cell-code}\n% Probability distribution of the number of heads observed when two coins are tossed\n\\documentclass[tikz = true,border = 2pt]{standalone}\n\\usepackage{tikz}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}\n\\definecolor[named]{fillColor}{RGB}{39,64,139}\n\\begin{document}\n\\begin{tikzpicture}[x=1pt,\n                      y=1pt,\n                      xscale=.4,\n                      yscale=.66,\n                      axisline/.style={\n                      \tdraw=black!70,\n                      \tline width= 0.4pt,\n                      \tline join=round,\n                      \tline cap=round},\n                      axislabel/.style={\n                      \ttext=black!70,\n                      \tinner sep=0pt,\n                      \tfont=\\footnotesize,\n                      \touter sep=0pt,\n                      \tanchor=east},\n                      xaxislabel/.style={\n                      \ttext=black!70,\n                      \tinner sep=0pt,\n                      \tfont=\\footnotesize,\n                      \touter sep=0pt},\n                       bar/.style={\n                      \tdraw=white,\n                      \tline width= 0.4pt,\n                      \tline join=round,\n                      \tline cap=round,\n                      \tfill=fillColor,\n                        rounded corners = 1.5pt}                ]\n\n% X-axis\n\\node[xaxislabel] at (100, 42) {0};\n\\node[xaxislabel] at (200, 42) {1};\n\\node[xaxislabel] at (300, 42) {2};\n\\node[black!70] at (204.07, 28) {Number of Heads};\n\n% Y-axis\n\\path[axisline] ( 42, 50) -- ( 42,250);\n\\path[axisline] ( 38, 50) -- ( 42, 50);\n\\path[axisline] ( 38,150) -- ( 42,150);\n\\path[axisline] ( 38,250) -- ( 42,250);\n\\node[axislabel] at ( 36, 50) {0};\n\\node[axislabel] at ( 36,150) {.25};\n\\node[axislabel] at ( 36,250) {.50};\n\n% Bars\n\\path[bar] (50, 50.00) rectangle (150,150);\n\\path[bar] (150, 50.00) rectangle (250,250);\n\\path[bar] (250, 50.00) rectangle (350,150);\n\\node at (100,  80) {\\includegraphics [width=36pt]{../images/QuarterTails.png}};\n\\node at (100, 120) {\\includegraphics [width=36pt]{../images/QuarterTails.png}};\n\\node at (200,  80) {\\includegraphics [width=36pt]{../images/QuarterHeads.png}};\n\\node at (200, 120) {\\includegraphics [width=36pt]{../images/QuarterTails.png}};\n\\node at (200, 180) {\\includegraphics [width=36pt]{../images/QuarterTails.png}};\n\\node at (200, 220) {\\includegraphics [width=36pt]{../images/QuarterHeads.png}};\n\\node at (300,  80) {\\includegraphics [width=36pt]{../images/QuarterHeads.png}};\n\\node at (300, 120) {\\includegraphics [width=36pt]{../images/QuarterHeads.png}};\n\n\\end{tikzpicture}\n\\end{document}\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe probability distribution of the number of heads observed when two coins are tossed at the same time is a member of the *binomial distribution* family. The binomial distribution occurs when [independent]{.defword title=\"Two random variable are said to be **independent** if the outcome of one variable does not alter the probability of any outcome in the other variable.\"} random variables with the same [Bernoulli distribution](#sec:BernoulliDist) are added together. In fact, Bernoulli discovered the binomial distribution as well as the Bernoulli distribution.\n\nImagine that a die is rolled 10 times and we count how often a 6 occurs.^[Wait! Hold on! I thought that throwing dice resulted in a (discrete) *uniform* distribution. Well, it still does. However, now we are asking a different question. We are only concerned with two outcomes each time the die is thrown: 6 and not 6. This is a Bernoulli distribution, not a uniform distribution, because the probability of the two events is unequal: {&frac16;,&frac56;}] Each roll of the die is called a [trial]{.defword title=\"Every time a random variable generates a number, that instance of the variable is called a **trial**, which is also known as an **experiment**.\"}. The sample space of this random variable is $\\{0,1,2,...,10\\}$. What is the probability that a 6 will occur 5 times? or 1 time? or not at all? Such questions are answered by the binomial distribution's [probability mass function](#sec:pmf):\n\n\n$$\nf_X(x;n,p)=\\frac{n!}{x!\\left(n-x\\right)!}p^x\\left(1-p\\right)^{n-x}\n$$\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|Symbol |Meaning                                                                                    |\n|:------|:------------------------------------------------------------------------------------------|\n|$X$    |The random variable (the number of sixes from 10 throws of the die)                        |\n|$x$    |Any particular member of the sample space (i.e., $x \\in \\{0,1,2,...,10\\}$)                 |\n|$n$    |The number of times that the die is thrown (i.e., $n=10$)                                  |\n|$p$    |The probability that a six will occur on a single throw of the die (i.e., $p=\\frac{1}{6}$) |\n\n\n:::\n:::\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\" fig.cap.location='top'}\n::: {.cell-output-display}\n![The probability distribution of the number of sixes observed when a six-sided die is thrown 10 times.](distributions_files/figure-html/fig-pmf6-1.png){#fig-pmf6 fig-align='left' width=350}\n:::\n:::\n\n\n\n\n\n\nBecause $n=10$ and $p=\\frac{1}{6}$, the probability mass function simplifies to:\n\n\\begin{equation*}\nf_X(x)=\\frac{n!}{x!\\left(n-x\\right)!}\\left(\\frac{1}{6}\\right)^x\\left(\\frac{5}{6}\\right)^{10-x}\n\\end{equation*}\n\n\nIf we take each element $x$ of the sample space from 0 to 10 and plug it into the equation above, the probability distribution will look like @fig-pmf6.\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code\n\n```{.r .cell-code}\n# Probability of sixes\n\ntibble(sample_space = 0:10) %>% \n  mutate(\n    probability = dbinom(\n      sample_space, \n      size = max(sample_space), \n      prob = 1 / 6),\n    probabiltiy_label = prob_label(probability, \n                                   digits = 2)) %>% \n  ggplot(aes(factor(sample_space), probability)) +\n  geom_col(fill = myfills[1]) + \n  ggtext::geom_richtext(\n    aes(label = probabiltiy_label),\n    size = ggtext_size(30),\n    angle = 90, \n    hjust = 0,\n    family = bfont, \n    label.margin = unit(c(0,0,0,1), \"mm\"),\n    label.padding = unit(c(1.6,0,0,0),\"mm\"), \n    label.colour = NA,\n    color = \"gray40\") +\n  theme_minimal(base_family = bfont, base_size = 30) + \n  scale_y_continuous(\"Probability\", \n                     expand = expansion(c(0,.09)), \n                     labels = prob_label) + \n  scale_x_discrete(\"Sample Space (Number of Sixes)\") + \n  theme(panel.grid.major.x = element_blank()) \n```\n\n\n:::\n:::\n\n\n\n\n\n#### Clinical Applications of the Binomial Distribution \n\nWhen would a binomial distribution be used by a clinician? One particularly important use of the binomial distribution is in the detection of [malingering]{.defword title=\"A person who **malingers** is pretending to be sick to avoid work or some other responsibility.\"}. Sometimes people pretend to have memory loss or attention problems in order to win a lawsuit or collect insurance benefits. There are a number of ways to detect malingering but a common method is to give a very easy test of memory in which the person has at least a 50\\% chance of getting each test item correct even if the person guesses randomly. \n\nSuppose that there are 20 questions. Even if a person has the worst memory possible, that person is likely to get about half the questions correct. However, it is possible for someone with a legitimate memory problem to guess randomly and by bad luck answer fewer than half of the questions correctly. Suppose that a person gets 4 questions correct. How likely is it that a person would, by random guessing, only answer 4 or fewer questions correctly?\n\nWe can use the binomial distribution's cumulative distribution function. However, doing so by hand is rather tedious. Using R, the answer is found with the `pbinom` function:\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\np <- pbinom(4,20,0.5)\n```\n:::\n\n\n\n\n\nWe can see that the probability of randomly guessing and getting 4 or fewer items correct out of 20 items total is approximately 0.006, which is so low that the hypothesis that the person is malingering seems plausible. Note here that there is a big difference between these two questions:\n\n* If the person is guessing at random (i.e., not malingering), what is the probability of answering correctly 4 questions or fewer out of 20?\n* If the person answers 4 out of 20 questions correctly, what is the probability that the person is guessing at random (and therefore not malingering)?\n\nHere we answer only the first question. It is an important question, but the answer to the second question is probably the one that we really want to know. We will answer it in another chapter when we discuss positive predictive power. For now, we should just remember that the questions are different and that the answers can be quite different.\n\n#### Graphing the binomial distribution\n\nSuppose that there are $n=10$ trials, each of which have a probability of $p=0.8$. The sample space is the sequence of integers from 0 to 10, which can be generated with the `seq` function (i.e., `seq(0,10)`) or with the colon operator `0:10`.  First, the sample space is generated (a sequence from 0 to 10.), using the `seq` function. The associated probability mass function probabilities are found using the `dbinom` function. The cumulative distribution function probabilities are found using the `pbinom` function.\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make a sequence of numbers from 0 to 10\nSampleSpace <- seq(0, 10)\n# Probability mass distribution for \n# binomial distribution (n = 10, p = 0.8)\npmfBinomial <- dbinom(SampleSpace, \n                      size = 10, \n                      prob = 0.8)\n# Generate a basic plot of the \n# probability mass distribution\nplot(pmfBinomial ~ SampleSpace, \n     type = \"b\")\n# Cumulative distribution function \n# for binomial distribution (n = 10, p = 0.8)\ncdfBinomial <- pbinom(SampleSpace, \n                      size = 10, \n                      prob = 0.8)\n```\n\n::: {.cell-output-display}\n![Basic plot of a binomial probability mass function](distributions_files/figure-html/fig-binomGraphSimple-1.png){#fig-binomGraphSimple fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Cumulative distribution function \n# for binomial distribution (n = 10, p = 0.8)\ncdfBinomial <- pbinom(SampleSpace, \n                      size = 10, \n                      prob = 0.8)\n# Generate a basic plot of the\n# binomial cumulative distribution function\nplot(cdfBinomial ~ SampleSpace, \n     type = \"b\")\n```\n\n::: {.cell-output-display}\n![Basic plot of a binomial cumulative distribution function](distributions_files/figure-html/fig-binomGraphSimplecdf-1.png){#fig-binomGraphSimplecdf fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\nHowever, making the graph look professional involves quite a bit of code that can look daunting at first. However, the results are often worth the effort. Try running the code below to see the difference. For presentation-worthy graphics, export the graph to the .pdf or .svg format. An .svg file can be imported directly into MS Word or MS PowerPoint. \n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![Probability Mass Function and Cumulative Distribution Function of the Binomial Distribution $(n = 10,~p = 0.8)$](distributions_files/figure-html/fig-BinomialDistribution-1.png){#fig-BinomialDistribution fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-BinomialDistribution\n\n```{.r .cell-code}\n# Probability Mass Function and Cumulative Distribution \n# Function of the Binomial Distribution\ntibble(SampleSpace = 0:10,\n       pmf = dbinom(SampleSpace, 10, 0.8),\n       cdf = pbinom(SampleSpace, 10, 0.8)) %>%\n  pivot_longer(-SampleSpace, \n               names_to = \"Function\", \n               values_to = \"Proportion\") %>% \n  mutate(Function = factor(Function, \n                           levels = c(\"pmf\", \"cdf\")) ) %>%\n  arrange(desc(Function)) %>%\n  ggplot(aes(x = SampleSpace,\n             y = Proportion,\n             color = Function)) +\n  geom_pointline(lty = \"dotted\", distance = unit(3, \"mm\")) +\n  geom_point(aes(size = Function)) +\n  theme_minimal(base_family = \"serif\",\n                base_size = 18) +\n  scale_color_manual(values = myfills) +\n  scale_x_continuous(\"Sample Space\",\n                     breaks = seq(0, 10, 2),\n                     expand = expansion(0.02)) +\n  scale_y_continuous(expand = expansion(0.02),\n                     labels = prob_label) +\n  scale_size_manual(values = c(3, 4.5)) +\n  theme(legend.position = \"none\") +\n  annotate(\n    x = 10 - 0.16,\n    y = dbinom(10, 10, 0.8),\n    geom = \"label\",\n    label = \"Probability Mass Function\",\n    hjust = 1,\n    size = 4.75,\n    color = myfills[1],\n    label.size = 0,\n    family = \"serif\",\n    label.padding = unit(0, \"mm\")\n  ) +\n  annotate(\n    x = 10 - 0.2,\n    y = 1,\n    geom = \"label\",\n    label = \"Cumulative Distribution Function\",\n    hjust = 1,\n    size = 4.75,\n    color = myfills[2],\n    label.size = 0,\n    family = \"serif\",\n    label.padding = unit(0, \"mm\")\n  )\n```\n\n\n:::\n:::\n\n\n\n\n\n\n### Poisson Distributions\n\n\n\n\n\n::: {#tbl-PoissonFeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-cap='Features of Poisson Distributions<br>**Notation note**: The notation $(0,\\infty)$ means all real numbers greater than 0.' tbl-colwidths='[50,50]'}\n::: {.cell-output-display}\n\n\n|Feature                          |Symbol                                                               |\n|:--------------------------------|:--------------------------------------------------------------------|\n|Parameter                        |$\\lambda \\in (0,\\infty)$                                             |\n|Sample Space                     |$x\\in \\{0,1,2,\\ldots\\}$                                              |\n|Mean                             |$\\mu = \\lambda$                                                      |\n|Variance                         |$\\sigma^2 = \\lambda$                                                 |\n|Skewness                         |$\\gamma_1 = \\frac{1}{\\sqrt{\\lambda}}$                                |\n|Kurtosis                         |$\\gamma_2 = \\frac{1}{\\lambda}$                                       |\n|Probability Mass Function        |$f_X(x;\\lambda) = \\frac{\\lambda^x}{e^{\\lambda} x!}$                  |\n|Cumulative Distribution Function |$F_X(x;\\lambda) =  \\sum_{i=0}^{x}{\\frac{\\lambda^i}{e^{\\lambda} i!}}$ |\n\n\n:::\n:::\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\" fig.cap.location='top'}\n::: {.cell-output-display}\n![Siméon Denis Poisson (1781--1840)<br>[Image Credits](https://en.wikipedia.org/wiki/File:Simeon_Poisson.jpg)](images/Simeon_Poisson.jpg){#fig-PoissonPortrait fig-align='left' width=250px}\n:::\n:::\n\n\n\n\n\n\n\nImagine that an event happens sporadically at random and we measure how often it occurs in regular time intervals (e.g., events per hour). Sometimes the event does not occur in the interval, sometimes just once, and sometimes more than once. However, we notice that over many intervals, the average number of events is constant. The distribution of the number of events in each interval will follow a [Poisson distribution]{.defword title=\"The **Poisson distribution** is a discrete distribution used to model how often an event will occur during a particular interval of time.\"}. Although \"Poisson\" means \"fish\" in French, fish have nothing to do with it. This distribution was named after Siméon Denis Poisson, whose work on the distribution made it famous.\n\n\n\nThe Poisson distribution has a single parameter $\\lambda$, the average number of events per time interval. Interestingly, $\\lambda$ is both the mean and the variance of this distribution. The distribution shape will differ depending on how long our interval is. If an event occurs on average 30 times per hour, $\\lambda$ = 30. If we count how often the event occurs in 10-minute intervals, the same event will occur about 5 times per interval, on average (i.e., $\\lambda$ = 1). If we choose to count how often the same event occurs every minute, then $\\lambda$ = 0.5.\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![The shape of the Poisson Distribution depends on the interval used for counting events. Here, the event occurs once per minute, on average.](distributions_files/figure-html/fig-samelambda-1.png){#fig-samelambda fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-samelambda\n\n```{.r .cell-code}\n# The shape of the Poisson Distribution\ncrossing(lambda = c(0.5, 5, 30), x = 0:60) %>%\n  mutate(p = dpois(x, lambda)) %>%\n  mutate(lambda = factor(lambda, labels = paste0(\n    \"Every \",\n    c(\"30 Seconds\", \"5 Minutes\", \"Half Hour\"),\n    \" (\\u03BB = \",\n    c(0.5, 5, 30),\n    \")\"\n  ))) %>%\n  ggplot(aes(x, p, color = lambda)) +\n  geom_line(linetype = 3, linewidth = 0.1) +\n  geom_point(size = 2.5, color = \"white\") +\n  geom_point(size = 1) +\n  facet_grid(lambda ~ ., scales = \"free\") +\n  scale_x_continuous(\"Number of Events\", breaks = seq(0, 120, 20)) +\n  scale_color_manual(values = c(myfills[1], \"darkorchid\", myfills[2])) +\n  theme_light(base_size = 20, base_family = bfont) +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(\"Probability\", labels = prob_label)\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### A clinical application of the the Poisson distribution\n\nSuppose that you begin treating an adult male client who has panic attacks that come at unpredictable times.  Some weeks there are no panic attacks and some weeks there are many, but on average he has 2 panic attacks each week. The client knows this because he has kept detailed records in a spreadsheet for the last 5 years. The client had sought treatment once before, but terminated early and abruptly because, according to him, \"It wasn't working.\" After sensitive querying, you discover that he expected that treatment should have quickly reduced the frequency of panic attacks to zero. When that did not happen, he became discouraged and stopped the treatment.\n\nBecause your client is well educated and quantitatively inclined, you decide to to use the data he has collected as part of the intervention and also to help set a more realistic set of expectations. Obviously, you and your client both would prefer 0 panic attacks per week, but sometimes it takes more time to get to the final goal. We do not want to terminate treatment that is working just because the final goal has not yet been achieved.\n\nYou plot the frequency of how often he had 0 panic attacks in a week, 1 panic attack in a week, 2 panic attacks in a week, and so forth, as shown in red in @fig-PanicFrequency. Because you have read this book, you immediately recognize that this is a Poisson distribution with $\\lambda$ = 2. When you graph an actual Poison distribution and compare it with your client's data, you see that it is almost a perfect match.^[Note that I am **not** claiming that all clients' panic attack frequencies have this kind of distribution. It just so happens to apply in this instance.] Then you explain that although the goal is permanent cessation of the panic attacks, sometimes an intervention can be considered successful if the frequency of panic attacks is merely reduced. For example, suppose that in the early stages of treatment the frequency of panic attacks were reduced from twice per week to once every other week ($\\lambda$ = 0.5), on average. If such a reduction were achieved, there would still be weeks in which two or more panic attacks occur. According to @fig-PanicCumulativeFrequency, this will occur about 9\\% of the time.\n\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![The variability of a hypothetical client's panic attack frequency before and after treatment](distributions_files/figure-html/fig-PanicFrequency-1.png){#fig-PanicFrequency fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-PanicFrequency\n\n```{.r .cell-code}\n# The variability of a hypothetical \n# client's panic attack frequency\nd_label <- tibble(\n  x = c(0, 2),\n  Time = c(\"After\", \"Before\"),\n  Proportion = dpois(x, lambda = c(0.5, 2)),\n  Label = c(\"After Treatment: \\u03BB = 0.5\",\n            \"Before Treatment: \\u03BB = 2\")\n)\n\ntibble(\n  x = seq(0, 7),\n  Before = dpois(x, lambda = 2),\n  After = dpois(x, lambda = 0.5)\n) %>%\n  gather(key = Time, value = Proportion,-x) %>%\n  ggplot(aes(x, Proportion, color = Time)) +\n  geom_pointline(linetype = \"dotted\", linesize = .5, size = 3, distance = unit(2, \"mm\")) +\n  # geom_point(size = 3) +\n  geom_label(\n    data = d_label,\n    aes(label = Label),\n    hjust = 0,\n    nudge_x = 0.2,\n    family = bfont,\n    label.padding = unit(0, \"lines\"),\n    label.size = 0,\n    size = 6\n  ) +\n  scale_color_manual(values = myfills) +\n  scale_x_continuous(\"Panic Attacks Per Week\",\n                     breaks = 0:7,\n                     minor_breaks = NULL) +\n  scale_y_continuous(labels = . %>% prob_label(., 0.1)) +\n  theme_minimal(base_size = 20, base_family = bfont) +\n  theme(legend.position = \"none\")\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIn R, you can use the `dpois` function to plot the Poisson probability mass function. For example, if the average number of events per time period is &lambda; = 2, then the probability that there will be 0 events is `dpois(x = 0, lambda = 2)`, which evaluates to 0.1353.\n\nTo calculate the cumulative distribution function of Poisson distribution in R, use the `ppois` function. For example, if we want to estimate the probability of having 4 panic attacks or more in a week if &lambda; = 2, we must subtract the probability of having 3 panic attacks or less from 1, like so:\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n1 - ppois(q = 3, lambda = 2)\n```\n:::\n\n\n\n\n\n$p = 0.143$\n\nHere is a simple way to plot the probability mass function and the cumulative distribution function using the `dpois` and `ppois` functions:\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make a sequence of integers from 0 to 7\nPanicAttacks <- seq(0, 7)\n\n# Generate the probability mass function with lambda = 2\nProbability <- dpois(PanicAttacks, 2)\n\n# Basic plot of the Poisson \n# distribution's probability mass function\nplot(Probability ~ PanicAttacks, type = \"b\") \n```\n\n::: {.cell-output-display}\n![Poisson Probability Mass Function $(\\lambda=2)$](distributions_files/figure-html/fig-Panicpmf-1.png){#fig-Panicpmf fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Generate the cumulative \n# distribution function with lambda = 2\nCumulativeProbability <- ppois(PanicAttacks, 2)\n\n# Basic plot of the Poisson distribution's \n# cumulative distribution function\nplot(CumulativeProbability ~ PanicAttacks, type = \"b\") \n```\n\n::: {.cell-output-display}\n![Poisson Cumulative Distribution Function $(\\lambda=2)$](distributions_files/figure-html/fig-PanicCDF-1.png){#fig-PanicCDF fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\nWith an additional series with $\\lambda = 0.5$, the plot can look like @fig-PanicCumulativeFrequency.\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![The cumulative distribution function of a hypothetical client's panic attack frequency before and after treatment](distributions_files/figure-html/fig-PanicCumulativeFrequency-1.png){#fig-PanicCumulativeFrequency fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-PanicCumulativeFrequency\n\n```{.r .cell-code}\n# The cumulative distribution function \n# of a hypothetical client's\n# panic attack frequency before \n# and after treatment\n\nd_label <- tibble(\n  x = c(0, 0),\n  Time = c(\"After\", \"Before\"),\n  Proportion = ppois(x, lambda = c(0.5, 2)),\n  Label = c(\"After Treatment: *&lambda;* = 0.5\",\n            \"Before Treatment: *&lambda;* = 2\")\n)\n\ntibble(\n  x = seq(0, 7),\n  Before = ppois(x, lambda = 2),\n  After = ppois(x, lambda = 0.5)\n) %>%\n  gather(key = Time, value = Proportion,-x) %>%\n  ggplot(aes(x, Proportion, color = Time)) +\n  geom_pointline(linetype = \"dotted\", linesize = .5, size = 3, distance = unit(2, \"mm\")) +\n  geom_richtext(\n    data = d_label,\n    aes(label = Label),\n    hjust = 0,\n    nudge_x = 0.2,\n    family = bfont,\n    label.padding = unit(0, \"lines\"),\n    label.size = 0,\n    size = ggtext_size(20)\n  ) +\n  scale_color_manual(values = myfills) +\n  scale_y_continuous(breaks = seq(0, 1, 0.2), \n                     limits = c(0, 1),\n                     labels = . %>% prob_label(., 0.1)) +\n  scale_x_continuous(\"Panic Attacks Per Week\",\n                     breaks = 0:7,\n                     minor_breaks = NULL) +\n  theme_minimal(base_size = 20, base_family = bfont) +\n  theme(legend.position = \"none\")\n```\n\n\n:::\n:::\n\n\n\n\n\n### Geometric Distributions\n\n\n\n\n\n::: {#tbl-GeometricFeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-cap='Features of Geometric Distributions' tbl-colwidths='[50,50]'}\n::: {.cell-output-display}\n\n\n|Feature                              |Symbol                              |\n|:------------------------------------|:-----------------------------------|\n|Probability of success in each trial |$p\\in[0,1]$                         |\n|Sample Space                         |$x \\in \\{1,2,3,\\ldots\\}$            |\n|Mean                                 |$\\mu = \\frac{1}{p}$                 |\n|Variance                             |$\\sigma^2 = \\frac{1-p}{p^2}$        |\n|Skewness                             |$\\gamma_1 = \\frac{2-p}{\\sqrt{1-p}}$ |\n|Kurtosis                             |$\\gamma_2 = 6 + \\frac{p^2}{1-p}$    |\n|Probability Mass Function            |$f_X(x;p) = (1-p)^{x-1}p^x$         |\n|Cumulative Distribution Function     |$F_X(x;p) = 1-(1-p)^x$              |\n\n\n:::\n:::\n\n\n\n\n\nAtul Gawande [-@gawandeBetterSurgeonNotes2007, pp. 219--223] tells a marvelous anecdote about how a doctor used some statistics to help a young patient with cystic fibrosis to return to taking her medication more regularly. Because the story is full of pathos and masterfully told, I will not repeat a clumsy version of it here. However, unlike Gawande, I *will* show how the doctor's statistics were calculated. \n\nAccording to the story, if a patient fails to take medication, the probability that a person with cystic fibrosis will develop a bad lung illness on any particular day is .005. If medication is taken, the risk is .0005. Although these probabilities are both close to zero, over the the course of a year, they result in very different levels of risk. Off medication, the patient has about an 84% chance of getting sick within a year's time. On medication, the patient's risk falls to 17%. As seen in @fig-WithMedicationCDF, the cumulative risk over the course of 10 years is quite different. Without medication, the probability of becoming seriously ill within 10 years at least once is almost certain. With medication, however, a small but substantial percentage (~16%) of patients will go at least 10 years without becoming ill. \n\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![The cumulative risk of serious lung disease with and without medication](distributions_files/figure-html/fig-WithMedicationCDF-1.png){#fig-WithMedicationCDF fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-WithMedicationCDF\n\n```{.r .cell-code}\n# The cumulative risk of serious lung \n# disease with and without medication\n\ntotal_years <- 10\n\ntibble(Days = seq(0, total_years * 365, 10),\n       WithoutMeds = pgeom(Days, 0.005),\n       WithMeds = pgeom(Days, 0.0005)) %>% \n  gather(Meds, p, -Days) %>% \n  mutate(Years = Days / 365) %>% \n  ggplot(aes(Years, p, color = Meds)) + \n  geom_line(linewidth = 1) + \n  theme_minimal(base_size = 18, base_family = bfont) + \n  theme(legend.position = \"none\") +\n  scale_x_continuous(breaks = seq(0,total_years,2)) + \n  scale_y_continuous(\"Cumulative Risk\", breaks = seq(0,1,0.2),\n                     labels = . %>% prob_label(., 0.1)) +\n  scale_color_manual(values = myfills) +\n  coord_fixed(ratio = 10) +\n  annotate(x = 4, y = 0.93, \n           label = \"Without Medication\\nDaily Risk = .005\",\n           geom = \"label\", \n           color = myfills[2],\n           label.padding = unit(0,\"lines\"),\n           label.size = 0,\n           family = bfont,\n           size = 5.2\n           ) +\n  annotate(x = 6.25, y = 0.535, \n           label = \"With Medication\\nDaily Risk = .0005\",\n           geom = \"label\", \n           color = myfills[1],\n           label.padding = unit(0,\"lines\"),\n           label.size = 0,\n           family = bfont,\n           size = 5.2\n           )\n```\n\n\n:::\n:::\n\n\n\n\n\n\nSuch calculations make use of the *geometric distribution*. Consider a series of [Bernoulli trials](#sec:BernoulliDist) in which an event has a probability $p$ of occurring on any particular trial. The probability mass function of the geometric distribution will tell us the probability that the x^th^ trial will be the first time the event occurs. \n\n$$\nf_X(x;p)=(1-p)^{x-1}p^x\n$$\n\n\n\n\n\n::: {#tbl-geometric .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|Symbol |Meaning                                                           |\n|:------|:-----------------------------------------------------------------|\n|$X$    |A random variable with a geometric distribution                   |\n|$f_X$  |The probability mass function of $X$                              |\n|$x$    |The number of Bernoulli trials on which the event first occurs    |\n|$p$    |The probability of an event occurring on a single Bernoulli trial |\n\n\n:::\n:::\n\n\n\n\n\n\nIn R, the probability mass function of the geometric distribution is calculated with the `dgeom` function:\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make a sequence of integers from 1 to 10\nx <- seq(1, 10)\n\n# Generate the probability mass \n# function with p = 0.6\nProbability <- dgeom(x, prob = 0.6)\n\n# Basic plot of the geometric \n# distribution's probability mass function\nplot(Probability ~ x, type = \"b\") \n```\n\n::: {.cell-output-display}\n![Geometric Probability Mass Function $(p=.6)$](distributions_files/figure-html/fig-GeometricPMF-1.png){#fig-GeometricPMF fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\nThe cumulative distribution function of the geometric distribution was used to create @fig-WithMedicationCDF. It tells us the probability that the event will occur on the $x^{th}$ trial or earlier:\n\n$$\nF_X(x;p)=1-(1-p)^x\n$$\n\nIn R, the cumulative distribution function of the geometric distribution uses the `pgeom` function:\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Generate the cumulative \n# distribution function with p = 0.6\nCumulativeProbability <- pgeom(x, prob = 0.6)\n\n# Basic plot of the geometric\n# distribution's cumulative distribution function\nplot(CumulativeProbability ~ x, type = \"b\") \n```\n\n::: {.cell-output-display}\n![Geometric Cumulative Distribution Function $(p=.6)$](distributions_files/figure-html/fig-GeometricCDF-1.png){#fig-GeometricCDF fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n## Continuous Distributions\n\n### Probability Density Functions {#sec:pdf}\n\nAlthough there are many more discrete distribution families, we will now consider some continuous distribution families. Most of what we have learned about discrete distributions applies to continuous distributions. However, there is a need of a name change for the probability mass function. In a discrete distribution, we can calculate an actual probability for a particular value in the sample space. In continuous distributions, doing so can be tricky. We can always calculate the probability that a score in a particular interval will occur. However, in continuous distributions, the intervals can become very small, approaching a width of 0. When that happens, the probability associated with that interval also approaches 0. Yet, some parts of the distribution are more probable than others. Therefore, we need a measure of probability that tells us the probability of a value *relative* to other values: the [probability density function]{.defword title=\"The **probability density function** is function that can show relative likelihoods of sample space elements of a continuous random variable.\"} \n\nConsidering the entire sample space of a discrete distribution, all of the associated probabilities from the probability mass function sum to 1. In a probability density function, it is the area under the curve that must sum to 1. That is, there is a 100\\% probability that a value generated by the random variable will be somewhere under the curve. There is nowhere else for it to go!\n\nHowever, unlike probability mass functions, probability density functions do not generate probabilities. Remember, the probability of any value in the sample space of a continuous variable is infinitesimal. We can only compare the probabilities to each other. To see this, compare the discrete uniform distribution and continuous uniform distribution in @fig-pdfIllustration. Both distributions range from 1 to 4. In the discrete distribution, there are 4 points, each with a probability of &frac14;. It is easy to see that these 4 probabilities of &frac14; sum to 1. Because of the scale of the figure, it is not easy to see exactly how high the probability density function is in the continuous distribution. It happens to be &frac13;. Why? First, it does not mean that each value has a &frac13; probability. There are an infinite number of points between 1 and 4 and it would be absurd if each of them had a &frac13; probability. The distance between 1 and 4 is 3. In order for the rectangle to have an area of 1, its height must be &frac13;. What does that &frac13; mean, then? In the case of a single value in the sample space, it does not mean much at all. It is simply a value that we can compare to other values in the sample space. It could be scaled to any value, but for the sake of convenience it is scaled such that the area under the curve is 1. \n\nNote that some probability density functions can produce values greater than 1. If the range of a continuous uniform distribution is less than 1, at least some portions of the curve must be greater than 1 to make the area under the curve equal 1. For example, if the bounds of a continuous distribution are 0 and &frac13;, the average height of the probability density function would need to be 3 so that the total area is equal to 1.\n\n### Continuous Uniform Distributions {#sec:Uniform}\n\n\n\n\n\n::: {#tbl-continuousuniformfeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-cap='Features of Continuous Discrete Distributions'}\n::: {.cell-output-display}\n\n\n|Feature                          |Symbol                            |\n|:--------------------------------|:---------------------------------|\n|Lower Bound                      |$a  \\in (-\\infty,\\infty)$         |\n|Upper Bound                      |$b \\in (a,\\infty)$                |\n|Sample Space                     |$x \\in \\lbrack a,b\\rbrack$        |\n|Mean                             |$\\mu = \\frac{a+b}{2}$             |\n|Variance                         |$\\sigma^2 = \\frac{(b-a)^2-1}{12}$ |\n|Skewness                         |$\\gamma_1 = 0$                    |\n|Kurtosis                         |$\\gamma_2 = -\\frac{6}{5}$         |\n|Probability Density Function     |$f_X(x;a,b) = \\frac{1}{b-a}$      |\n|Cumulative Distribution Function |$F_X(x;a,b) = \\frac{x-a}{b-a}$    |\n\n\n:::\n:::\n\n\n\n\n\n\nUnlike the [discrete uniform distribution](#sec:DiscreteUniform), the uniform distribution is [continuous](#sec:DiscreteVsContinuous).^[For the sake of clarity, the uniform distribution is often referred to as the *continuous uniform distribution*.] In both distributions, there is an upper and lower bound and all members of the sample space are equally probable.\n\n#### Generating random samples from the continuous uniform distribution\n\nTo generate a sample of $n$ numbers with a continuous uniform distribution between $a$ and $b$, use the `runif` function like so:\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Sample size\nn <- 1000\n# Lower and upper bounds\na <- 10\nb <- 30\n# Sample\nx <- runif(n, min = a, max = b)\n```\n:::\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\" fig.cap.location='top'}\n::: {.cell-output-display}\n![Random sample (*n* = 1000) of a continuous uniform distribution between 10 and 30. Points are randomly jittered to show the distribution more clearly.](distributions_files/figure-html/fig-uniformdist-1.png){#fig-uniformdist fig-align='left' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-uniformdist\n\n```{.r .cell-code}\n# Plot\ntibble(x) %>% \nggplot(aes(x, y = 0.5)) + \n  geom_jitter(size = 0.5, \n              pch = 16,\n              color = myfills[1], \n              height = 0.45) +\n  scale_x_continuous(NULL) +\n  scale_y_continuous(NULL, \n                     breaks = NULL, \n                     limits = c(0,1), expand = expansion()) + \n  theme_minimal(base_family = bfont, base_size = bsize)\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Using the continuous uniform distribution to generate random samples from other distributions\n\nUniform distributions can begin and end at any real number but one member of the uniform distribution family is particularly important---the uniform distribution between 0 and 1. If you need to use Excel instead of a statistical package, you can use this distribution to generate random numbers from many other distributions. \n\nThe cumulative distribution function of any continuous distribution converts into a continuous uniform distribution. A distribution's [quantile function](#sec:Quantile) converts a continuous uniform distribution into that distribution. Most of the time, this process also works for discrete distributions. This process is particularly useful for generating random numbers with an unusual distribution. If the distribution's quantile function is known, a sample with a continuous uniform distribution can easily be generated and converted.\n\nFor example, the `RAND` function in Excel generates random numbers between 0 and 1 with a continuous uniform distribution. The `BINOM.INV` function is the binomial distribution's quantile function. Suppose that $n$ (number of Bernoulli trials) is 5 and $p$ (probability of success on each Bernoulli trial) is 0.6. A randomly generated number from the binomial distribution with $n=5$ and $p=0.6$ is generated like so:\n\n`=BINOM.INV(5,0.6,RAND())`\n\nExcel has quantile functions for many distributions (e.g., `BETA.INV, BINOM.INV, CHISQ.INV, F.INV, GAMMA.INV, LOGNORM.INV, NORM.INV, T.INV`). This method of combining `RAND` and a quantile function works reasonably well in Excel for quick-and-dirty projects, but when high levels of accuracy are needed, random samples should be generated in a dedicated statistical program like R, Python (via the numpy package), Julia, STATA, SAS, or SPSS.\n\n\n\n### Normal Distributions {#sec:normal}\n\n(Unfinished)\n\n\n\n\n\n\n\n\n::: {#tbl-normalfeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-colwidths='[30,70]' tbl-cap='Features of Normal Distributions'}\n::: {.cell-output-display}\n\n\n|Feature                          |Symbol                                                                                                                                           |\n|:--------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------|\n|Sample Space                     |$x \\in (-\\infty,\\infty)$                                                                                                                         |\n|Mean                             |$\\mu = \\mathcal{E}\\left(X\\right)$                                                                                                                |\n|Variance                         |$\\sigma^2 = \\mathcal{E}\\left(\\left(X - \\mu\\right)^2\\right)$                                                                                      |\n|Skewness                         |$\\gamma_1 = 0$                                                                                                                                   |\n|Kurtosis                         |$\\gamma_2 = 0$                                                                                                                                   |\n|Probability Density Function     |$f_X(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma ^ 2}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$                                    |\n|Cumulative Distribution Function |$F_X(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} {\\displaystyle \\int_{-\\infty}^{x} e ^ {-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx}$ |\n\n\n:::\n:::\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\" fig.cap.location='top'}\n::: {.cell-output-display}\n![Carl Friedrich Gauss (1777--1855)<br>[Image Credits](https://en.wikipedia.org/wiki/File:Carl_Friedrich_Gauss_1840_by_Jensen.jpg)](images/Carl_Friedrich_Gauss2.jpg){#fig-GaussImage fig-align='left' width=300}\n:::\n:::\n\n\n\n\n\nThe normal distribution is sometimes called the *Gaussian* distribution after its discoverer, Carl Friedrich Gauss [@fig-GaussImage]. It is a small injustice that most people do not use Gauss's name to refer to the normal distribution. Thankfully, Gauss is not exactly languishing in obscurity. He made so many discoveries that his name is all over mathematics and statistics.\n\n\nThe normal distribution is probably the most important distribution in statistics and in psychological assessment. In the absence of other information, assuming that an individual difference variable is normally distributed is a good bet. Not a sure bet, of course, but a good bet. Why? What is so special about the normal distribution? \n\nTo get a sense of the answer to this question, consider what happens to the binomial distribution as the number of events ($n$) increases. To make the example more concrete, let's assume that we are tossing coins and counting the number of heads $(p=0.5)$. In @fig-ManyCoins, the first plot shows the probability mass function for the number of heads when there is a single coin $(n=1)$). In the second plot, $n=2$ coins. That is, if we flip 2 coins, there will be 0, 1, or 2 heads. In each subsequent plot, we double the number of coins that we flip simultaneously. Even with as few as 4 coins, the distribution begins to resemble the normal distribution, although the resemblance is very rough. With 128 coins, however, the resemblance is very close.\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![The binomial distribution begins to resemble the normal distribution when the number of events is large.](distributions_files/figure-html/fig-ManyCoins-1.png){#fig-ManyCoins fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n\n\nThis resemblance to the normal distribution in the example is not coincidental to the fact that $p=0.5$, making the binomial distribution symmetric. If $p$ is extreme (close to 0 or 1), the binomial distribution is asymmetric. However, if $n$ is large enough, the binomial distribution eventually becomes very close to normal.\n\nMany other distributions, such as the Poisson, Student's T, F, and $\\chi^2$ distributions, have distinctive shapes under some conditions but approximate the normal distribution in others (See @fig-nearlynormal). Why? In the conditions in which non-normal distributions approximate the normal distribution, it is because, like in @fig-ManyCoins, many independent events are summed.\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![Many distributions become nearly normal when their parameters are high.](distributions_files/figure-html/fig-nearlynormal-1.png){#fig-nearlynormal fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n#### Notation for Normal Variates\n\nStatisticians write about variables with normal distributions so often that a compact notation for specifying a normal variable's parameters was useful to develop. If I want to specify that $X$ is a normally  variable with a mean of $\\mu$ and a variance of $\\sigma^2$, I will use this notation: \n\n$$X \\sim \\mathcal{N}(\\mu, \\sigma^2)$$ \n\n\n\n\n\n::: {#tbl-normal_notation .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-cap='Features of Half-Normal Distributions' tbl-colwidths='[40,60]'}\n::: {.cell-output-display}\n\n\n|Symbol        |Meaning                   |\n|:-------------|:-------------------------|\n|$X$           |A random variable.        |\n|$\\sim$        |Is distributed as         |\n|$\\mathcal{N}$ |Has a normal distribution |\n|$\\mu$         |The population mean       |\n|$\\sigma^2$    |The population variance   |\n\n\n:::\n:::\n\n\n\n\n\n\nMany authors list the standard deviation $\\sigma$ instead of the variance $\\sigma^2$. When I specify normal distributions with specific means and variances, I will avoid ambiguity by always showing the variance as the standard deviation squared. For example, a normal variate with a mean of 10 and a standard deviation of 3 will be written as $X \\sim \\mathcal{N}(10,3^2)$.\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![Percentiles convert a distribution into a uniform distribution](distributions_files/figure-html/fig-PercentileContinuous-1.png){#fig-PercentileContinuous fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![Evenly spaced percentile ranks are associated with unevenly spaced scores.](distributions_files/figure-html/fig-percentileuneven-1.png){#fig-percentileuneven fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![Evenly spaced scores are associated with unevenly spaced percentiles](distributions_files/figure-html/fig-scoreuneven-1.png){#fig-scoreuneven fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n\n#### Half-Normal Distribution\n\n(Unfinished)\n\n\n\n\n\n\n\n\n::: {#tbl-halfnormalfeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-cap='Features of Half-Normal Distributions' tbl-colwidths='[40,60]'}\n::: {.cell-output-display}\n\n\n|Feature                          |Symbol                                                                                                                                  |\n|:--------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------|\n|Sample Space                     |$x \\in [\\mu,\\infty)$                                                                                                                    |\n|Mu                               |$\\mu \\in (-\\infty,\\infty)$                                                                                                              |\n|Sigma                            |$\\sigma \\in [0,\\infty)$                                                                                                                 |\n|Mean                             |$\\mu + \\sigma\\sqrt{\\frac{2}{\\pi}}$                                                                                                      |\n|Variance                         |$\\sigma^2\\left(1-\\frac{2}{\\pi}\\right)$                                                                                                  |\n|Skewness                         |$\\sqrt{2}(4-\\pi)(\\pi-2)^{-\\frac{3}{2}}$                                                                                                 |\n|Kurtosis                         |$8(\\pi-3)(\\pi-2)^{-2}$                                                                                                                  |\n|Probability Density Function     |$f_X(x;\\mu,\\sigma) = \\sqrt{\\frac{2}{\\pi \\sigma ^ 2}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$                               |\n|Cumulative Distribution Function |$F_X(x;\\mu,\\sigma) = \\sqrt{\\frac{2}{\\pi\\sigma}} {\\displaystyle \\int_{\\mu}^{x} e ^ {-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx}$ |\n\n\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![The half-normal distribution is the normal distribution with the left half of the distribution stacked on top of the right half of the distribution.](distributions_files/figure-html/fig-halfnormal-1.png){#fig-halfnormal fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-halfnormal\n\n```{.r .cell-code}\n# Half normal distribution\nxlim <- 4\nn <- length(seq(-xlim, 0, 0.01))\nt1 <- tibble(\n  x = c(0,-xlim,\n        seq(-xlim, 0, 0.01),\n        0,\n        0,\n        seq(0, xlim, 0.01),\n        xlim,\n        0),\n  y = c(0,\n        0,\n        dnorm(seq(-xlim, 0, 0.01)),\n        0,\n        0,\n        dnorm(seq(0, xlim, 0.01)),\n        0,\n        0),\n  side = c(rep(F, n + 3), rep(T, n + 3)),\n  Type = 1\n)\nt2 <- t1 %>%\n  mutate(y = if_else(side, y, 2 * y)) %>%\n  mutate(x = abs(x),\n         Type = 2)\n\nbind_rows(t1, t2) %>%\n  mutate(Type = factor(Type)) %>%\n  ggplot(aes(x, y, fill = side)) +\n  geom_polygon() +\n  geom_text(\n    data = tibble(\n      x = 0,\n      y = dnorm(0) * c(1, 2) + 0.14,\n      Type = factor(c(1,2)),\n      label = c(\n        \"Normal\",\n        \"Half-Normal\"),\n      side = T),\n    aes(label = label),\n    family = bfont, fontface = \"bold\",\n    size = ggtext_size(30), \n    vjust = 1\n  ) +\n  geom_richtext(\n    data = tibble(\n      x = 0,\n      y = dnorm(0) * c(1, 2) + 0,\n      Type = factor(c(1,2)),\n      label = c(\n        paste0(\"*X* ~ \",\n               span_style(\"N\", style = \"font-family:'Lucida Calligraphy'\"),\n               \"(*\",\n               span_style(\"&mu;\"),\n               \"*, *\",\n               span_style(\"&sigma;\"),\n               \"*<sup>2</sup>)\"),\n        paste0(\"*X* ~ |\",\n               span_style(\"N\", style = \"font-family:'Lucida Calligraphy'\"),\n               \"(0, *\",\n               span_style(\"&sigma;\"),\n               \"*<sup>2</sup>)| + *\",\n               span_style(\"&mu;\"),\n               \"*\")),\n      side = T),\n    aes(label = label),\n    family = c(\"Equity Text A\"),\n    size = ggtext_size(30), \n    vjust = 0, \n    label.padding = unit(0,\"lines\"), \n    label.color = NA,\n    fill = NA) +\n  theme_void(base_size = 30,\n                base_family = bfont) +\n  theme(\n    legend.position = \"none\",\n    strip.text = element_blank()\n  ) +\n  scale_fill_manual(values = myfills) +\n  facet_grid(rows = vars(Type), space = \"free_y\", scales = \"free_y\") \n```\n\n\n:::\n:::\n\n\n\n\n\nSuppose that $X$ is a normally distributed variable such that \n\n$$X \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n\nVariable $Y$ then has a half-normal distribution such that $Y = |X-\\mu|+\\mu$. In other words, imagine that a normal distribution is folded at the mean with the left half of the distribution now stacked on top of the right half of the distribution (See @fig-halfnormal).\n\n\n\n\n#### Truncated Normal Distributions\n\n(Unfinished)\n\n#### Multivariate Normal Distributions\n\n(Unfinished)\n\n\n### Chi Square Distributions\n\n(Unfinished)\n\n\n\n\n\n::: {#tbl-chisquarefeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-cap='Features of Chi-Square Distributions' tbl-colwidths='[50,50]'}\n::: {.cell-output-display}\n\n\n|Feature                          |Symbol                                                                             |\n|:--------------------------------|:----------------------------------------------------------------------------------|\n|Sample Space                     |$x \\in [0,\\infty)$                                                                 |\n|Degrees of freedom               |$\\nu \\in [0,\\infty)$                                                               |\n|Mean                             |$\\nu$                                                                              |\n|Variance                         |$2\\nu$                                                                             |\n|Skewness                         |$\\sqrt{8/\\nu}$                                                                     |\n|Kurtosis                         |$12/\\nu$                                                                           |\n|Probability Density Function     |$f_X(x;\\nu) = \\frac{x^{\\nu/2-1}}{2^{\\nu/2}\\;\\Gamma(\\nu/2)\\,\\sqrt{e^x}}$            |\n|Cumulative Distribution Function |$F_X(x;\\nu) = \\frac{\\gamma\\left(\\frac{\\nu}{2},\\frac{x}{2}\\right)}{\\Gamma(\\nu/2 )}$ |\n\n\n:::\n:::\n\n\n\n\n\nI have always thought that the $\\chi^2$ distribution has an unusual name. The *chi* part is fine, but why *square*? Why not call it the $\\chi$ distribution?^[Actually, there *is* a $\\chi$ distribution. It is simply the square root of the $\\chi^2$ distribution. The half-normal distribution happens to be a $\\chi$ distribution with 1 degree of freedom.] As it turns out, the $\\chi^2$ distribution is formed from squared quantities.\n\n\n[**Notation note**: A $\\chi^2$ distribution with $\\nu$ degrees of freedom can be written as $\\chi^2_\\nu$ or $\\chi^2(\\nu)$.]{.column-margin}\n\n\nThe $\\chi^2$ distribution has a straightforward relationship with the normal distribution. It is the sum of multiple independent squared normal variates. That is, if $z$ is a standard normal variate--- $z\\sim\\mathcal{N}(0,1^2)$---then $z^2$ has a $\\chi^2$ distribution with 1 degree of freedom $(\\nu)$:\n\n\n$$z^2\\sim \\chi^2_1$$\n\n\nIf $z_1$ and $z_2$ are independent standard normal variates, the sum of their squares has a $\\chi^2$ distribution with 2 degrees of freedom:\n\n$$z_1^2+z_2^2 \\sim \\chi^2_2$$\nIf $\\{z_1,z_2,\\ldots,z_{\\nu} \\}$ is a series of $\\nu$ independent standard normal variates, the sum of their squares has a $\\chi^2$ distribution with $\\nu$ degrees of freedom: \n\n$$\\sum^\\nu_{i=1}{z_i^2} \\sim \\chi^2_\\nu$$\n\n#### Clinical Uses of the $\\chi^2$ distribution\n\nThe $\\chi^2$ distribution has many applications, but the mostly likely of these to be used in psychological assessment is the $\\chi^2$ Test of Goodness of Fit and the $\\chi^2$ Test of Independence. \n\nSuppose we suspect that a child's temper tantrums are more likely to occur on weekdays than on weekends. The child's mother has kept a record of each tantrum for the past year and was able to count the frequency of tantrums. If tantrums were equally likely to occur on any day, 5 of 7 tantrums should occur on weekdays, and 2 of 7 tantrums should occur on weekends. The observed frequencies are compared with the expected frequencies below.\n\n$$\\begin{array}{r|c|c|c}\n& \\text{Weekday} & \\text{Weekend} & \\text{Total} \\\\\n\\hline\n\\text{Observed Frequency}\\, (o) & 14 & 13 & n=27\\\\\n\\text{Expected Proportion}\\,(p) & \\frac{5}{7} & \\frac{2}{7} & 1\\\\\n\\text{Expected Frequency}\\, (e = np)& 27\\times \\frac{5}{7}= 19.2857& 27\\times \\frac{2}{7}= 7.7143& 27\\\\\n\\text{Difference}\\,(o-e) & -5.2857 & 5.2857&0\\\\\n\\frac{(o-e)^2}{e} & 1.4487 & 3.6217 & \\chi^2 = 5.07\n\\end{array}$$\n\nIn the table above, if the observed frequencies $(o_i)$ are compared to their respective expected frequencies $(e_i)$, then:\n\n$$\\chi^2_{k-1}=\\sum_{i=1}^k{\\frac{(o_i-e_i)^2}{e_i}}=5.07$$\n\nUsing the $\\chi^2$ cumulative distribution function, we find that the probability of observing the frequencies listed is low under the assumption that tantrums are equally likely each day. \n\n\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\nobserved_frequencies <- c(14, 13)\nexpected_probabilities <- c(5,2) / 7\n\nfit <- chisq.test(observed_frequencies, p = expected_probabilities)\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tChi-squared test for given probabilities\n\ndata:  observed_frequencies\nX-squared = 5.0704, df = 1, p-value = 0.02434\n```\n\n\n:::\n\n```{.r .cell-code}\n# View expected frequencies and residuals\nbroom::augment(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  Var1  .observed .prop .expected .resid .std.resid\n  <fct>     <dbl> <dbl>     <dbl>  <dbl>      <dbl>\n1 A            14 0.519     19.3   -1.20      -2.25\n2 B            13 0.481      7.71   1.90       2.25\n```\n\n\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n<tr>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"1\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">A</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\"> B </th>\n   <th style=\"text-align:center;\"> 0 </th>\n   <th style=\"text-align:center;\"> 1 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 0 </td>\n   <td style=\"text-align:center;\"> 36 </td>\n   <td style=\"text-align:center;\"> 39 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 20 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 9\n  A     B     .observed .prop .row.prop .col.prop .expected .resid .std.resid\n  <fct> <fct>     <int> <dbl>     <dbl>     <dbl>     <dbl>  <dbl>      <dbl>\n1 0     0            36  0.36     0.878      0.48      30.8  0.947       2.47\n2 1     0            39  0.39     0.661      0.52      44.2 -0.789      -2.47\n3 0     1             5  0.05     0.122      0.2       10.2 -1.64       -2.47\n4 1     1            20  0.2      0.339      0.8       14.8  1.37        2.47\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n### Student's *t* Distributions\n\n\n\n\n\n::: {#tbl-tfeatures .cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-cap='Features of *t* Distributions<br>**Notation note**: $\\Gamma$ is the gamma function. $_2F_1$ is the hypergeometric function.' tbl-colwidths='[50,50]'}\n::: {.cell-output-display}\n\n\n|Feature                          |Symbol                                                                                                                                                                                                                    |\n|:--------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|Sample Space                     |$x \\in (-\\infty,\\infty)$                                                                                                                                                                                                  |\n|Degrees of Freedom               |$\\nu \\in (0,\\infty)$                                                                                                                                                                                                      |\n|Mean                             |$\\left\\{\n\\begin{array}{ll}\n      0 & \\nu \\gt  1 \\\\\n      \\text{Undefined} & \\nu \\le 1 \\\\\n\\end{array} \n\\right.$                                                                                                                 |\n|Variance                         |$\\left\\{\n\\begin{array}{ll}\n      \\frac{\\nu}{\\nu-2} & \\nu\\gt 2 \\\\\n      \\infty & 1 \\lt \\nu \\le 2\\\\\n      \\text{Undefined} & \\nu \\le 1 \\\\\n\\end{array} \n\\right.$                                                                   |\n|Skewness                         |$\\left\\{\n\\begin{array}{ll}\n      0 & \\nu \\gt  3 \\\\\n      \\text{Undefined} & \\nu \\le 3 \\\\\n\\end{array} \n\\right.$                                                                                                                 |\n|Kurtosis                         |$\\left\\{\n\\begin{array}{ll}\n      \\frac{6}{\\nu-4} & \\nu \\gt 4 \\\\\n      \\infty & 2 \\lt \\nu \\le 4\\\\\n      \\text{Undefined} & \\nu \\le 2 \\\\\n\\end{array} \n\\right.$                                                                    |\n|Probability Density Function     |$f_X(x; \\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})} {\\sqrt{\\nu\\pi}\\,\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{x^2}{\\nu} \\right)^{-\\frac{\\nu+1}{2}}$                                                                                  |\n|Cumulative Distribution Function |$F_X(x; \\nu)=\\frac{1}{2} + x \\Gamma \\left( \\frac{\\nu+1}{2} \\right)  \\frac{\\phantom{\\,}_{2}F_1 \\left(\\frac{1}{2},\\frac{\\nu+1}{2};\\frac{3}{2};-\\frac{x^2}{\\nu} \\right)} {\\sqrt{\\pi\\nu}\\,\\Gamma \\left(\\frac{\\nu}{2}\\right)}$ |\n\n\n:::\n:::\n\n::: {.cell .fig-column-margin .fig-cap-location-top .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n![\"Student\" statistician, William Sealy Gosset (1876--1937)<br>[Image Credit](https://en.wikipedia.org/wiki/File:William_Sealy_Gosset.jpg)](images/william_sealy_gosset.jpg){#fig-gosset fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n\n(Unfinished)\n\nGuinness Beer gets free advertisement every time the origin story of the Student *t* distribution is retold, and statisticians retell the story often. The fact that the original purpose of the *t* distribution was to brew better beer seems too good to be true. \n\nWilliam Sealy Gosset (1876--1937), self-trained statistician and head brewer at Guinness Brewery in Dublin, continually experimented on small batches to improve and standardize the brewing process. With some help from statistician Karl Pearson, Gosset used then-current statistical methods to analyze his experimental results. Gosset found that Pearson's methods required small adjustments when applied to small samples. With Pearson's help and encouragement (and later from Ronald Fisher), Gosset published a series of innovative papers about a wide range of statistical methods, including the *t* distribution, which can be used to describe the distribution of sample means.\n\nWorried about having its trade secrets divulged, Guinness did not allow its employees to publish scientific papers related to their work at Guinness. Thus, Gosset published his papers under the pseudonym, \"A Student.\" The straightforward names of most statistical concepts need no historical treatment. Few of us who regularly use the Bernoulli, Pareto, Cauchy, and Gumbell distributions could tell you anything about the people who discovered them. But the oddly named \"Student's *t* distribution\" cries out for explanation. Thus, in the long run, it was Gosset's anonymity that made him famous. \n\n\n\n\n\n\n\n\n\n\n::: {.cell .fig-column-margin .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"left\" fig.cap.location='top'}\n::: {.cell-output-display}\n![The *t* distribution approaches the standard normal distribution as the degrees of freedom (*df*) parameter increases.](images/tdist_norm.gif){#fig-tnorm fig-align='left' width=100%}\n:::\n:::\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" button_before='true' button_after='true' codelabel='R Code'}\n:::{.callout-note collapse=\"true\" appearance=\"minimal\"}\n## R Code for @fig-tnorm\n\n```{.r .cell-code}\n# The t distribution approaches the normal distribution\nd <- crossing(x = seq(-6,6,0.02), \n         df = c(seq(1,15,1),\n                seq(20,45,5),\n                seq(50,100,10),\n                seq(200,700,100))) %>%\n  mutate(y = dt(x,df),\n         Normal = dnorm(x)) \n\nt_size <- 40\n\nd_label <- d %>% \n  select(df) %>% \n  unique() %>% \n  mutate(lb = qt(.025, df),\n         ub = qt(0.975, df)) %>% \n  pivot_longer(c(lb, ub), values_to = \"x\", names_to = \"bounds\") %>% \n  mutate(label_x = signs::signs(x, accuracy = .01),\n         y = 0,\n         yend = dt(x, df))\n\np <- ggplot(d, aes(x, y)) + \n  geom_area(aes(y = Normal), alpha = 0.25, fill = myfills[1]) +\n  geom_line() +\n  geom_area(data = . %>% filter(x >= 1.96), \n            alpha = 0.25, \n            fill = myfills[1],\n            aes(y = Normal)) +\n  geom_area(data = . %>% filter(x <= -1.96), \n            alpha = 0.25, \n            fill = myfills[1],\n            aes(y = Normal)) +\n  geom_text(data = d_label, \n            aes(label = label_x), \n            family = bfont, \n            vjust = 1.25,\n            size = ggtext_size(t_size)) + \n  geom_text(data = d_label %>% select(df) %>% unique,\n            aes(x = 0, y = 0, label = paste0(\"df = \", df)), \n            vjust = 1.25, \n            family = bfont,\n            size = ggtext_size(t_size)) + \n  geom_segment(data = d_label, aes(xend = x, yend = yend)) +\n  transition_states(states = df, \n                    transition_length =  1, \n                    state_length = 2) +\n  theme_void(base_size = t_size, base_family = bfont) +\n  # labs(title = \"df = {closest_state}\") +\n  annotate(x = qnorm(c(0.025, 0.975)), \n           y = 0, \n           label = signs::signs(qnorm(c(0.025, 0.975)), accuracy = .01), \n           geom = \"text\", \n           size = ggtext_size(t_size),\n           color = myfills[1],\n           vjust = 2.6, \n           family = bfont) + \n  coord_cartesian(xlim = c(-6,6), ylim = c(-.045, NA)) \n\nanimate(p, \n        renderer = magick_renderer(), \n        device = \"svglite\", \n        fps = 2, \n        height = 6, \n        width = 10)\ngganimate::anim_save(\"tdist_norm.gif\")\n```\n\n\n:::\n:::\n\n\n\n\n\n#### The *t* distribution's relationship Relationship to the normal distribution.\n\nSuppose we have two independent standard normal variates $Z_0 \\sim \\mathcal{N}(0, 1^2)$ and $Z_1 \\sim \\mathcal{N}(0, 1^2)$. \n\nA *t* distribution with one degree of freedom is created like so:\n\n$$\nT_1 = z_0\\sqrt{\\frac{1}{z_1^2}}\n$$\n\nA *t* distribution with two degrees of freedom is created like so:\n\n$$\nT_2 = z_0\\sqrt{\\frac{2}{z_1^2 + z_2^2}}\n$$\n\nWhere $z_0$, $z_1$ and $z_2$ are independent standard normal variates.\n\nA *t* distribution with $\\nu$ degrees of freedom is created like so:\n\n$$\nT_v = z_0\\sqrt{\\frac{\\nu}{\\sum_{i=1}^\\nu z_i^2}}\n$$\n\nThe sum of $\\nu$ squared standard normal variates $\\left(\\sum_{i=1}^\\nu z_i^2\\right)$ has a $\\chi^2$ distribution with $\\nu$ degrees of freedom, which has a mean of $\\nu$. Therefore, $\\sqrt{\\frac{\\nu}{\\sum_{i=1}^\\nu z_i^2}}$, on average, equals one. However, the expression $\\sqrt{\\frac{\\nu}{\\sum_{i=1}^\\nu z_i^2}}$ has a variability approaches 0 as $\\nu$ increases. When $\\nu$ is high, $z_0$ is being multiplied by a value very close to 1. Thus, $T_\\nu$ is nearly normal at high levels of $nu$. \n\n\n\n## Additional Distributions\n\n### F Distributions\n\nSuppose that $X$ is the ratio of two independent $\\chi^2$ variates $U_1$ and $U_2$ scaled by their degrees of freedom $\\nu_1$ and $\\nu_2$, respectively:\n\n$$X=\\frac{\\frac{U_1}{\\nu_1}}{\\frac{U_2}{\\nu_2}}$$\n\nThe random variate $X$ will have an $F$ distribution with parameters, $\\nu_1$ and $\\nu_2$.\n\nThe primary application of the $F$ distribution is to test the equality of variances in ANOVA. I am unaware of any direct applications of the *F* distribution in psychological assessment.\n\n\n### Weibull Distributions\n\nHow long do we have to wait before an event occurs? With Weibull distributions, we model wait times in which the probability of the event changes depending on how long we have waited. Some machines are designed to last a long time, but defects in a part might cause it fail quickly. If the machine is going to fail, it is likely to fail early. If the machine works flawlessly in the early period, we worry about it less. Of course, all physical objects wear out eventually, but a good design and regular maintenance might allow a machine to operate for decades. The longer machine has been working well, the less risk that it will irreparably fail on any particular day.\n\nFor some things, the risk of failure on any particular day becomes increasingly likely the longer it has been used. Biological aging causes increasing risk of death over time such that the historical records have no instances of anyone living beyond \n\nFor some events, there is a constant probability that the event will occur. For others, the probability is higher at first but becomes steadily less likely over time\n\n\nthe longer we wait the greater the probability will occur. For example, as animals age the probability of death accelerates such that beyond a certain age no individual as been observed to survive. \n\n### Unfinished\n\n* Gumbel Distributions\n* Beta Distributions\n* Exponential Distributions\n* Pareto Distributions\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}